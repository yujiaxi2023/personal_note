[[programming language learning/深度学习/Tramsformer/论文精讲李沐/1706.03762.pdf]]
[[1706.03762] Attention Is All You Need (arxiv.org)](https://arxiv.org/abs/1706.03762)

for natural language processing
"The cat eats the mouse"
将这段话转换为一个vector，然后用这段vector转换为目标语言
把合适的词，转换为vector，然后用神经网络转换为hidden state，
![[Pasted image 20230217175430.png]]
RNN的操作原理就是从这些隐藏层链接到下一个单词的隐藏层，最后输出一个隐藏层，然后从这个隐藏层可以对应由另一个语言的输出
但是在这句话中，需要注意的是两种语言的语法是不同的，对于名词的翻译是简单的可以通过RNN来完成，但是要翻译“eats”这种动词，那只靠RNN来翻译是不现实的，

这种叫long-range dependency：
```explaination

在自然语言处理（NLP）中，长距离依赖是指在一个句子或文本中彼此相距甚远的词或短语之间的关系。这些依赖关系可以是句法上的，也可以是语义上的，由于其遥远的性质，往往难以用传统的机器学习模型准确捕捉。

例如，考虑这样一个句子："1990年出生的约翰，去年从大学毕业"。"毕业 "这个词取决于 "约翰 "和 "大学"，但这些词离 "毕业 "有几个词的距离。为了准确模拟 "毕业 "和它的依赖关系，机器学习模型必须能够考虑整个句子，而不仅仅是紧紧围绕目标词的几个词。

长距离的依赖关系是NLP中常见的挑战，特别是在机器翻译、情感分析和问题回答等任务中。近年来，转化器等深度学习模型通过使用自我关注机制同时关注句子中的所有单词，在捕捉这些依赖关系方面显示出巨大的成功。

```
![[Pasted image 20230222183803.png]]
结论一
源语句中是上边的句子，猫吃老鼠，下面是预期输出的语句，别的语言版本，这时候文章给出了一个 transform architecture，这两个句子就是其中的input 和output，这个input的部分不是完整翻译出来的句子，而是首先翻译出来的第一个词语或者是前几个词语，然后根据这个载返回到最上边的最大可能性的output probability![[Pasted image 20230222184137.png]]


结论二
![[Pasted image 20230222184341.png]]
这个地方需要添加一个位置信息，这样才能知道这几个词在句子中的位置
![[Pasted image 20230222184557.png]]
利用三角函数的周期性，来定义这个句子或者单词字母在的位置

结论三
![[Pasted image 20230222185505.png]]
首先，源语句中会把句子切分为一些value，然后通过向量空间去对应这些value，也就是key，这部分key会通过vector找到对应的value进行dot-product，然后目标语句中的query是去索引这个value的