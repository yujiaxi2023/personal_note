#       **I.**            **PROPOSED METHOD**

## A.   Overview of the Proposed Method
![[Pasted image 20240705235932.png]]
Fig. 1. The overview of the prior knowledge-based deep learning approach for post-flood building damage assessment. This approach consists of both fully supervised and semi-supervised learning processes. "Pre" and "Post" denote the images before and after the disaster. "Conv" represents the convolutional module, "Concat" signifies concatenation, and the BIT Tokenizer is a convolutional module that transforms feature maps into tokens. Flood event caused damage levels are divided into no-damage, and damage (minor-damage, major-damage, and destroyed).

Fig. 1 illustrates the experimental workflow for achieving benchmark performance in attention modules and image-level consistency regularization. This workflow comprises a fully supervised process and a semi-supervised process. The fully supervised learning process tests the performance of CD networks with prior and self-attention modules in post-disaster assessment tasks, comparing the advantages of the two mechanisms under different tasks. The semi-supervised learning process evaluates the impact of various image-level consistency regularization perturbations and compares whether different reference distributions affect the amount of information learned from unlabeled data.

The proposed method in this paper adopts a two-stage improvement approach:

(1)   The improvements in the red dashed box in Fig. 1 are specific to the model architecture. The aim is to understand the capability of attention mechanisms in distinguishing subtle changes in images and eliminating background noise. This paper implements two enhancements to the encoder part of the CD model: self-attention and prior-attention, which is based on the visual neural interaction mechanism. The consideration for these encoder improvements is to enhance the model's perception by focusing on retaining and integrating shallow information with depth information.

(2)   The semi-supervised learning strategy employs a comprehensive approach involving consistency regularization and entropy minimization for unlabeled data. The improvements are shown in the blue dashed box in Fig. 1 This paper assumes four image-level reference distributions most likely to approximate the ground-truth label classification distribution of unlabeled data as practical perturbations: 1) Pseudo-label predictions generated by a well-trained model on unlabeled data. 2) True label predictions generated by a well-trained model on labeled data. 3) Real label group from the same dataset. 4) A combined distribution formed by combining the above three reference distributions. This assumption is based on the idea that when the preset reference distribution is closer to unlabeled data's ground-truth label classification distribution, the semi-supervised learning method trained with it will produce the most accurate predictions.

The following sections will describe the above improvements in detail.

## A.   Self-attention CD Network (BIT)

In CD, several studies have adopted self-attention mechanisms. For example, Xing et al. [14] incorporated self-attention to generate features between multimodal branches in FSA-UNet. Chen et al. [25] designed a pyramid spatial-temporal attention module (PAM) for spatial information utilization in STANet. Zhang et al. [15] enhanced depth information with the DSIFN, and Chen et al. [12] utilized a Transformer encoder and decoder in BIT.

In the proposed method, BIT serves as the primary reference design. The network processes bimodal images $(x_{pre}, x_{post}) \in \mathbb{R}^{H \times W \times C}$, where $H$, $W$, and $C$ represent the height, width, and channel information of the image, respectively. The first convolutional operation transforms the input into feature maps $(F_{pre}, F_{post})$, which serves the purpose of compressing image information to reduce computational burden. Subsequently, the feature maps undergo point-wise convolution to obtain tokenizers $(T_{pre}, T_{post}) \in \mathbb{R}^{L^{'} \times C}$, where $L^{'} \ll (H \times W)$. This transformation converts the image information into tokens similar to those in natural language processing (NLP). Concatenating $(T_{pre}, T_{post})$ forms $T \in \mathbb{R}^{2L^{'} \times C}$, which serves as the input to the Transformer. To compute self-attention, $T$ is linearly mapped into three inputs: query $Q$, key $K$, and value $V$, as follows:

$$

\begin{align*}

\mathbf{Q} &= \mathbf{T}\mathbf{W}_q, \\

\mathbf{K} &= \mathbf{T}\mathbf{W}_k, \\

\mathbf{V} &= \mathbf{T}\mathbf{W}_v,

\end{align*}

$$

where $\mathbf{W}_q, \mathbf{W}_k, \mathbf{W}_v \in \mathbb{R}^{C \times d}$ are learnable weight matrices. $C$ represents the original tokenizer's channel dimension, while $d$ is the channel dimension of the weight matrix. Based on $Q$, $K$, and $V$, one self-attention head can be calculated as follows:

$$

\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d}}\right)\mathbf{V}

$$

To avoid excessive computational load and enhance robustness, the Transformer architecture incorporates multi-head self-attention (MSA), which can be seen as a concatenation of multiple independent attention heads. Formally:

$$

\text{MultiHead}(\mathbf{X}) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)\mathbf{W}_O

$$

where $W_O \in \mathbb{R^{h \times d \times C}}$ are linear projection matrices that transform the encoded information back into the original tensor size of $T$. $h$ denotes the number of attention heads.

After the Transformer encoder, the encoded context information $T_{new} \in \mathbb{R}^{2L^{'} \times C}$ is split into two sets $(T_{new\_pre}, T_{new\_post})$ and then input into the Transformer decoder. In the decoder, the query $Q$ is derived from the Feature maps $(F_{pre}, F_{post})$ through a linear transformation. Consequently, the multi-head cross-attention (MA) in the decoder is defined as:

$$

\begin{align*}

\text{MA}(\mathbf{F}_{i}, \mathbf{T}, \mathbf{T}) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h) \mathbf{W}_O, \\

\text{where} \quad \text{head}_j &= \text{Att}(\mathbf{F}_{i}\mathbf{W}_{qj}, \mathbf{T}\mathbf{W}_{kj}, \mathbf{T}\mathbf{W}_{vj}).

\end{align*}

$$

Here, $\mathbf{W}_{qj}, \mathbf{W}_{kj}, \mathbf{W}_{vj} \in \mathbb{R}^{C \times d}$ are the linear projection matrices, and $W_O \in \mathbb{R^{h \times d \times C}}$ are the matrices that project the information back into the original tensor size of $T$. $h$ represents the number of attention heads.

Finally, a prediction head consisting of convolutional layers processes the attention feature maps to obtain the final change maps.

## A.   Simple Prior-attention CD Network (SPAUNet)
![[Pasted image 20240706000012.png]]
Fig. 2. Illustration of our SPAUNet model.
BIT transforms the image into semantically dense tokens and generates attention feature maps using self-attention. This operation can be seen as compressing and extracting information across the three dimensions of the shallow layers before amplifying important information through the attention mechanism, thereby making the changes in the features more pronounced after differentiation.

In designing the prior-attention CD model, this paper considered that the change features in disaster scenarios are subtle and prone to loss during downsampling. Therefore, a UNet capable of retaining shallow information was used as the base model, with the addition of the prior-attention module as shown in Fig. 2. During the encoding phase, five convolutional units are utilized to generate downsampled feature maps, thereby extracting multi-scale features. In the decoding phase, attention is chosen for the skip connection because the coarse-scale information before downsampling contains more comprehensive change features. However, not all coarse-scale features can be retained, as doing so might lead to background noise overwhelming the subtle change features.

_Prior-attention Module:_ This paper adopts the prior-attention module proposed by Yang et al. [29], which is based on the spatial inhibitory properties of neurons. This work designs an energy formula to calculate the importance of each neuron in a neural network:

$$ e^*_t = \frac{4(\hat{\sigma}^2 + \lambda)}{(t - {\hat{\mu})}^{2} + 2\hat{\sigma}^2 + 2\lambda} $$

Here, $\mu = \frac{1}{S} \sum_{i=1}^{S} x_i$ and $\hat{\sigma}^{2} = \frac{1}{S} \sum_{i=1}^{S}(x_i - \hat{\mu})^2$, where $t$ and $x_{\mathcal{i}}$ are the target neuron and other neurons in a channel of the input feature $X \in \mathbb{R}^{H \times W \times C}$, respectively, and $\mathcal{i}$ is the index of the spatial dimension. In this formula, $S = H \times W$ represents the number of neurons in a channel. The lower the value of $e^*_t$, the greater the difference between the target neuron $t$ and its surrounding neurons, indicating greater importance. Therefore, the importance of each neuron can be obtained by calculating $\frac{1} {e_t^{*}}$.

The final attention maps $\widetilde{X}$ are defined as:

$$ \widetilde{X} = \text{sigmoid}\left(\frac{1}{E}\right) \odot X $$

where $E$ represents the aggregation of all $e^*_t$ values across both spatial and channel dimensions.

The generated attention maps are then concatenated with the convolutional layers in the decoder and ultimately projected back into the pixel space to obtain the change maps.

## A.   Semi-supervised Learning Framework
![[Pasted image 20240706000033.png]]
Fig. 3. Overall framework of semi-supervised learning.

The semi-supervised learning method framework utilizing consistency constraint and proxy-label method is depicted in Fig. 3. The semi-supervised dataset $D$ in this experiment can be divided into two parts: the labeled dataset $D_l = \{(x_{l\_pre}^{\mathcal{i}}, x_{l\_post}^{\mathcal{i}}), y_l\}_{\mathcal{i=1}}^{M}$ and the unlabeled dataset $D_u = \{(x_{u\_pre}^{\mathcal{i}}, x_{u\_post}^{\mathcal{i}})\}_{\mathcal{i=1}}^{N}$. Here, $(x_{u\_pre}, x_{u\_post})$ and $(x_{l\_pre}, x_{l\_post})$ represent image pairs before and after the disaster, respectively, with $M$ and $N$ denoting the number of image pairs. $y_l$ is a length $L$ vector, where each element is drawn from the set $\{0,1,2,3\}$ representing the four possible classes.

First, the labeled dataset $D_l$ is used to fully train the CD model $f_\theta$, resulting in a pre-trained CD model $f_{\theta}'$. By inputting labeled images $x_l$ into the pre-trained model $f_{\theta}'$, segmentation predictions $\hat{y}_l$ are obtained. Utilizing $\hat{y}_l$ and ground truth $y_l$, the model is optimized through supervised loss $\mathcal{L}_s$. For the unlabeled dataset $D_u$, this paper employs a comprehensive approach combining proxy-label and consistency training. The essence of the proxy-label method is to minimize the model's entropy in low-density regions [34] by pre-training model $f_{\theta}'$ to generate predictions $\hat{y}_u$ for unlabeled data $x_u$ and then minimizing the information entropy $\mathcal{L_{u\_entropy}}$ of $\hat{y}_u$ to optimize the model.

![[Pasted image 20240706000048.png]]Fig. 4. Illustration of the process of obtaining reference distributions.

The semi-supervised learning framework in this study focuses on comparing the prediction distribution and reference distribution to obtain consistency regularization loss $\mathcal{L_{u\_ \,kl\, divergence}}$. This loss term determines whether it can correctly penalize the model for learning the correct label distribution, which depends on the reference distribution. This paper made the following assumptions for the reference distribution:

1)      The pseudo-label prediction group generated by fully pre-trained models on unlabeled data (strategy 1) is assumed. This hypothesis suggests that the generated pseudo-labels will be closer to the actual distribution if the model is reliable enough.

2)      The ground-truth label prediction group generated by fully pre-trained models on labeled data (strategy 2) is considered. This rule can be seen as using the distribution of ground-truth labels with noise as the reference distribution, which is believed to be more beneficial.

3)      The ground-truth label group from the same dataset (strategy 3) is hypothesized. This assumption posits that unlabeled and labeled data belong to the same event or similar events, and the label distribution should follow the same pattern.

4)      The pseudo-label prediction group, the ground-truth label group, and the ground-truth label prediction group are used as reference distributions to compute similarity with the predicted probability distribution. Then, the calculated loss functions are summed with equal weights (strategy 4). This strategy aims to confirm the positive and negative impacts of strategies 1), 2), and 3) on the model.

As shown in Fig. 1, a control unit is used to generate different reference distributions $Q$ based on the adopted strategies. The predictions $\hat{y}_u$ from unlabeled data are transformed into class distributions $P$, and the KL divergence $\mathcal{L_{u\_ \,kl\,divergence}}$ is calculated using $P$ and $Q$ to optimize the model.

The process of generating the reference distribution $Q$ is depicted in Fig. 4 In this process, batches of generated labels are used to count the elements in each class and store them in a distribution buffer. A buffer size $n$ is set, and after a certain number of labels are accumulated in the buffer, the average is calculated to obtain the reference distribution $Q$.

The four different reference distributions generated will serve as different optimization directions for semi-supervised learning, and their effectiveness will be compared through experiments to determine their superiority or inferiority.

## A.   Loss Function

Our proposed method involves two training processes, utilizing three loss functions for training the CD model. The loss function in the fully supervised training process is consistent with the supervised part of the semi-supervised training process, both being $\mathcal{L_s}$. The semi-supervised loss consists of minimizing entropy loss and KL divergence.

_1) Supervised Loss:_

In this experiment, which is a multi-classification task, the weighted cross-entropy loss is used to optimize the prediction probability distribution in supervised learning. This loss function only acts on labeled data and can be expressed as:

$$

\mathcal{L_s} = -\frac{1}{M} \sum_{i=1}^{M} \sum_{c=1}^{4} w_c y_{i,c} \log(p_{i,c})

$$

Here: $\mathcal{L_s}$ represents the loss, $M$ denotes the number of samples, $i$ is the index of the sample, $c$ is the index of the category, ranging from 1 to 4, representing the four damage categories. $w_c$ represents the weight of the category, $y_{i,c}$ is the true label of sample $i$ for category $c$, and $p_{i,c}$ is the model's predicted probability distribution for sample $i$ in category $c$.

_2) Entropy Minimization Loss:_

Generally, models tend to produce low-certainty, high-entropy predictions for unlabeled data. The pseudo-labeling method is based on the assumption that similar data in low-density regions are separable [36]. Therefore, minimizing the information entropy of pseudo-labels during training is necessary to prevent the decision boundary from approaching data points. The information entropy loss term used in this experiment is defined as:

$$

\mathcal{L_{u\_\,entropy}} = -\frac{1}{N}\sum_{i=1}^{N} \sum_{c=1}^{4} p_{i,c} \log p_{i,c}

$$

Here: $\mathcal{L}$ represents the loss function, $N$ denotes the number of samples, $C$ is the number of categories, and $p_{ij}$ is the probability that sample $i$ belongs to category $c$.

_3) Kullback-Leibler divergence:_

Since there are no original labels, this research assumes the ground-truth label distribution of unlabeled data as the reference distribution $Q$. By learning, we aim to make the predicted probability distribution of categories $P$ approach the reference distribution. We use the standard KL divergence to measure these two distributions' differences. The definition of KL divergence is as follows:

$$ D_{KL}(P \| Q) = \sum_{i} P(i) \log\frac{P(i)}{Q(i)} $$

In this formula, $P(i)$ and $Q(i)$ represent the probabilities of distribution $P$ and $Q$ at the $i$ th element, respectively. Based on the KL divergence, the loss function is defined as:

$$ \mathcal{L}_{\text{u\_\,kl\,divergency}} = \frac{1}{N} \sum_{i=1}^{N} D_{KL}(P, Q) $$

Here: $\mathcal{L}_{\text{KL}}$ denotes the loss function for KL divergence, $N$ is the number of samples, $D_{KL}(P,Q)$ represents the KL divergence between the reference distribution of unlabeled data and the model's predicted distribution, with $P$ representing the model's predicted distribution and $Q$ representing the reference distribution of unlabeled data.

This loss function aims to minimize the KL divergence, thereby making the model's prediction distribution as close as possible to the ground-truth label classification distribution of labeled data, improving the model's performance on unlabeled data.

#       **I.**            **EXPERIMENT AND RESULT**

## A.   Description of Dataset

The xBD dataset is the largest building damage assessment dataset (Gupta et al., 2019), consisting of images sourced from the Maxar/DigitalGlobe Open Data Program (https://www.digitalglobe.com/ecosystem/open-data). This dataset encompasses over 453,610 square kilometers and includes 850,736 building instances. xBD provides building polygons, labels of damage levels (Table x), and high-spatial-resolution (HSR) bitemporal optical satellite images with dimensions of 1024 × 1024 pixels and a ground sample distance (GSD) of less than 0.8 meters, capturing scenes before and after various disaster events. To assess building damage across multiple disaster types, xBD employs the Joint Damage Scale, developed with the assistance of the National Aeronautics and Space Administration (NASA), the California Department of Forestry and Fire Protection (CAL FIRE), the Federal Emergency Management Agency (FEMA), and the California Air National Guard [3]. The Joint Damage Scale comprises four discrete damage levels: No-Damage, Minor Damage, Major Damage, and Destroyed, serving as the criteria for damage classification.

In this study, we focused on pure flood events and manually selected relevant samples from the xBD dataset classified as flood-related. Our dataset comprises 1,064 pairs of high-spatial-resolution (HSR) remote sensing images. The images were divided into training, validation, and test sets in a 6:2:2 ratio and cropped to 256 × 256-pixel blocks. On the training set, image cropping was performed with a stride of 128 pixels, while on the validation and test sets, the stride size was 256 pixels. This resulted in 93,786/10,224/10,224 image blocks for the respective sets. The differences between the dataset used in this study and the original xBD dataset are presented in Table 1.

Table 1. Classification label ratios for all events and flood events in the xBD dataset.

|   |   |   |
|---|---|---|
|Included disaster events|All events|Flood events|
|Original sample number|22068|2128|
|No-damage class ratio|87.20%|91.70%|
|Minor-damage class ratio|4.57%|4.28%|
|Major-damage class ratio|5.48%|3.68%|
|Destroyed class ratio|2.74%|0.33%|

![[Pasted image 20240706000124.png]]Fig. 5. Dataset after data augmentation, including pre-disaster images, post-disaster images, and the damage classification.

## A.   Implementation Details

The experiment utilized the PyTorch framework and trained on a single NVIDIA RTX 4090 GPU. The Adam optimizer was selected, with an initial learning rate of 3e-5. The initial learning rate was reduced by 80% for every 60 iterations. The training epochs were set to 150, and the batch size was configured at 24.

In the supervised learning setup, the energy function bias term $\lambda$ for the prior attention module is set to $1e-4$. The size of the convolution kernel is set to 3×3, and the number of kernels in each convolution unit for the basic UNet is set to {16, 32, 64, 128, 256}.

In the semi-supervised learning setup, the sampling ratios for the dataset were set to {5%, 10%, 20%, 50%}. The weights $\alpha$ and $\beta$ for the three loss functions were set to 0.001. In strategy 4, since three different reference distributions are used to calculate the loss terms, there are three KL loss weights, $\beta_{1}$, $\beta_{2}$, $\beta_{3}$, all set to 0.001. The buffer size $n$ for the distribution buffer used to store the reference distributions was set to 10.

## B.    Evaluation Metrics

This research employed overall accuracy (OA), precision, Recall, f1 score, and Kappa as evaluation metrics, comparing the 4-class- and 2-class versions. To assess the model's overall performance, we categorized the "no damage" class from the 4-class version into the "no damage" class in the 2-class version. This research unified the "minor damaged," "major damaged," and "destroyed" classes from the 4-class version into the "damaged" class in the 2-class version. The definitions of these metrics are as follows:

$$ OA = \frac{TP + TN}{TP + TN + FP + FN} $$

$$ Precision = \frac{TP}{TP + FP} $$

$$ Recall = \frac{TP}{TP + FN} $$

$$ F1\ Score = \frac{2 \cdot Precision \cdot Recall}{Precision + Recall} $$

$$ Kappa = \frac{p_o - p_e}{1 - p_e} $$

$$ p_o = \frac{TP + TN}{TP + TN + FP + FN} $$

$$ p_e = \frac{(TP + FP) \cdot (TP + FN) + (FP + TN) \cdot (FN + TN)}{(TP + TN + FP + FN)^2} $$

In these formulas, TP (True Positive) represents the number of positive samples correctly predicted by the model, FP (False Positive) signifies the number of negative samples incorrectly predicted as positive, and FN (False Negative) indicates the number of positive samples incorrectly predicted as negative. $p_o$ is the observed classification consistency, while $p_e$ is the expected classification consistency. Note that higher F1-score, OA, and Kappa indicate better overall performance.

