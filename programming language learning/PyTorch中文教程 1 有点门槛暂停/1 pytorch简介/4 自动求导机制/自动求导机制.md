pytorch框架就是帮我们完全计算好了反向传播的过程
```python
import torch
```
需要求导的，可以手动定义：
```python
# 方法1
x = torch.randn(3,4,requires_grad=True)
print(x)
```
![[Pasted image 20230430152339.png]]
requires_grad=True
代表可以指定对其求导
构建x的时候直接将x导入了

```python
# 方法2
x = torch.randn(3,4)
x.requires_grad=True
print(x)
```
![[Pasted image 20230430152753.png]]
这样output也是一样的，只不过比上面的方法麻烦一点，重新写了一行代码

```python
b = torch.randn(3,4,requires_grad=True)
t = x + b
y = t.sum()
print(y)
```
![[Pasted image 20230430152955.png]]
这里我们可以把y看作是一个损失函数，这里的参数w是1，偏置项b是个定义的矩阵，然后损失函数就是t求和
```python
y.backward()
b.grad
```
![[Pasted image 20230430153111.png]]
上面是b打印出来的值
虽然没有指定t的requires_grad，但是由于我们需要用到他，所以也会默认可以求导，所以在输入下列命令的时候t.requires_grad也是True
```python
x.require_grad, b.require_grad, t.require_grad
```
前两者因为我们在定义x和b的时候让其可以求导，所以一定是true值
![[Pasted image 20230430153441.png]]
所以最后的结果就是true值

下面就是方程在神经网络中求偏导的链式法则过程
y = wx
z = y+b
![[Pasted image 20230430153825.png]]
```python
# 计算流程
x = torch.rand(1)
b = torch.rand(1, requires_grad = True)
w = torch.rand(1, requires_grad = True)
y = w * x
z = y + b

x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad # 注意y也是需要的，所以y的结果是True
```
![[Pasted image 20230430154120.png]]
```python
x.is_leaf, w.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf
```
可以看出这些参数什么是叶子
![[Pasted image 20230430154547.png]]

反向传播计算
```python
z.backward(retain_graph=True) # 如果不清空会累加起来
w.grad
b.grad
```
我们可以试着运行一下这个反向传播过程，可以看到如果不清空是什么效果
![[Pasted image 20230430155459.png]]
![[Pasted image 20230430155513.png]]
![[Pasted image 20230430155524.png]]
![[Pasted image 20230430155534.png]]
就会累加越来越大，看到b的值在累加起来
