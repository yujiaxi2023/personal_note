![[Pasted image 20230921163412.png]]
求导的方向是下降速度最快的方向
![[Pasted image 20230921163705.png]]
计算梯度是最耗费资源的，所以尽量选择合适大小

![[Pasted image 20230921163810.png]]
当batch更大的时候，近似的损失更加精确，当batch小的时候，误差很多

![[Pasted image 20230921163946.png]]

![[Pasted image 20230921164052.png]]

**问题1：损失求平均是为了让梯度下降是一个合适的值，如果不求平均就需要在学习率的地方进行添加**
**问题2：batchsize采样越小其实更好，因为会减少噪音，但是如果batchsize更大，增加噪音可以增加鲁棒性**
**问题3：每个batch每个参数更新减去的梯度是每个样本对应的梯度求均值**
**问题4：detach告诉不要算梯度了**
**问题5：在dataload的时候，数据集是放在硬盘上的，是每次随机抓一些数据，也就是batch大小的内存**
**问题6：yield相比于return是更节省内存，因为每个batch算一次**
**问题7：当样本大小不是批量数的整数倍，会有三种做法：第一种是减少样本是整数倍，第二是丢弃剩余的，第三是从下一个epoch中获得**
**问题8：优化算法中需要按照真实拿进来的batch size数量进行平均，这个在pytorch中实现了**
