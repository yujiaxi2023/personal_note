![[Pasted image 20230922002639.png]]
蓝色用来分割左正右负
黄色分割上正下负
求是否一样，正正为正，负负为正，正负为负

![[Pasted image 20230922003103.png]]
添加隐藏层，隐藏层大小是超参数，因为输入时固定的，输出根据你要的分类，其中可以设置的隐藏层

![[Pasted image 20230922003304.png]]
![[Pasted image 20230922003551.png]]
![[Pasted image 20230922003702.png]]
![[Pasted image 20230922003817.png]]
![[Pasted image 20230922003904.png]]
![[Pasted image 20230922004139.png]]
![[Pasted image 20230922004317.png]]
![[Pasted image 20230922004420.png]]
当输入数据比较难的时候，如果使用多层感知机，使用单隐藏层，那隐藏层的大小需要大一点，如果使用更深度的隐藏层，然后越深越小，实际上机器学习就是压缩，这样需要多层的隐藏层，慢慢的变小

![[Pasted image 20230922005026.png]]


**问题1：一层网络通常是带权重的一层网络，通常是权重然后进行激活函数，所以输入层一般不算层
问题2：SVM相比MLP有更多的解释性，但是没有MLP那么好用
问题3：理论上一层隐藏层可以拟合所有的函数，但是实际无法解决
问题4：学习过程，需要保持模型复杂度，有两种方式，第一种是一层胖一些，第二种是模型更深，在capacity上类似，但是胖一些的不好学习，深度学习比较好学习
问题5：ReLU即使是在大于零部分是线性，但是整个是非线性的，激活函数就是引入非线性性
问题6：激活函数本质上并不会影响太多深度学习的性能，相比于选择隐藏层大小这种超参数
问题7：当从128 -> 2的情况，中间添加一个隐藏层 16 32 64 128 这些不同大小，中间添加两个隐藏层，第一步的时候发现32和64比较好，然后接下来一个隐藏层用8 16 32这些尝试什么最好
问题8：稳定性的研究