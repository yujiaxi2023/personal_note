## 感知机

- 给定输入 **x** 权重是 **w** 和偏移**b**, 感知机输出:
$$
o = \sigma(\langle w, x\rangle + b)  \quad \quad \sigma(x) = \begin{cases} 1 & \text{if} \quad x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

从公式上看,就是输入一系列的$x_1$ ~ $x_d$  获得一个结果 $o$ 对$o$条件判断得到输出值0或者1

- 二分类: 0或者1
	- vs 回归输出实数, 这里输出的是离散的类
	- vs softmax回归是输出概率,可以多分类,这里只能二分类

## 训练感知机

$\textbf{initialize} \quad w = 0 \quad and \quad b = 0$ 
$\textbf{repeat}$
$\quad \textbf{if} \quad y_i \mathbf{[\langle w, x_i \rangle + b]} \leq 0 \quad \textbf{then}$
$\quad \quad w \leftarrow w + y_ix_i \quad \text{and} \quad b \leftarrow b + y_i$
$\quad \textbf{end if}$ 
$\textbf{until} \quad \text{all classified correctly}$ 
等价使用批量大小为1的梯度下降
并使用如下的损失函数
$\ell{(y,x,w) = \max{(0,-y\langle w, x \rangle)}}$ 

回到上边的感知机输出
o也就是y，当$\sigma{x}$中$x \textgreater 0$ 的时候，也就是$\langle w, x\rangle + b \textgreater 0$ 的时候y输出1，否则输出0，这里我们假装是-1比较好判断等式
所以在$o \quad \text{and} \quad \langle w, x\rangle + b$ 是同号的时候就代表预测正确，不同号就是预测错误
如果预测错了，我们就对w进行更新，b进行更新，一直到所有分类正确

在感知机模型中，一个样本的分类决策是基于 `y_pred = sign(w * x + b)` 的计算结果，其中 `sign` 是符号函数，如果 `w * x + b` 大于0，则预测分类为+1，否则为-1。

当我们有一个样本 `x[i]` 被错误分类时，即 `y[i] * (w * x[i] + b) <= 0`，我们希望更新 `w` 和 `b` 以便更正确地分类这个样本。

- 如果 `y[i]` 是+1，但 `w * x[i] + b` 小于等于0，说明 `w` 和 `x[i]` 的方向相反或者夹角过大，我们需要增加 `w` 和 `x[i]` 的相似度，所以我们加上 `y[i] * x[i]`（即 `x[i]`）。
    
- 如果 `y[i]` 是-1，但 `w * x[i] + b` 大于等于0，说明 `w` 和 `x[i]` 的方向相同或者夹角过小，我们需要减小 `w` 和 `x[i]` 的相似度，所以我们加上 `y[i] * x[i]`（即 `-x[i]`）。
    

这是一个很巧妙的想法，通过y的符号来决定w是朝着x相近还是相反的方向接近。

等价于一个batch1的梯度下降，并不是随机的梯度下降
$\ell{(y,x,w) = \max{(0,-y\langle w, x \rangle)}}$ 
如果分类正确$y\langle w, x \rangle$是大于0的，所以输出就会是0，而0是一个常数，就不会做梯度更新
如果分类错误，这时候就会有梯度了，进行梯度下降的计算更新权重

![[Pasted image 20231109203229.png]]
第一次输入数据的时候的权重是这样分类的
![[Pasted image 20231109203425.png]]
第二次输入数据的时候，因为出现了一个错误案例，所以就会更新
![[Pasted image 20231109203506.png]]


### 收敛定理

- 数据在半径 $r$ 内
- 余量 $\rho$ 分类两类
$y(x^{\top}w + b) \geq \rho$
对于 $\| w \|_2 + b^2 \leq 1$ 
- 感知机保证在 $\frac{r^2 + 1}{{\rho^2}}$ 步后收敛

上面都是假设条件
其中半径r代表的是所有数据都可以放在这个范围内，注意这里不只是包含着中心点，而是所有的数据边缘也是
第二点就是可以有一个余量margin，将两类数据分开
第三点就是在上面两点满足的前提下满足的结果，就是一定会收敛
![[Pasted image 20231109204930.png]]

**我们从直观上来看，如果这个r很大，代表数据的活动范围很广，这样收敛起来就会慢
而如果我们的margin很大，代表两个数据分的很开，收敛起来就会更快**

### XOR问题（Minsky和Papert 1969）

感知机不能拟合XOR函数，只能产生线性分割面

![[Pasted image 20231109205219.png]]
