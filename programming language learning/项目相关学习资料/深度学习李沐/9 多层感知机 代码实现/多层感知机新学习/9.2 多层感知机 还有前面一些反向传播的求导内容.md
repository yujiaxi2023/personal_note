### 学习XOR

![[Pasted image 20231109213555.png]]
现在我们有1234为标号的绿球和红球
我们要进行尽可能的分类

参照下面的表格
![[Pasted image 20231109213642.png]]

其中蓝色是将13划分到一边，24划分到一边，可以是根据x的值来看
黄色是看y的值进行划分

这时候对两个分类器结果进行判断
如果一致，结果是正，如果不一致，结果是负
![[Pasted image 20231109214048.png]]

**这就是多层感知机的解决方法
把最终的分类问题拆分成多个简单步骤的组合**

### 单隐藏层

![[Pasted image 20231109214221.png]]

输入的是 x1 x2 x3 x4 四个特征
然后我们进入到 1 个隐藏层进行计算
假设一共有5个隐藏层，**隐藏层的数量是超参数**
一直算到最后一个隐藏层，这时候隐藏层的输出转换为下一层的输入，也就是最终分类数量

**输入取决于你的数据的特征维度，输出取决于你需要的分类数量，所以唯一可以决定的就是隐藏层的内容**

#### 单隐藏层-单分类

- 输入 $x \in \mathbb{R}^n$
- 隐藏层$W_1 \in \mathbb{R}^{m \times n} \text{,} \quad b_1 \in \mathbb{R}^m$ 
- 输出层$w_2 \in \mathbb{R}^m \text{,} \quad b_2 \in \mathbb{R}$
$$
\begin{align}
h = \sigma{(W_1x + b_z)}\\
o = w_2^Th + b_2
\end{align}
$$
$\sigma$ 是按元素的激活函数

输入是一个n维的向量
隐藏层假设有m个，所以参数就应该是一个m行n列的矩阵，因为m个样本，n个特征，偏移就是m个样本数的偏移b
输出层只有1个，所以就是长为m的向量，偏移就是1

跟softmax操作类似，输入$x$ 乘以权重$W_1$ ，$W_1x$ 然后加上偏移$b_1$
这里的$\sigma$ 是一个激活函数，是一个按元素计算的函数
h这里就是一个隐藏层的输出值，作为下一层的输入值传入到o中


###### 为什么我们需要一个激活函数，而且是非线性的？

$$\textbf{hence}\ o = w_2^TW_1x + b^{'}$$
假设激活函数是本身，如果把h带入下面就是上面的式子
因为b是一个m长的向量，w2也是m长的向量，因为这里输出层只有1个
所以向量和转置相乘就是标量，所以偏移还是一个标量

$w_2^TW_1$ 中$w_2^T$是1xm的行向量，$W_1$是mxn的矩阵，结果是1xn的行向量
所以还是线性变化的函数


### sigmoid激活函数

将输入投影到 (0,1)，是一个软的
$$\sigma(x) = \begin{cases} 1 & \text{if} \quad x > 0 \\
0 & \text{otherwise}
\end{cases}$$
$\operatorname{sigmoid}(x) = \frac{1} {1+\exp(-x)}$ 
![[Pasted image 20231109232545.png]]
因为指数函数一定是正数，区别是在于>0 <0的时候是>1还是<1
所以结果一定是在0-1之间的开区间内，并且中间值还能保持1/2


### tanh激活函数

将输入投影到(-1,1)
$$\operatorname{tanh}(x) = \frac{1-\exp(-2x)}{1+\exp(-2x)}$$

![[Pasted image 20231109233323.png]]

### ReLU激活函数

最常用最有效的激活函数 rectified linear unit

$$\operatorname{ReLU}(x) = \max(x,0)$$
![[Pasted image 20231109233946.png]]

这个其实是数学当中的模型重命名的
还有back propagation也是简单的求导重命名

这个函数最好的就是算起来很快，指数运算是很贵的，cpu上1次指数运算等于100次乘法运算，GPU上稍微好一点


### 多类分类

$$y_1, y_2, \dots, y_k = \operatorname{softmax}(o_1, o_2, \dots, o_k) $$
![[Pasted image 20231109235200.png]]

softmax只是将所有的拉到0和1之间的区域
我们只是添加了隐藏层，就会变成多层感知机，也就是softmax添加了一层


- 输入 $x \in \mathbb{R}^n$
- 隐藏层$W_1 \in \mathbb{R}^{m \times n} \text{,} \quad b_1 \in \mathbb{R}^m$ 
- 输出层$W_2 \in \mathbb{R}^{m \times k} \text{,} \quad b_2 \in \mathbb{R}^k$
$$
\begin{align}
h = \sigma{(W_1x + b_z)}\\
o = W_2^Th + b_2\\
y = \operatorname{softmax}(o)
\end{align}
$$
输出层不是1个单元，而是k个单元
要对output做一层softmax


### 多隐藏层

![[Pasted image 20231110000105.png]]

$$
\begin{align}
h_1 = \sigma(W_1x + b_1)\\
h_2 = \sigma(W_2x + b_2)\\
h_3 = \sigma(W_3x + b_3)\\
o = W_4h_3 + b_4
\end{align}
$$
超参数
- 隐藏层数
- 每层隐藏层的大小

从数学上，每一个隐藏层都有一个权重（矩阵）和一个偏移
每层一定要有激活函数，否则层数就-1
输出是不需要激活函数的，因为激活函数是主要避免层数塌陷

##### 技巧

将m1，也就是第一层的隐藏层的选择：
- 选择更广的隐藏层，数字大一些
- 选择更深的隐藏层，数字小一些

越深数字越小，一般来说输入数据比较复杂，比如128，256维度，但是输出数据是比较简单的，10类，5类
所以就是把一个很复杂的维度压缩到很简单的输出上面
最好是慢慢的压缩回去，这是经验之谈

而且在m1，第一个隐藏层的时候，会适当的扩张一些，相比于输入数据
因为如果进行压缩，就是在损失信息，我们希望最开始信息保存的完整一些
后边想要还原信息是比较困难的
CNN就是先压缩后扩张的，这是另一个好处，是避免overfitting


### 总结

- MLP是使用隐藏层和激活函数得到非线性模型
- 常用激活函数sigmoid tanh ReLU
- 使用softmax处理多类分类
- 超参数作为隐藏层数，和各个隐藏层的大小



### 仿射变换

$$
price = w_{area} \ \cdot \ area \ + \ w_{age} \ \cdot \ age \ + b
$$
这里的公式其实是输入特征的一个*仿射变换* （affine transformation）。仿射变换的特点是通过加权和对特征进行线性变换，并通过偏置项进行平移


### 线性模型的问题

在实际问题中，有很多是可以转换为线性问题进行计算的，但是也有很多问题不行。
例如在图像分类问题中，增加位置（11，12）的像素强度是否总是增加猫/狗或者其他分类的似然？对线性模型的依赖对应于一个隐含的假设，即区分猫和狗的唯一要求是评估单个像素的强度，这显然是不符合现实世界的理解的，因为无论正反来看，或者偏移来看，猫狗的特征都能被人类识别出来。

因为我们处理图像的时候，是对一个个像素进行评估，任何像素对于上下文依赖的重要程度是无法被简单判定的。虽然可能存在一种考虑到特征之间相互作用的线性模型，但是手动计算无法进行。对于深度神经网络，使用观测数据来联合学习 隐藏层的表示 和 应用于该表示的线性预测其


### 从线性到非线性

同之前的章节一样，我们通过矩阵$\mathbf{X} \in \mathbb{R}^{n \times d}$来表示$n$个样本的小批量，其中每个样本具有$d$个输入特征。对于具有$h$个隐藏单元的单隐藏层多层感知机用$\mathbf{H} \in \mathbb{R}^{n \times h}$表示隐藏层的输出，称为*隐藏表示*（hidden representations）。
在数学或代码中，$\mathbf{H}$也被称为*隐藏层变量*（hidden-layer variable）或*隐藏变量*（hidden variable）。因为隐藏层和输出层都是全连接的，所以我们有隐藏层权重$\mathbf{W}^{(1)} \in \mathbb{R}^{d \times h}$和隐藏层偏置$\mathbf{b}^{(1)} \in \mathbb{R}^{1 \times h}$以及输出层权重$\mathbf{W}^{(2)} \in \mathbb{R}^{h \times q}$和输出层偏置$\mathbf{b}^{(2)} \in \mathbb{R}^{1 \times q}$。形式上，我们按如下方式计算单隐藏层多层感知机的输出$\mathbf{O} \in \mathbb{R}^{n \times q}$：

$$
\begin{aligned}
    \mathbf{H} & = \mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}, \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.
\end{aligned}
$$

注意在添加隐藏层之后，模型现在需要跟踪和更新额外的参数。
可我们能从中得到什么好处呢？在上面定义的模型里，我们没有好处！
原因很简单：上面的隐藏单元由输入的仿射函数给出，而输出（softmax操作前）只是隐藏单元的仿射函数。仿射函数的仿射函数本身就是仿射函数，
但是我们之前的线性模型已经能够表示任何仿射函数。

我们可以证明这一等价性，即对于任意权重值，
我们只需合并隐藏层，便可产生具有参数
$\mathbf{W} = \mathbf{W}^{(1)}\mathbf{W}^{(2)}$
和$\mathbf{b} = \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)}$
的等价单层模型：

$$
\mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} = \mathbf{X} \mathbf{W} + \mathbf{b}.
$$

为了发挥多层架构的潜力，
我们还需要一个额外的关键要素：
在仿射变换之后对每个隐藏单元应用非线性的*激活函数*（activation function）$\sigma$。
激活函数的输出（例如，$\sigma(\cdot)$）被称为*活性值*（activations）。
一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：

$$
\begin{aligned}
    \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
\end{aligned}
$$

由于$\mathbf{X}$中的每一行对应于小批量中的一个样本，
出于记号习惯的考量，我们定义非线性函数$\sigma$也以按行的方式作用于其输入，即一次计算一个样本。以相同的方式使用了softmax符号来表示按行操作。但是本节应用于隐藏层的激活函数通常不仅按行操作，也按元素操作。这意味着在计算每一层的线性部分之后，我们可以计算每个活性值，而不需要查看其他隐藏单元所取的值。对于大多数激活函数都是这样。

为了构建更通用的多层感知机，我们可以继续堆叠这样的隐藏层，
例如$\mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})$和$\mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})$，一层叠一层，从而产生更有表达能力的模型。


### 通用近似定理

MLP可以通过hidden layer捕捉到输入之间的复杂相互作用，这些神经元依赖于每个输入的值。我们可以很容易的设计隐藏节点来执行任意计算。理论上只要是足够的神经元和正确的权重，就能够对任意函数进行建模。

就有点像C语言，C语言可以像任何现代编程语言一样，表达可以计算的任何程序，但是实际上，想出一个符合规范的程序才是最困难的部分。

### 激活函数

激活函数activation function通过计算加权和偏置确定神经元是否应该被激活，因为输出值可能是0，如果不是0，那神经元就激活了。

#### ReLU函数

修正线性单元 Rectified linear unit，这种情况下当输入值精确等于0的时候，ReLU函数不可导，会默认使用左侧的导数，输入0，导数就是0

#### sigmoid函数

挤压函数 squashing function，将无穷区间内的任意输入压缩到区间 0 - 1

sigmoid的倒数图像是在0处达到峰值0.25，越远离0点，就越接近0
![[Pasted image 20231110153046.png]]


#### tanh函数

和sigmoid类似，双曲正切tanh函数也能将输入压缩转换到区间（-1，1）上
其导数最大值是1
![[Pasted image 20231110153153.png]]

### 练习

1. 计算pReLU激活函数的导数。
  
2. 证明一个仅使用ReLU（或pReLU）的多层感知机构造了一个连续的分段线性函数。

3. 证明tanh(x)+1=2sigmoid(2x)。

4. 假设我们有一个非线性单元，将它一次应用于一个小批量的数据。这会导致什么样的问题？


1. pReLU（Parametric Rectified Linear Unit）激活函数定义为： $$pReLU(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \leq 0 \end{cases}$$ 其中 ($\alpha$) 是一个小的正常数，通常是一个学习参数。

导数（或梯度）为：  $$pReLU'(x) = \begin{cases} 1 & \text{if } x > 0 \\ \alpha & \text{if } x \leq 0 \end{cases}$$ 当 ( x > 0 ) 时，导数为 1，当 ( $x \leq 0$ ) 时，导数为 ($\alpha$)。

2. ReLU（Rectified Linear Unit）激活函数定义为： $ReLU(x) = \max(0, x)$ 一个仅使用 ReLU 或 pReLU 的多层感知机构造了一个连续的分段线性函数，因为 ReLU 和 pReLU 本身就是分段线性的：在正数部分是线性的，而在负数部分要么是 0（ReLU），要么是 ($\alpha x$)（$pReLU$）。当这些函数堆叠在多层感知机中时，每个隐藏层的输出都是上一个层输入的分段线性函数。由于分段线性函数的组合仍然是分段线性的，所以整个多层感知机是连续的分段线性函数。

3. 要证明 $( \tanh(x) + 1 = 2 \cdot \text{sigmoid}(2x) )$，首先我们要知道它们的定义：$- (\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}) - (\text{sigmoid}(x) = \frac{1}{1 + e^{-x}})$

开始证明： 
$$ 
\begin{align} 
\text{sigmoid}(2x) &= \frac{1}{1 + e^{-2x}} \\ 
&= \frac{e^{2x}}{e^{2x} + 1} \\ 
&= \frac{e^{2x} + 1 - 1}{e^{2x} + 1} \\ 
&= 1 - \frac{1}{e^{2x} + 1} \\ 
&= 1 - \frac{e^{-x}e^x}{e^{2x} + 1} \\ 
&= 1 - \frac{e^{-x}}{(e^x + e^{-x})} \\ 
&= 1 - \frac{1}{2}\frac{2e^{-x}}{(e^x + e^{-x})} \\ 
&= 1 - \frac{1}{2}\frac{e^{x} + e^{-x} + e^{-x} - e^{x}}{e^{x} + e^{-x}} \\ 
&= 1 - \frac{1}{2}(\frac{e^{x} + e^{-x}}{e^{x} + e^{-x}} + \frac{e^{-x} - e^{x}}{e^{x} + e^{-x}}) \\ 
&= 1 - \frac{1}{2}(1 + (-\tanh(x))) \\ 
&= \frac{1}{2}(1 + \tanh(x) )
\end{align} 
$$ 所以，$$ 2 \cdot \text{sigmoid}(2x) = 1 + 1 \cdot \tanh(x) $$得到 $$ \tanh(x) + 1 = 2 \cdot \text{sigmoid}(2x) $$证明完成。


4. 如果我们对一个小批量的数据只应用一次非线性单元，可能会导致以下问题： - 信息损失：如果非线性单元是一个不是很好的特征转换，比如用一个不恰当的激活函数，可能会导致有用信息的丢失



##### detach方法

```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)  
y = torch.relu(x)  
d2l.plot(x.detach(), y.detach(), 'x', 'rele(x)', figsize=(5,2.5))
```

在 PyTorch 中，`detach` 方法被用来将一个 Tensor 从计算图中分离出来，使得这个 Tensor 不再具有梯度。这样做的目的主要是避免在计算或显示 Tensor 值的过程中影响到梯度的计算。

一般在神经网络的反向传播过程中，每个 Tensor 都会在计算图中记录它的操作历史，以便于在计算梯度时能追踪到全部计算过程。但有些情况下，我们只想获取 Tensor 的值而不需要梯度信息，也不希望这个过程对之前的计算历史造成影响，这时就可以使用 `detach` 方法。


##### requires_grad参数

```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)  
y = torch.relu(x)  
d2l.plot(x.detach(), y.detach(), 'x', 'rele(x)', figsize=(5,2.5))
```

requires_grad属性用于指定一个tensor是否需要计算梯度，当设置为True的时候，pytorch会自动跟踪和记录tensor上的所有操作，以便后续进行自动梯度计算（自动微分）

在使用 PyTorch 的 `DataLoader` 时，通常是将数据加载为 Tensor，但这些数据 Tensor 默认是不需要计算梯度的，即它们的 `requires_grad` 属性默认为 False。这是因为数据本身（如输入特征和目标标签）通常不是需要优化的参数。

然而，当这些数据通过神经网络时，网络中的参数（权重和偏置）应该有 `requires_grad=True`，这样在前向传播后，可以在反向传播过程中自动计算这些参数的梯度。换句话说，**网络参数需要梯度以便优化，而数据不需要**。


##### zero_() 

```python
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
```
`zero_()` 是一个原地操作（in-place operation），意味着它会直接修改 `x.grad` 而不是创建一个新的 Tensor。会将x的梯度清零，在新版本中直接使用`x.grad.zero_()`是更加常见和推荐的


##### backward()

```python
x.grad.data.zero_()
y.backward(torch.ones_like(x),retain_graph=True)
```
这个操作是计算y的梯度，如果y是一个标量，则不需要提供任何参数，如果y不是标量，就需要传递一个跟y一样形状的tensor作为参数，这个参数通常表示外部梯度

##### ones_like()

创建了一个和x形状相同，所有元素为1的tensor，类似于对y中每个元素求导

###### 为什么类似于求导

在 PyTorch 中，当使用 y.backward(torch.ones_like(x)) 这种形式的时候，实际上是在指定 y 中每个元素相对于 x 的梯度为 1。这是因为 y.backward() 函数在计算梯度时，需要一个与 y 形状相同的 Tensor 来表示 y 中每个元素的外部梯度。

具体来说：

- 当 y 是一个标量（只包含一个元素的 Tensor）时，调用 y.backward() 会计算 y 相对于 x 的梯度。在这种情况下，不需要指定外部梯度，因为标量函数的梯度是直接计算的。

- 当 y 是一个非标量（包含多个元素的 Tensor）时，PyTorch 不能直接计算梯度，因为它需要知道每个元素的梯度应该如何累积。在这种情况下，torch.ones_like(x) 就起到了指定外部梯度的作用。这个 Tensor 的每个元素都是 1，它告诉 PyTorch 对于 y 中的每个元素，其相对于 x 的梯度应该是 1。

因此，使用 torch.ones_like(x) 相当于是告诉 PyTorch 对 y 中的每个元素进行求导，并且假设这些导数的初始值是 1。这就是为什么说 torch.ones_like(x) 相当于对 y 中每个元素求导。

###### y是非标量的情况下 反向传播的内容 偏导数

当 `y` 是一个非标量（即包含多个元素的 Tensor）时，对其进行求导需要考虑每个元素相对于输入 `x` 的梯度。在这种情况下，我们通常关心的是 `y` 中所有元素的总和对 `x` 的梯度。这是因为在神经网络中，我们常常对损失函数（一个标量值）关于网络参数的梯度感兴趣。

假设 `y` 是由输入 `x` 通过某个函数（比如神经网络层）计算得到的，其中 `x` 和 `y` 都是向量，且 `y = [y_1, y_2, ..., y_n]`。我们想要计算的是总和 `sum(y)` 对 `x` 的梯度。

总和 `sum(y)` 可以表示为：

$$ \text{sum}(y) = y_1 + y_2 + \cdots + y_n $$

对 `x` 中的每个元素 `x_i` 的梯度是 `sum(y)` 对 `x_i` 的偏导数。根据链式法则，这可以表示为：

$$ \frac{\partial \text{sum}(y)}{\partial x_i} = \frac{\partial y_1}{\partial x_i} + \frac{\partial y_2}{\partial x_i} + \cdots + \frac{\partial y_n}{\partial x_i} $$

这里，`∂y_j/∂x_i` 表示 `y` 的第 `j` 个元素相对于 `x` 的第 `i` 个元素的偏导数。

当我们调用 `y.backward(torch.ones_like(y))` 时，这相当于对每个 `y_j` 都施加了一个外部梯度 1。这意味着我们计算的是每个 `y_j` 对 `x` 的梯度，并将这些梯度相加，从而得到 `sum(y)` 对 `x` 的总梯度。因此，`torch.ones_like(y)` 为每个 `y_j` 提供了单位外部梯度，这使得我们可以计算总和 `sum(y)` 对 `x` 的梯度。

##### retain_graph=True

计算梯度的时候，默认pytorch会消耗计算图，如果想在同一计算图上进行多次后向传播，需要设置为True，告诉pytorch保留计算图，大多数情况下，一个batch数据上进行一次反向传播的时候，不需要设置retain_graph=True

