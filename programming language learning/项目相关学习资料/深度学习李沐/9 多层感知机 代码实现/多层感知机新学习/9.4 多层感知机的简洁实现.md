
我们在这里添加了2个全连接层,linear层
我们从0开始设置参数是可以调用hiddens层两次,这里用API就是需要写两层
```python
import torch  
from torch import nn  
from d2l import torch as d2l  
  
net = nn.Sequential(nn.Flatten(), nn.Linear(784,256), nn.ReLU(), nn.Linear(256, 10))  
  
def init_weights(m):  
    if type(m) == nn.Linear:  
        nn.init.normal_(m.weight, std=0)  
  
net.apply(init_weights)  
  
batch_size, lr, num_epochs = 256, 0.1, 10  
loss = nn.CrossEntropyLoss()  
trainer = torch.optim.SGD(net.parameters(), lr=lr)  
  
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)  
d2l.plt.show()
```

## 小结

- 使用高级API实现全连接层非常简单,只用采取Linear就可以完成主体,其他的初始化和损失函数,还有激活函数都是固定操作
- 对于相同分类问题,softmax和mlp的实现是相同的,只是多层感知机中间添加了带有激活函数的隐藏层


## 练习

1. 尝试添加不同数量的隐藏层（也可以修改学习率），怎么样设置效果最好？
2. 尝试不同的激活函数，哪个效果最好？
3. 尝试不同的方案来初始化权重，什么方法效果最好？

1. 添加不同数量的隐藏层：添加隐藏层的数量和修改学习率对模型性能有显著影响。通常，更多的隐藏层可以增加模型的表达能力，但也可能导致训练困难和过拟合。最佳设置取决于具体的任务和数据。一般来说，可以从少量的隐藏层开始，逐渐增加，同时观察验证集上的性能。对于学习率，通常需要进行多次实验来找到最佳值。一个好的起点是从一个较小的学习率开始，例如0.01或0.001，然后根据模型的表现进行调整。

2. 尝试不同的激活函数：常见的激活函数包括ReLU（线性整流单元）、sigmoid、tanh等。ReLU通常是首选，因为它在训练深层网络时有助于缓解梯度消失问题，同时计算效率高。然而，根据具体任务的不同，其他激活函数（如Leaky ReLU、ELU等）也可能表现更好。实验不同的激活函数，并观察哪个在验证数据上表现最好，可以帮助确定最适合当前任务的激活函数。

3. 尝试不同的权重初始化方案：权重初始化对深度学习模型的训练非常重要。常见的初始化方法包括随机初始化（如正态分布或均匀分布），Xavier/Glorot 初始化，和He初始化。He初始化通常用于ReLU激活函数，而Xavier初始化适合于tanh或sigmoid激活函数。选择最佳的初始化方法通常取决于模型架构和所选激活函数。通过试验不同的初始化方法并观察它们对模型性能的影响，可以找到最适合特定模型和任务的初始化方案。

在进行这些实验时，重要的是要保持其他所有条件不变，这样才能准确评估每个变化对性能的影响。同时，确保在验证集上评估模型性能，以避免过拟合。

