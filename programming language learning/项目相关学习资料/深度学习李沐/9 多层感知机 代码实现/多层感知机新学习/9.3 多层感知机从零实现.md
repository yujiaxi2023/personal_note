```python
import torch  
from torch import nn  
from d2l import torch as d2l  
  
batch_size = 256  
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```
初始设置都是类似的

## 初始化模型参数

Fashion-MNIST中的每个图像是784个灰度像素值决定的,所有图像10个类别,忽略空间结构,可以将每个图像视为一个输入784个特征,输出10个类的简单分类数据集

首先我们实现一个具有**单隐藏层的多层感知机multilayer perception,包含有256个隐藏单元** 我们可以将这两个变量都视为超参数, 通常我们选择2的若干次幂作为层的宽度. 因为**内存在硬件中的分配和寻址方式**, 这样做往往可以在计算上更加高效

##### 内存分配和寻址方式
在计算机系统中，内存的分配和寻址方式对计算效率有重要影响。当我们提到以2的若干次幂作为神经网络层的宽度时，其背后的原因与硬件的内存管理和数据访问模式有关。以下是一些关键点来理解这一点：

- 内存对齐（Memory Alignment）：现代计算机系统中的内存分配通常是按照一定的字节块进行的，这些字节块往往是2的幂次大小。当数据结构（如神经网络层的权重矩阵）的大小符合这些字节块的大小时，内存访问可以更加高效。这是因为内存对齐可以减少处理器访问内存所需的时间。

- 缓存行（Cache Lines）：计算机的缓存系统是为了减少从主内存访问数据的时间而设计的。缓存中的数据是以缓存行为单位存储的，而这些缓存行的大小也通常是2的幂次大小。当数据结构的大小与缓存行的大小匹配时，可以减少缓存未命中（cache misses），从而提高效率。

- 硬件优化：许多现代处理器（如CPU和GPU）在执行向量化操作时，对2的幂次的数据大小进行了优化。这意味着当数据大小是2的幂次时，处理器可以更有效地进行并行处理，提高计算速度。

- 内存寻址：计算机的内存寻址通常也是基于2的幂次进行的。当数据结构的大小是2的幂次时，它更容易被系统有效地寻址和处理。


我们使用几个tensor来表示参数, 每一层都需要记录一个权重矩阵和一个偏置向量. 我们要为损失关于这些参数的梯度(求导, 反向就是求积分)分配内存
```python
num_inputs, num_outputs, num_hiddens = 784, 10, 256  
  
W1 = nn.Parameter(  
    torch.randn(num_inputs, num_hiddens, requires_grad=True)  
)  
  
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))  
  
W2 = nn.Parameter(  
    torch.randn(num_hiddens, num_outputs, requires_grad=True)  
)  
  
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))  
  
params = [W1, b1, W2, b2]
```
这里的requires_grad就是分配内存存储前向传播的梯度

## 激活函数

为了确保对细节了解, 这里实现ReLU激活函数
```python
# 实现激活ReLU函数  
def relu(X):  
    a = torch.zeros_like(X) # 设置形状一样  
    return torch.max(X, a)
```

## 模型

忽略了空间结构的情况下,使用reshape将每个二维图转换为长度为`num_inputs`的向量
```python
# 实现模型  
def net(X):  
    X = X.reshape((-1, num_inputs)) # 拉成二维矩阵，-1代表batchsize，因为是自动输入  
    H = relu(X @ W1 + b1) # 创建W的时候是786x256  
    return (H @ W2 + b2)
```

## 损失函数

因为从0实现过softmax,所以直接使用API调用交叉熵损失函数
```python
loss = nn.CrossEntropyLoss()
```

## 训练

跟softmax完全一样
```python
# 感知机的训练过程是完全一样的  
num_epochs, lr = 10, 0.1  
updater = torch.optim.SGD(params, lr=lr)  
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)  
d2l.plt.show()  
# 可以比较跟softmax的区别
```

## 小结

- 手动实现一个简单的多层感知机比较简单,如果从0开始实现大量的多层感知机比较困难,因为你需要设置和命名记录模型的参数


## 练习

1. 在所有其他参数保持不变的情况下，更改超参数`num_hiddens`的值，并查看此超参数的变化对结果有何影响。确定此超参数的最佳值。
2. 尝试添加更多的隐藏层，并查看它对结果有何影响。
3. 改变学习速率会如何影响结果？保持模型架构和其他超参数（包括轮数）不变，学习率设置为多少会带来最好的结果？
4. 通过对所有超参数（学习率、轮数、隐藏层数、每层的隐藏单元数）进行联合优化，可以得到的最佳结果是什么？
5. 描述为什么涉及多个超参数更具挑战性。
6. 如果想要构建多个超参数的搜索方法，请想出一个聪明的策略。


1. 更改num_hiddens对结果的影响：num_hiddens是指隐藏层中的单元数，更改这个值会影响模型的容量。较小的num_hiddens可能导致模型学习不足（欠拟合），而较大的值可能导致过拟合。确定最佳值通常需要通过试验和错误的方法，观察不同值对验证集上的性能影响。

2. 添加更多隐藏层的影响：增加隐藏层数量可以增加模型的复杂度和学习能力。然而，更多的层也可能导致训练困难，比如梯度消失或爆炸。实验中需要平衡模型的深度和训练的稳定性。

3. 改变学习速率的影响：学习速率对优化过程至关重要。过高的学习速率可能导致训练不稳定，而过低的学习速率会导致训练速度慢。找到最佳学习速率需要通过试验，观察不同学习速率对训练过程和最终性能的影响。

4. 超参数联合优化：通过对所有超参数（学习率、轮数、隐藏层数、每层的隐藏单元数）进行联合优化，可以找到模型的最佳配置。这通常涉及到大量的实验和调整，使用方法如网格搜索或随机搜索来系统地探索超参数空间。

5. 多个超参数更具挑战性的原因：涉及多个超参数时，优化变得更加复杂。每个超参数都可能影响其他超参数的最佳值。此外，超参数空间是高维的，可能存在大量的局部最优解，这使得找到全局最优解变得困难。

6. 构建多个超参数的搜索方法：有效的策略包括网格搜索（系统地探索所有可能的超参数组合），随机搜索（随机选取超参数组合进行试验），以及基于贝叶斯优化的方法（利用先前的结果来指导后续的搜索）。一种聪明的策略可能是先用随机搜索找到一个合理的参数范围，然后用更细粒度的网格搜索在这个范围内进行优化。同时，可以使用一些智能的方法比如早停（early stopping）来减少资源的消耗。


