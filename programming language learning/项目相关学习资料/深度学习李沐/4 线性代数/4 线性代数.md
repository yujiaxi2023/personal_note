# 标量

- ### 简单操作

```
c = a + b
c = a * b
c = sina
```

- ### 长度

![[Pasted image 20230921000738.png]]

# 向量

- ### 简单操作

![[Pasted image 20230921000823.png]]

- ### 长度

![[Pasted image 20230921000854.png]]

![[Pasted image 20230921000925.png]]

- ### 点乘
- ### 正交
![[Pasted image 20230921001023.png]]

# 矩阵

- ### 简单操作
![[Pasted image 20230921001048.png]]

- ### 乘法

![[Pasted image 20230921001116.png]]

![[Pasted image 20230921001144.png]]

矩阵乘法类似于将空间扭曲

![[Pasted image 20230921001234.png]]

- ### 范数

![[Pasted image 20230921001257.png]]

- ### 常见范数
![[Pasted image 20230921001405.png]]

![[Pasted image 20230921001429.png]]

![[Pasted image 20230921001457.png]]

![[Pasted image 20230921001536.png]]

剩下的内容在jupyter笔记中

sum操作中有参数axis按照维度进行sum操作
还有keepdims的操作
![[Pasted image 20230921004211.png]]
按照轴来理解，从什么轴计算，形状中就会去掉什么轴

# 问题

**问题1：稀疏矩阵是为了节约内存**
**问题2：深度学习使用张量表示的原因？因为机器学习是类似统计问题**
**问题3：copy和clone的区别？copy可能不copy内存，深度和浅层的区别，clone是一定有关内存**
**问题4：对什么维度求和就等于去除一个维度，或者说对某一维度进行操作的过程就是让这一个维度的很多值reduce成一个值**
**问题5：torch是否区分行向量列向量？1维张量一定是行向量，列向量一定在矩阵中存在**
**问题6：sum（axis=【0，1】）如何求？就是拍平操作**
**问题7：通常nlp中会使用embedding 的 layer 解决稀疏问题**
