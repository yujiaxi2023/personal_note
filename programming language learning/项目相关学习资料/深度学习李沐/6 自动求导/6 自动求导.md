# 向量链式法则

- ### 标量链式法则

![[Pasted image 20230921153021.png]]

- ### 拓展到向量

![[Pasted image 20230921153030.png]]
![[Pasted image 20230921153110.png]]
1. 当y是标量 x是n维向量 结果是 1xn的
2. 当u是k维度向量，结果还是1xn
3. 当全为向量，结果还是不变的，因为中间过程计算时候消失了

# 例子

![[Pasted image 20230921153627.png]]

![[Pasted image 20230921155256.png]]
X是mxn矩阵

# 自动求导

- ### 自动求导计算一个函数在指定值上的导数
- ### 区别于
![[Pasted image 20230921155558.png]]

数值求导是用数值拟合这个导数

# 计算图

![[Pasted image 20230921155724.png]]

![[Pasted image 20230921155847.png]]

![[Pasted image 20230921155909.png]]

![[Pasted image 20230921155945.png]]

![[Pasted image 20230921160115.png]]

![[Pasted image 20230921160159.png]]

![[Pasted image 20230921160216.png]]

**问题1：隐式构造和显示构造的区别？显示构造是先给公式再给值**
**问题2：在share weight的时候累计梯度有好处**
**问题3：深度学习loss一般是一个标量，如果是矩阵就没法算**
**问题4：需要梯度的时候才会求backward**
