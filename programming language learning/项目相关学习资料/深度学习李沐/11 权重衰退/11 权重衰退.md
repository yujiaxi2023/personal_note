weight decay是最常见处理过拟合的方法

# 用均方范数作为硬性限制

- 通过限制参数值的选择范围来控制模型容量
限制w的L2 long小于一个特定的值
![[Pasted image 20230928180448.png]]
![[Pasted image 20230928180738.png]]
	- 通常不限制偏移b（因为不限制也一样）
	- 小的θ意味着更强的正则项

# 使用均方范数作为柔性限制

- 对于每个θ，都可以找到λ使得前面的目标函数等价于下面
![[Pasted image 20230928184016.png]]
- 可以通过拉格朗日乘子来证明
- 超参数λ控制了正则项的重要程度
	- λ=0：无作用
	- ![[Pasted image 20230928184105.png]]

![[Pasted image 20230928184854.png]]
loss找到一个最低点，但是里真实的最低点还有比较大的距离的时候
如果需要把这个最低点向最优解移动，l会变大，但是正则化项会减小的更多，也就是惩罚更多，那就会向着最优解移动直到罚无法弥补loss的增加


# 参数更新法则

- 计算梯度

![[Pasted image 20230928185022.png]]

- 时间t更新参数

![[Pasted image 20230928185209.png]]

- 通常ηλ<1，再深度学习中叫权重衰退


# 总结

- 权重衰退通过L2正则项使得模型参数不会过大，从而控制模型复杂度
- 正则项权重是控制模型复杂度的超参数

数据越简单，模型越复杂越容易过拟合


**问题1：如果使用L1 penalty会得到更小的gap在train 和 test之间
问题2：权重衰退对模型复杂度控制的效果并不是非常良好
问题3：范数代表的是下标，对于向量来说L2是标准范数
![[Pasted image 20230928194814.png]]
这里2是平方，蓝色的才代表范数
问题4：为什么要把w向小的拉，如果最优解W就是比较大的数的情况怎么办？
![[Pasted image 20230928194929.png]]
假设w~是最优点，但是因为有噪音，所以会学到圈外
![[Pasted image 20230928195024.png]]
因为模型是尝试记住所有的东西，所以会记住噪音，所以会相对于最小值产生偏移
如果λ过小，会拉到
![[Pasted image 20230928195143.png]]
如果λ太大
![[Pasted image 20230928195155.png]]
问题5：weight decay有1e-2 1e-3 1e-1等选择
问题6：用验证集选择超参数可以人为提升精度
问题7：