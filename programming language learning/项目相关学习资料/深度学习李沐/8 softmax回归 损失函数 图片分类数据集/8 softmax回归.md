# 回归vs分类

- ### 回归估计一个连续值
- ### 分类预测一个离散类别
- ### MNIST & ImageNet

![[Pasted image 20231102193759.png]]



# Kaggle的分类问题

### 分类人类蛋白质18类
![[Pasted image 20230921182609.png]]
### 将恶意软件进行分类9类
![[Pasted image 20230921182627.png]]
### 分类wikipedia评论
![[Pasted image 20230921182650.png]]

# 回归到多类别分类

![[Pasted image 20230921182718.png]]

## 均方损失

- 对类别进行有效编码
- 使用均方损失训练
- 最大值预测
![[Pasted image 20230921182859.png]]

## 无校验比例

![[Pasted image 20230921183017.png]]
并不关心实际值，是在意正确类型的置信度是否是特别大的
要设置oy正确类和错误类的差距超过一定的置信度

## 校验比例

![[Pasted image 20230921183300.png]]

使用softmax算子作用在o上

![[Pasted image 20230921183657.png]]

![[Pasted image 20230921183758.png]]


# 实现softmax

如果要使用softmax，输入需要一个向量，但是这种会损失很多空间信息

**问题1：softlabel策略，是因为softmax是很难通过指数逼近1的结果，所以label可以从0和1变为0.1和0.9**
**问题2：logistics regression和softmax回归是logistics regression的特例，只用得到0和1其中一项结果就行
问题3：使用交叉熵，不用相对熵，互信息的原因和使用L1距离L2距离等都是因为方便计算，而且深度学习关注的是预测分布和标签分布的距离，所以就是选择最简单的方式获得距离最小值就是有效的
问题4：关心正确的类是因为独热编码中正确标签是1，在softlabel的情况下会关心非正确的类
问题5：对于每个类别中，只要有足够的训练数据，就代表有用，不需要在意是否平衡
问题6：如果是L2，就是远端拉到最低点很快，但是近端就很慢，如果是L1就是均匀的拉到最低点
问题7：最小化损失等于最大化似然函数
问题8：做ranking的时候，只关心相对值，softmax也是可以用的，而且区别比较小**
**问题9：dataloader的num workers是并行的，在后台开多个python运行
问题10：batchsize设置多少不会有计算量的差别，gpu上有多个核，所以计算上对并行度有要求，batchsize要设置的大一些增加并行度
问题11：cnn几年前的研究是认为学习到的是纹理而不是轮廓
