一开始导入的几个包是传统的
d2l中集成了timer和plt
```python
import torch, torchvision  
from torch.utils import data # 读取数据，小批量的函数  
from torchvision import transforms # 对数据操作的module  
from d2l import torch as d2l  
  
d2l.use_svg_display()  
# 使用svg可以让图片清晰度更高一些  
  ```

直接使用torchvision中的FashionMNIST方法来获得数据
```python
# 使用ToTensor实例将图像数据从PIL类型转换为32位浮点数格式  
# 除以255使得所有像素的数值都在0-1之间  
trans = transforms.ToTensor() # 最简单的预处理  
# train是代表是否是训练数据集，transform=这个实例化对象，也就是做转换为tensor的操作，download是网络上下载  
mnist_train = torchvision.datasets.FashionMNIST(root="../data", train=True, transform=trans, download=True)  
mnist_test = torchvision.datasets.FashionMNIST(root="../data", train=False, transform=trans, download=True)  
  
print(len(mnist_train), len(mnist_test))  
  
# 可以查找一下第一个图片tensor的shape  
  
print(mnist_train[0][0].shape)  
  
# torch.Size([1, 28, 28]) 灰度图代表channel=1在第一个维度，后边两个是长宽  

```

定义两个函数，一个用来给数据集中将标签转换为文字显示
另一个函数用来显示一个批量中的图片
```python
# 传入的是数据集中的labels，返回值是列表，因为列表是有顺序的  
def get_fashion_mnist_labels(labels):  
    """返回Fashion-MNIST数据集的文本标签"""  
    text_labels = [  
        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal',  
        'shirt', 'sneaker', 'bag', 'ankle boot'  
    ]  
  
    return [text_labels[int(i)] for i in labels]  
  
  
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  
    """Plot a list of images"""  
    figsize = (num_cols * scale, num_rows * scale)  
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)  
    axes = axes.flatten()  
    for i, (ax, img) in enumerate(zip(axes, imgs)):  
        if torch.is_tensor(img):  
            # 图片张量  
            ax.imshow(img.numpy())  
        else:  
            # PIL图片  
            ax.imshow(img)  
        ax.axes.get_xaxis().set_visible(False)  
        ax.axes.get_yaxis().set_visible(False)  
        if titles:  
            ax.set_title(titles[i])  
    return axes  
  
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))  
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))  
d2l.plt.show()  
  ```

使用进程数来读取整个数据集
  ```python
# 读取一小批量数据，大小为batch_size：  
  
batch_size = 256  
  
def get_dataloader_workers(n):  
    """使用进程数来读取数据"""  
    return n  
  
train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers(4))  
  
timer = d2l.Timer()  
for X, y in train_iter:  
    continue  
  
print(f"{timer.stop():.2f} sec")  
# 访问一次所有的batch需要的时间  
# 训练之前可以检查一下读取数据的速度，避免读取速度要慢过训练时间  
# 读取速度是bottleneck的重要影响因素  
  ```


  ```python
def load_data_fashion_mnist(batch_size, resize=None):  
    """下载Fashion-MNIST数据集，然后加载到内存里面"""  
    trans = [transforms.ToTensor()]  
    if resize:  
        trans.insert(0, transforms.Resize(resize))  
    trans = transforms.Compose(trans)  
    mnist_train = torchvision.datasets.FashionMNIST(  
        root="../data", train=True, transform=trans, download=True)  
    mnist_test = torchvision.datasets.FashionMNIST(  
        root="../data", train=False, transform=trans, download=True)  
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,num_workers=get_dataloader_workers()),  
            data.DataLoader(mnist_test, batch_size, shuffle=False,num_workers=get_dataloader_workers()))  
  
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)  
for X, y in train_iter:  
    print(X.shape, X.dtype, y.shape, y.dtype)  
    break
```

##### transforms.Resize

`transforms.Resize` 是PyTorch中的图像变换操作之一，用于调整图像的大小。它的主要作用是将输入的图像调整为指定的目标尺寸，以适应模型的输入要求或数据处理的需要。这可以在数据预处理阶段用于将图像标准化为相同的尺寸，以便进行批处理和训练。

`transforms.Resize` 可以接受不同类型的输入参数，包括：

1. 一个整数（e.g., `transforms.Resize(256)`）：此时，它将图像的较小边缩放到指定的大小，然后保持原始宽高比。例如，如果图像的原始大小为(400, 300)，将其缩放到(256, 192)。
    
2. 一个元组（e.g., `transforms.Resize((256, 256))`）：此时，它将图像缩放到指定的目标宽度和高度。这不会保持原始宽高比，导致图像变形。
    
3. 一个整数和一个插值方法（e.g., `transforms.Resize(256, interpolation=Image.BILINEAR)`）：你可以指定插值方法，用于在调整大小时进行像素插值。常见的插值方法包括双线性插值（`Image.BILINEAR`）、最近邻插值（`Image.NEAREST`）等。


### 小结

- Fashion-MNIST是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。
- 我们将高度$h$像素，宽度$w$像素图像的形状记为$h \times w$或（$h$,$w$）。
- 数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。

### 练习

1. 减少`batch_size`（如减少到1）是否会影响读取性能？
2. 数据迭代器的性能非常重要。当前的实现足够快吗？探索各种选择来改进它。
3. 查阅框架的在线API文档。还有哪些其他数据集可用？

1. **减少`batch_size`对读取性能的影响**
    
    是的，减小`batch_size`（如减少到1）可能会影响数据加载和模型训练的性能。在深度学习中，我们通常使用批量处理，因为它可以有效地利用硬件并行性，如GPU，从而提高计算效率。对于较小的`batch_size`，每个批次的数据量减少，这可能会导致GPU资源未被充分利用，从而降低训练速度。此外，较小的`batch_size`也可能影响模型的训练稳定性和收敛速度。
    
2. **改进数据迭代器的性能**
    
    当前的实现是否足够快取决于多个因素，如数据大小，硬件配置，以及你的具体需求。如果你觉得数据加载是训练过程中的瓶颈，可以考虑以下策略来改进数据迭代器的性能：
    
    - **预加载数据**：如果你的数据集足够小，可以适应内存，那么在训练前预加载所有数据到内存中可以显著减少磁盘I/O的影响。
    - **并行化数据加载**：如果你的数据集很大，不适合预加载，那么使用多线程或多进程来并行加载数据是一个好主意。在PyTorch中，可以通过设置`DataLoader`的`num_workers`参数来实现。
    - **使用更快的数据格式**：例如，使用HDF5或TFRecord格式的数据通常比使用JPEG或PNG格式的数据更快。

1. **PyTorch框架中可用的常见数据集**
    
    PyTorch提供了许多内置的数据集供你使用，这些数据集都包含在`torchvision.datasets`模块中。以下是一些常见的数据集：
    
    - **MNIST**：这是一个手写数字识别的数据集，包含60,000个训练样本和10,000个测试样本。
    - **CIFAR10/CIFAR100**：这些数据集分别包含10类和100类的32x32像素的彩色图像。
    - **ImageNet**：这是一个大规模的视觉对象识别的数据集，包含超过1400万幅图像，涵盖超过20000个类别。
    - **COCO**：这是一个大规模的物体检测，分割和标注数据集，包含超过200,000个标注的图像。
    
    你可以通过查看[PyTorch的在线API文档](https://pytorch.org/vision/stable/datasets.html)获取更多关于这些数据集的信息。


##### Dataloader

Dataloader是深度学习中常用的数据加载模块，用于读取和预处理训练或测试数据。它主要有两个功能：数据的加载和数据的处理，以及将数据组织成可以喂给模型的格式。

在解释Dataloader的功能和使用方法之前，我们需要了解一些相关的概念。

1. 数据集（Dataset）：数据集是具体的数据来源，可以是图像、文本或其他形式的数据。在使用Dataloader之前，我们需要首先创建一个数据集对象。
    
2. 批次（Batch）：批次是指从数据集中一次性读取的一组样本。批次的大小可以根据需求进行设置，较大的批次可以提高训练效率，但会占用更多的内存。
    
3. 数据预处理（Data preprocessing）：数据预处理是对原始数据进行一系列操作，使其适合模型的输入。例如，对图像数据进行缩放、裁剪或标准化。
    

现在我们来介绍Dataloader的功能和使用方法：

1. 导入所需的库和模块：
```python
import torch
from torch.utils.data import DataLoader, Dataset
```

2. 创建自定义数据集类（Dataset）：

首先，我们需要创建一个自定义的数据集类，继承自torch.utils.data.Dataset。在该类中，我们需要实现两个主要方法： * `__len__`：返回数据集的大小（即样本数量） * `__getitem__`：根据索引返回一个样本

```python
class CustomDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        sample = self.data[index]
        # 对样本进行预处理操作
        # 返回处理后的样本
        return sample
```

3. 创建数据集对象：

使用定义好的数据集类，创建数据集对象，并传入具体的数据。

```python
data = [...]  # 具体的数据
dataset = CustomDataset(data)
```

4. 创建数据加载器对象（Dataloader）：

使用torch.utils.data.DataLoader类创建数据加载器对象。在创建时，我们需要指定数据集对象、批次大小和其他参数。

```python
batch_size = 32
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
```

其中，shuffle参数用于在每个epoch开始时是否对数据进行洗牌，以增加数据的随机性。

5. 迭代数据集：

使用for循环来迭代数据集，这样可以逐个获取批次的数据。

```python
for batch in dataloader:
    # 对批次数据进行处理或喂给模型
    # ...
```

6. 数据处理与模型训练：

在上述循环中，可以使用批次数据进行数据处理操作，例如输入模型进行训练、验证或测试。

综上，Dataloader的功能是将数据加载和处理封装成一个迭代器，提供了更加便捷的数据操作方式。它的使用方法包括创建自定义数据集类、创建数据集对象、创建数据加载器对象以及通过迭代器获取批次的数据。

##### DataLoader常用的参数

torch.utils.data.DataLoader类有一些常用的参数，可以根据需要进行配置。以下是这些参数及其功能的详细解释：

dataset（必需参数）：指定要加载的数据集对象。

batch_size（可选参数，默认值为1）：指定每个批次的样本数量。较大的批次大小可以提高训练效率，但会占用更多的内存。

shuffle（可选参数，默认值为False）：指定在每个epoch开始时是否对数据进行洗牌。洗牌可以增加数据的随机性，有助于提高模型的泛化能力。

sampler（可选参数）：指定数据样本的抽样方法，用于定义样本的选取策略。例如，可以使用torch.utils.data.RandomSampler实现随机抽样。

batch_sampler（可选参数）：指定批次样本的抽样方法，用于定义批次样本的选取策略。可以与sampler参数一起使用。

num_workers（可选参数，默认值为0）：指定用于数据加载的子进程数。通过使用多个子进程，可以加快数据加载的速度。

pin_memory（可选参数，默认值为False）：指定是否将数据加载到固定的内存位置上，即锁页内存。锁页内存可以加速数据传输，适用于GPU计算。

drop_last（可选参数，默认值为False）：指定是否丢弃最后一个批次，如果该批次的大小小于批次大小。如果设置为True，则会丢弃最后一个不完整的批次。

timeout（可选参数，默认值为0）：指定数据加载的超时时间（以秒为单位）。如果数据加载超时，则会引发一个异常。

worker_init_fn（可选参数）：指定进程初始化函数的处理逻辑。可以使用该函数来初始化子进程的环境，例如设置随机数种子。

这些参数可用于灵活地配置Dataloader对象，以满足不同场景下的需求。根据具体的使用情况，可以选择使用适当的参数组合。

##### DataLoader迭代器

封装的迭代器是指通过torch.utils.data.DataLoader创建的对象，它可以迭代地生成批次的数据，以便进行模型训练或测试。Dataloader对象作为一个迭代器，具有以下几个重要的属性：

dataset：Dataloader对象关联的数据集对象。可以通过dataloader.dataset访问。

batch_size：每个批次的样本数量。可以通过dataloader.batch_size访问。

shuffle：每个epoch开始时是否对数据进行洗牌。可以通过dataloader.shuffle访问。

sampler：数据样本的抽样方法。可以通过dataloader.sampler访问。

batch_sampler：批次样本的抽样方法。可以通过dataloader.batch_sampler访问。

num_workers：用于数据加载的子进程数。可以通过dataloader.num_workers访问。

pin_memory：数据是否加载到固定的内存位置上。可以通过dataloader.pin_memory访问。

drop_last：是否丢弃最后一个批次，如果该批次的大小小于批次大小。可以通过dataloader.drop_last访问。

timeout：数据加载的超时时间（以秒为单位）。可以通过dataloader.timeout访问。

worker_init_fn：进程初始化函数的处理逻辑。可以通过dataloader.worker_init_fn访问。

除了以上属性，Dataloader对象还具有一些常用的方法，例如__iter__和__len__。其中， * __iter__方法使Dataloader对象成为一个迭代器，可以使用for循环来迭代数据集，逐个获取批次的数据。 * __len__方法返回Dataloader对象中的批次数量。

通过使用这些属性和方法，我们可以方便地进行数据加载和处理操作，将数据高效地喂给深度学习模型。