```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```


pytorch框架中的高级API, 是为了简化深度学习模型开发过程设计的抽象层, 这些API提供了构建, 训练和测试模型所需要的高级构建块, 这让用户可以用更加直观和简洁的方式来实现复杂的功能. 一些例子包括

1. **模型构建API** (`torch.nn`)：提供了构建神经网络所需的所有基础组件，如层（全连接层、卷积层等）、激活函数、损失函数等。通过这些高级构件，用户可以很容易地定义复杂的网络结构。
    
2. **数据加载和处理API** (`torch.utils.data`)：提供了数据处理工具，例如数据加载器（`DataLoader`），它可以批量加载数据，提供多线程处理和数据打乱等功能，以及数据集抽象（`Dataset`），允许定义和操作自定义数据集。
    
3. **优化器API** (`torch.optim`)：包含了许多用于网络训练的优化算法，如SGD、Adam等。这些API帮助用户轻松地应用和管理这些算法，以更新模型的参数。
    
4. **预训练模型和模型组件** (`torchvision.models`, `torch.hub`)：PyTorch提供了一系列预训练的模型和模型组件，允许用户直接加载并使用这些模型，或者根据需要进行微调。
    
5. **自动微分API** (`torch.autograd`)：这是PyTorch的一个核心特性，它允许用户自动计算模型参数的梯度。这极大地简化了梯度下降等优化算法的实现。


在计算机科学和软件工程中, "抽象"是一种设计原则, 旨在隐藏系统的复杂性, 同时只暴露关键的操作和数据.

### 初始化模型参数

因为softmax回归的输出层是一个全连接层,因此,为了实现我们的模型,我们只需要在Sequential中添加一个带有10个输出的全连接层,这里Sequential并不是必要的东西,但是一般深度学习都会使用它.
我们使用0的均值,0.01的标准差随机初始化权重
```python
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```
`nn.Sequential` 是 PyTorch 中的一个容器模块，它按照顺序包含了一系列子模块（如神经网络层），并自动实现了前向传播。使用 `nn.Sequential` 可以使模型的创建更加简单、结构更加清晰。

`Sequential`中有一个`apply`方法,会对网络中所有模块递归的应用一个函数
`weight`是`nn.Linear`中的一个属性,代表该层的权重矩阵,这里意思是`[10,784]`的矩阵,并且使用`normal_(m.weight,std=0.01)`其中mean默认是0

#### 矩阵乘法

在线性代数中，两个矩阵进行乘法运算需要满足特定的条件。具体来说，如果我们要将一个矩阵 A 乘以另一个矩阵 B，那么矩阵 A 的列数必须与矩阵 B 的行数相等。

假设矩阵 A 的形状为 m x n（表示 A 有 m 行和 n 列），矩阵 B 的形状为 p x q（表示 B 有 p 行和 q 列），那么我们只有当 n 等于 p 时才能进行矩阵乘法。换句话说，左矩阵的列数需要等于右矩阵的行数。

乘法结果将是一个新的矩阵，其形状为 m x q。也就是说，结果矩阵的行数等于左矩阵的行数，结果矩阵的列数等于右矩阵的列数。

在你之前的问题中，我们有一个 1x784 的输入矩阵（向量）和一个 784x10 的权重矩阵。这两个矩阵可以进行乘法运算，因为输入矩阵的列数（784）等于权重矩阵的行数（784）。乘法的结果是一个 1x10 的输出矩阵（向量）。


当你使用 `nn.Sequential` 时，你只需按照网络层或组件应该被执行的顺序将它们传递给 `nn.Sequential`。在执行前向传播时，PyTorch 会自动按照这个顺序一个接一个地执行这些层，并将每一层的输出作为下一层的输入。这样就避免了在模型的 `forward` 方法中手动编写一系列层次传递的代码。

这是 `nn.Sequential` 的一个基本示例：

```python
# 导入PyTorch的nn模块
import torch.nn as nn

# 使用nn.Sequential定义一个简单的前馈神经网络
model = nn.Sequential(
    nn.Linear(in_features=784, out_features=128),
    nn.ReLU(),
    nn.Linear(in_features=128, out_features=10)
)
```
在这个例子中，输入数据首先通过一个有 784 个输入特征和 128 个输出特征的全连接层，然后是一个 ReLU 激活函数，最后是一个将这 128 个特征映射到 10 个输出特征（例如，用于10类分类任务）的全连接层。

使用 `nn.Sequential` 的好处包括：

- **简化代码**：它减少了模型构造的复杂性，使得代码更加简洁易读。
- **模块化**：它使得各个部分可以模块化，易于理解和维护。
- **重用性**：通过 `nn.Sequential` 构建的模型或模型的部分可以很容易地在不同的项目中复用。

因此，`nn.Sequential` 可以被认为是构建深度学习模型的基础，它提供了一种快速、高效的方式来定义一系列层，这在实践中是非常常见和有用的。尽管 `nn.Sequential` 提供了很多便利，但它也有局限性，比如它不支持模型中有分支的情况，这种情况下需要定义更复杂的 `forward` 方法。

### 重新看待Softmax的实现

在从零开始实现softmax的例子时候, 我们计算了模型的输出, 然后将这个输出送到交叉熵损失中, 这在数学中是完全合理的事情. 从计算角度来看, 指数可能产生数值稳定性的问题.

$\hat{y_j} = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$  其中 $\hat{y_j}$ 是预测的概率分布 $o_j$ 是未规范化的预测 $o$ 的第 $j$ 个元素. 如果 $o_k$ 中的一些数值非常大, 那么 $exp(o_k)$ 可能大于数据类型容许的最大数字, 即上溢(overflow), 这将使得分母或者分子变为`inf`,最后得到的是0, `inf` `nan` (不是数字) 的 $\hat{y_j}$ 在这些情况下, 我们无法得到一个明确定义的交叉熵值

在计算机程序中，每种数据类型都有其能表示的最小和最大值的范围，这是由存储该数据的内存大小决定的。当尝试存储超过这个范围的值时，就会发生溢出（overflow）。

##### overflow 上溢

在Python中，整数（int）类型不会发生溢出，因为Python会根据需要动态分配内存来存储整数。这意味着，只要你的计算机有足够的内存，你就可以创建和操作任意大小的整数。但是，对于浮点数（float），Python使用的是固定的内存大小（通常是64位），所以它有一个最大值和最小值。当尝试创建或计算超过这个范围的浮点数时，Python不会报错，而是会返回一个特殊的值，称为`inf`（无穷大）。

以下是一个Python溢出的例子：

```python
# 创建一个非常大的浮点数
x = 1.79e308
print(x)  # prints: 1.79e+308

# 尝试将这个数乘以10
y = x * 10
print(y)  # prints: inf
```

在这个例子中，`1.79e308`是Python浮点数的最大值。当我们试图将它乘以10时，结果超过了浮点数能够表示的最大值，所以Python返回了`inf`。

需要注意的是，不同的编程语言可能处理溢出的方式不同。例如，一些语言（如C和Java）的整数类型有固定的大小，所以它们可能会在整数溢出时产生错误或返回不可预见的结果。

不是所有的数据类型都会固定一个内存大小。这主要取决于编程语言的设计以及特定的数据类型。

在Python中，整数（int）的大小是动态调整的，这意味着Python会根据需要分配更多的内存来存储大的整数。然而，浮点数（float）在Python中通常使用64位内存进行存储，这意味着它们有一个最大值和最小值。

在其他编程语言中，比如C，每种数据类型都有固定的大小。例如，int通常是32位（但这取决于系统和编译器），short通常是16位，long long通常是64位，等等。这些类型都有一个确定的最大值和最小值，如果尝试存储超出这个范围的值，就会发生溢出。

值得注意的是，在一些语言中（如Java），有些特定的数据类型，比如BigInteger和BigDecimal，可以存储任意大小的整数和小数，只要内存允许。

所以，是否为所有数据类型固定内存大小，以及具体的大小，主要取决于你正在使用的编程语言以及特定的数据类型。



解决这个问题的一个技巧是: 在继续softmax计算之前, 先从所有的 $o_k$ 中减去 $max(o_k)$ , 这里可以看到每个 $o_k$ 按照常数进行的移动不会改变 softmax的返回值

$$
\begin{aligned}
\hat y_j & =  \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.
\end{aligned}
$$

这个操作是在计算softmax函数时为防止数值溢出而进行的一种技巧，有时被称为“softmax的数值稳定版本”。主要的思想是在计算指数函数时，通过减去最大的logit（即$\max(o_k)$）来防止数值上溢。

让我们来详细看看这个等式：

$$  
\begin{aligned}  
\hat y_j & = \frac{\exp(o_j - \max(o_k))\exp(\max(o_k))}{\sum_k \exp(o_k - \max(o_k))\exp(\max(o_k))} \\  
& = \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}.  
\end{aligned}  
$$

在第一步中，我们减去了最大的logit（$o_k$）并同时在分子和分母中加上了相同的值。由于指数函数的性质，$\exp(a+b) = \exp(a)\exp(b)$，所以$\exp(o_j - \max(o_k))\exp(\max(o_k))$就简化为$\exp(o_j)$，同样，分母中的每一项也都可以简化为$\exp(o_k)$。

然后，在第二步中，我们可以看到$\exp(\max(o_k))$在分子和分母中都被取消了。

因此，这个操作并不会改变softmax函数的输出，但它确实可以提高数值稳定性，防止在计算过程中出现上溢或下溢。

##### logit逻辑函数的逆函数

在机器学习和统计学中，"logit"函数指的是逻辑函数的逆函数，它通常用于表示一个事件的发生概率与不发生概率的对数比。在深度学习中，特别是在分类问题中，"logit"一词经常被用来指代神经网络最后一层（通常是一个线性层）的输出，即在应用softmax激活函数之前的原始预测值。

例如，在一个多分类问题中，假设你有一个模型预测三个类别，最后一个全连接层（线性层）可能会输出三个值，这三个值表示每个类别的logit。softmax函数随后被应用于这些logit，将它们转换成概率，这些概率的和等于1。

简单地说，logit是在概率（在0和1之间）和R（实数线上的任意值）之间进行转换的一种方式。在逻辑回归模型中，这种转换是通过sigmoid函数的逆函数完成的，即logit函数，而在softmax多分类中，logit是指应用softmax之前的值。

也就是未规范化的输出预测数据



在减法和规范化步骤后, 可能有些$o_j - \max(o_k)$具有较大的负值。
由于精度受限，$\exp(o_j - \max(o_k))$将有接近零的值，即*下溢*（underflow）。
这些值可能会四舍五入为零，使$\hat y_j$为零，
并且使得$\log(\hat y_j)$的值为`-inf`。
反向传播几步后，求积分会需要放到分母, 我们可能会发现自己面对一屏幕可怕的`nan`结果。

尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。
通过将softmax和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。
如下面的等式所示，我们避免计算$\exp(o_j - \max(o_k))$，
而可以直接使用$o_j - \max(o_k)$，因为$\log(\exp(\cdot))$被抵消了。

$$
\begin{aligned}
\log{(\hat y_j)} & = \log\left( \frac{\exp(o_j - \max(o_k))}{\sum_k \exp(o_k - \max(o_k))}\right) \\
& = \log{(\exp(o_j - \max(o_k)))}-\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)} \\
& = o_j - \max(o_k) -\log{\left( \sum_k \exp(o_k - \max(o_k)) \right)}.
\end{aligned}
$$

我们也希望保留传统的softmax函数，以备我们需要评估通过模型输出的概率。但是，我们没有将softmax概率传递到损失函数中， 而是**在交叉熵损失函数中传递未规范化的预测，并同时计算softmax及其对数**， 这是一种类似["LogSumExp技巧"](https://en.wikipedia.org/wiki/LogSumExp)的聪明方式。

##### 数值稳定版本的softmax和其他的函数将预测值转换为概率

Softmax函数的主要目标是将一组数值转换为一组概率分布，这些概率分布的总和为1，且每个概率都在0和1之间。这是一个非常强大的特性，因为它允许我们在一组选项中进行选择，其中每个选项都有一个相应的概率。然而，由于softmax函数的指数形式，它可能会导致数值上溢或下溢的问题。

尽管数值稳定版本的softmax函数（即在计算指数之前减去最大值）已经大大减少了这种风险，但是我们可以考虑其他的函数，它们也可以将一组数值转换为概率分布，而不会出现这种问题。

一种选择是Sigmoid函数，它通常用于二分类问题，可以将任意实数映射到(0,1)之间，表示概率。但是，对于多于两个的类别，sigmoid就不再适用。

另一种选择是Gumbel Softmax（或者称为Concrete Distribution）。Gumbel Softmax是一种可以在计算图中直接对离散随机变量进行反向传播的方法。这种方法虽然相对复杂，但在处理离散数据和避免数值不稳定性方面显示出优越性。

然而，无论我们选择哪种方法，都需要注意每种方法的适用性和可能的限制。在大多数情况下，使用数值稳定版本的softmax函数已经足够了，因为它既简单又有效。


### 小结

- 使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。
- 从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。


### 练习

1. 尝试调整超参数，例如批量大小、迭代周期数和学习率，并查看结果。
2. 增加迭代周期的数量。为什么测试精度会在一段时间后降低？我们怎么解决这个问题？

调整超参数：超参数是在开始学习过程之前设置的参数，与模型内部的参数（通常通过训练数据学习的）相比，它们更难以优化。超参数包括学习率、批量大小、迭代周期数等。调整这些超参数可以影响模型的学习速度和性能：

学习率：学习率决定了模型参数每次更新的步长。如果学习率过高，模型可能在最优点附近震荡，而无法收敛；如果学习率过低，训练可能会过于缓慢。

批量大小：批量大小决定了每次参数更新所使用的样本数量。较大的批量可能会提高计算效率，但也可能需要更多的内存，并且可能不会收敛到最优点。

迭代周期数：迭代周期数（或者称为训练轮次）决定了模型遍历整个训练集的次数。训练轮次过少可能会导致模型欠拟合，而训练轮次过多，则可能会导致过拟合。

增加迭代周期的数量：当我们增加迭代周期的数量时，训练集的损失通常会继续下降，因为模型正在学习适应更多的数据。然而，测试精度在一段时间后可能会降低，这是因为模型可能开始过拟合训练数据。过拟合是指模型在训练数据上表现得过于优秀，以至于它开始记忆训练数据的特定噪声和异常值，而不是学习潜在的一般规律。这就导致了模型在未见过的新数据（如测试集或验证集）上的性能下降。

解决过拟合问题的常见方法包括：

早停法：在每个迭代周期后，用验证集评估模型的性能。如果验证性能在一定数量的连续迭代周期内未能改善，那么就停止训练。

正则化：添加一个正则项到损失函数中，以惩罚模型的复杂度。L1和L2正则化是最常见的正则化形式。

数据增强：通过对训练数据进行随机的、有意义的变换（例如旋转、缩放、剪切等），以创建更多的训练样本，从而提高模型的泛化能力。

Dropout：在训练过程中随机关闭一部分神经元，可以减少模型的复杂度和过拟合的风险。

集成方法：训练多个模型，并将它们的预测结果结合起来。这可以提高模型的稳健性和泛化能力。


