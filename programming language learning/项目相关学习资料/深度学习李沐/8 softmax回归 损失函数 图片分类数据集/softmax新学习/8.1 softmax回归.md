![[Pasted image 20231102193927.png]]

### 回归 & 分类

- 回归是估计一个连续值，类似于房价的预测，是一个曲线
- 分类预测是一个离散的类别，例如识别手写数字，那就是需要0-9的10类数字，还有imagenet自然物体分类1000类

### kaggle竞赛的问题

人类蛋白质图片的分类，判断是否是恶意软件，判断wikipedia的评论是否是恶意评论

- 回归
	- 单连续数值输出
	- 自然区间R
	- 与真实值的区别作为损失
- 分类
	- 通常是多个输出
	- 输出i是预测第i类的***置信度***

可以简单的认为从单输出，到多输出
![[Pasted image 20231102194509.png]]
在相同网络模型下，输出值变为多个

### 回归到分类-均方损失

- 对类别需要进行有效编码
独热编码，需要一个长n类的向量，只有在类别的那一类是1
![[Pasted image 20231102194644.png]]

- 使用均方损失训练
- 最大值为预测结果
![[Pasted image 20231102194708.png]]

### 无校验比例

- 对类别进行一位有效编码
- 最大值为预测
- 需要更置信的识别正确类（大余量）
![[Pasted image 20231102194843.png]]

我这里希望正确的分类值，要大于其他分类值，而且要超过一个阈值
这样可以将ground truth 和 错误值拉开距离

### 校验比例

- 输出匹配概率（非负，和为1）
![[Pasted image 20231102195131.png]]

- 概率y和yhat的区别作为损失

我们现在有的是一个o1-on的向量，现在引入一个操作子叫softmax
将o作为输入进入到这个函数
可以转换为一个yhat，是长为n的向量，每个元素非负，并且和为1
这里yhaii的分母是1，分子是oi，也就是o中间的第i个元素，并且上下都进行指数操作，这样可以保证非负

这时候回到最开始设置的独热编码，只有真实标签是100%，所以我们要用真实标签和预测标签做损失

### softmax和交叉熵损失

- 交叉熵常用来衡量两个概率的区别 H(p,q) = -pilog(qi)求和
- 将它作为损失
![[Pasted image 20231102201341.png]]
其中是求和所有的yi，也就是只有真实标签是1，其余都是0，所以最后只会剩下真实标签的预测值 yi x yihat = yihat
所以这证明了，我们并不关心不正确的类的预测值是多少，因为都会变为0，我们只关心正确预测的值会有多大

- 其梯度是真实概率和预测概率的区别
![[Pasted image 20231102202313.png]]


### 总结

- softmax是多类别分类模型
- 使用softmax操作子获得每一个类的预测置信度
- 使用交叉熵预测和label的区别进行计算

### 分类问题

subsec_classification_problem

假设现在输入一张2x2的灰度图像，这个tensor可以写成tensor x1 x2 x3 x4
假设图像有3类，猫狗鸡
我们需要给图片打上tag，我们选择什么方式打tag很关键，最简单的是 {1，2，3}对三个动物标记为猫狗鸡
如果我们的类别是存在自然顺序的，比如说婴儿 儿童 青少年 青年 中年 老年 这个问题在转变为回归问题，这种标记方式是合适的

但是一般的分类问题中类别之间没有自然顺序关系，这个时候我们可以选择采用独热编码来编辑
{(1,0,0), (0,1,0), (0,0,1)} 其中三个编码对应三个种类
$$y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$$


### 网络架构

$$
\begin{aligned}
o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\
o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\
o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.
\end{aligned}
$$
这里我们需要12个weight来对每个像素权重，再加4个偏置项
这里4个特征每个特征添加权重，决定最后的输出是1 还是 2 还是 3
每一个类是不能权重共享的，而是需要各自进行反向传播的

由更好的表现形式$\mathbf{o} = \mathbf{W} \mathbf{x} + \mathbf{b}$
用这种向量的表现形式更好

### 全连接层的参数开销

全连接层如果是d个输入q个输出，参数开销就是$\mathcal{O}(dq)$
可以将d个输入转换为q个输出的成本可以减少到$\mathcal{O}(\frac{dq}{n})$


### softmax运算

希望模型输出的 $\hat{y}_j$ 是可以视为类型j的概率，然后选择具有最大输出值的类别 $\operatorname*{argmax}_j y_j$ 作为我们的预测值
例如， 如果 $\hat{y}_1$ , $\hat{y}_2$ , $\hat{y}_3$ 分别是0.1 0.8 和 0.1
我们预测的类别是2代表的动物

然而我们并不能直接将未经过规范化，也就是处理的$o$ 直接视为输出，虽然我们从逻辑上已经知道了这个最大的输出类就是我们需要的结果

我们没有限制这些输出的总和是1，另一方面，因为输入的不同，他们的值有可能为负值，不满足概率论公理，因为概率从不会是负数，并且样本空间一定是1

我们需要有一个训练的目标函数用来激励模型精准的估计概率，例如在分类器输出0.5的所有样本中，我们希望这些样本刚好有一半实际上属于预测的类别。这个属性叫做校准（calibration）

邓肯卢斯在1959年发明了softmax函数
可以将未规范化的预测变换未非负数并且总和是1，同时让模型可导

$$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$$
这里对于所有的$j$ 总有$0 \leq \hat{y}_j \leq 1$
因此 $\hat{\mathbf{y}}$ 可以视为一个正确的概率分布
softmax不会改变未规范化预测的顺序，只会确定分配给每个类别的概率
因此我们还能用来选择类别

$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j
$$

尽管softmax是一个非线性函数，但是softmax回归的输出依然由输入特征的仿射变换决定
因此，softmax回归是一个线性模型


### 小批量样本的矢量化

为了提高效率并充分利用GPU，通常会对小批量样本的数据执行矢量计算，假设读取了一个批量的样本 $\mathbf{X}$ ，其中特征维度（输入数量）是 $d$，批量大小是$n$，此外，假设我们的输出中是$q$个类别。
那么小批量样本的特征为 $\mathbf{X} \in \mathbb{R}^{n \times d}$，
权重是$\mathbf{W} \in \mathbb{R}^{d \times q}$
偏置项是$\mathbf{b} \in \mathbb{R}^{1\times q}$
softmax回归的矢量计算表达式是：

$$ \begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} &= \mathrm{softmax}(\mathbf{O}). \end{aligned} $$
我们在观察X作为x特征的一个集合，是可以看作一个n行d列的矩阵，其中n代表batch_size，d代表特征数量
W是d行q列，所以可以相乘，且最后矩阵是一个n行q列的矩阵，也就是还是n个数据，然后q列代表着输出的特征数量
偏置项就是跟着输出特征数量来

而且也不用担心$\mathbf{X} \mathbf{W} + \mathbf{b}$ 是否可以求和，因为b是1行q列，所以一定可以使用广播机制进行求和


### 损失函数

损失函数在这里使用跟linear regression中一样的方法

使用最大似然估计
损失函数为
$$ l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j. $$
而因为我们在设置y的时候是独热编码，所以除了实际标签那个类别的预测值$\hat{y}$ 被保留下来了以外，其他的值全部归零，而求对数因为$\hat{y}$ 必然是小于1的，因为对数小于1的部分都是小于0，所以结果一定是正数

如果预测的 $\hat{y} = 1$ 的情况下，损失函数就不可能更小，因为这个时候损失函数已经=0了，但是实际上这种情况不会发生，因为数据集中肯定存在标签噪声，比如说误标等，或者输入特征没有足够的信息来使得计算出的结果完美的对应某一个样本分类


### softmax和导数

我们可以计算一下loss funciton，通过带入softmax的原式

$$
\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}
$$

考虑到相对于任何没有规范化的预测$o_j$ 的导数
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$

因为exp求导还是自己，所以对于 $\log \sum_{k=1}^q \exp(o_k)$
求导首先链式法则把log去掉变为$\frac{1}{X}$ 然后后边求和部分求$o_j$的偏导数，只会保留exp 的第j项

刚好结果符合softmax的定义，所以可以计算得到求偏导值如上

#### 交叉熵损失

如果考虑到整个结果分布的情况，观察到的不仅仅是一个结果，对于标签y，可以用以前的表现形式，但是是使用一个概率向量 $\mathbf{(0.1, 0.2, 0.7)}$ 
而不是一个二元向量 $\mathbf{(0, 0, 1)}$ ，我们使用交叉熵定义损失，它是所有标签分布的预期损失值。

### 信息论基础

信息论（information theory）涉及到编码，解码，发送以及尽可能简洁地处理信息或者数据

##### 熵

信息论的核心思想是量化数据中的信息内容，在信息论中，该数值称为分布P的熵（entropy），可以通过以下方程得到：
$$H[P] = \sum_j - P(j) \log P(j).$$
信息论的基本定理之一指出，为了对从分布$p$中随机抽取的数据进行编码，
我们至少需要$H[P]$“纳特（nat）”对其进行编码。
“纳特”相当于*比特*（bit），但是对数底为$e$而不是2。因此，一个纳特是$\frac{1}{\log(2)} \approx 1.44$比特。

##### 信息量

压缩与预测有什么关系呢？
想象一下，我们有一个要压缩的数据流。
如果我们很容易预测下一个数据，那么这个数据就很容易压缩。
为什么呢？
举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。
由于它们总是相同的，我们总是知道下一个数据是什么。
所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到"惊异"。
克劳德·香农决定用信息量$\log \frac{1}{P(j)} = -\log P(j)$来量化这种惊异程度。
在观察一个事件$j$时，并赋予它（主观）概率$P(j)$。
当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。

##### 重新审视交叉熵

如果把熵$H(P)$想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？
交叉熵*从*$P$*到*$Q$，记为$H(P, Q)$。
我们可以把交叉熵想象为“主观概率为$Q$的观察者在看到根据概率$P$生成的数据时的预期惊异”。
当$P=Q$时，交叉熵达到最低。
在这种情况下，从$P$到$Q$的交叉熵是$H(P, P)= H(P)$。

简而言之，我们可以从两方面来考虑交叉熵分类目标：
（i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。


##### 模型预测和评估

在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。
通常我们使用预测概率最高的类别作为输出类别。
如果预测与实际类别（标签）一致，则预测是正确的。
在接下来的实验中，我们将使用*精度*（accuracy）来评估模型的性能。
精度等于正确预测数与预测总数之间的比率。



### 练习

1. 我们可以更深入地探讨指数族与softmax之间的联系。
    1. 计算softmax交叉熵损失$l(\mathbf{y},\hat{\mathbf{y}})$的二阶导数。
    2. 计算$\mathrm{softmax}(\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配。
2. 假设我们有三个类发生的概率相等，即概率向量是$(\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$。
    1. 如果我们尝试为它设计二进制代码，有什么问题？
    2. 请设计一个更好的代码。提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码$n$个观测值怎么办？
3. softmax是对上面介绍的映射的误称（虽然深度学习领域中很多人都使用这个名字）。真正的softmax被定义为$\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$。
    1. 证明$\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$。
    2. 证明$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) > \mathrm{max}(a, b)$成立，前提是$\lambda > 0$。
    3. 证明对于$\lambda \to \infty$，有$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$。
    4. soft-min会是什么样子？
    5. 将其扩展到两个以上的数字。


1. **探讨指数族与softmax之间的联系**
    
    1. **计算softmax交叉熵损失$l(\mathbf{y},\hat{\mathbf{y}})$的二阶导数**
        
        假设我们的交叉熵损失函数定义如下： $$l(\mathbf{y},\hat{\mathbf{y}}) = -\sum \mathbf{y}_i \log(\hat{\mathbf{y}}_i)$$
        
        我们需要计算的是关于$\hat{\mathbf{y}}$的二阶导数。这涉及到对$\hat{\mathbf{y}}$（即softmax函数）和$\log(\hat{\mathbf{y}})$的导数的计算，这些计算可能相当复杂。
        
    2. **计算$\mathrm{softmax}(\mathbf{o})$给出的分布方差，并与上面计算的二阶导数匹配**
        
        softmax函数给出的是一个概率分布。对于一个概率分布$p$，其方差计算公式如下： $$\mathrm{Var}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$
        
        对于softmax函数，我们可以首先计算其期望$\mathbb{E}[X]$，然后计算$\mathbb{E}[X^2]$，最后用上述公式计算方差。


2. **三类发生的概率相等的情况**
    
    1. **如果我们尝试为它设计二进制代码，有什么问题？**
        
        如果我们尝试为三个等概率的类别设计二进制代码，我们遇到的主要问题是我们不能用两个二进制位来代表三个状态。我们需要至少两位才能区分三个类别，但这意味着我们有四种可能的组合（00, 01, 10, 11），其中有一种组合将无法使用。
        
    2. **请设计一个更好的代码**
        
        一个更好的代码设计可能是使用基于频率的编码，如哈夫曼编码。哈夫曼编码是一种优化的字符编码技术，用于有效编码数据，使平均编码长度最小。然而，在这种情况下，由于所有类别的概率都是相等的，所以任何编码方案都会有相同的平均长度。



3. **关于softmax与RealSoftMax**
    
    1. **证明$\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$**
        
        对于任意的$a$和$b$，我们有： $$\mathrm{RealSoftMax}(a, b) = \log (\exp(a) + \exp(b))$$
        
        由于$\exp(a)$和$\exp(b)$都是正的，所以它们的和大于它们中的任何一个，因此： $$\exp(\mathrm{RealSoftMax}(a, b)) = \exp(a) + \exp(b) > \exp(a), \exp(b)$$
        
        因此，我们有$\mathrm{RealSoftMax}(a, b) > \mathrm{max}(a, b)$。
        
    2. **证明$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) > \mathrm{max}(a, b)$成立，前提是$\lambda > 0$**
        
        我们只需要证明： $$\lambda^{-1} \log (\exp(\lambda a) + \exp(\lambda b)) > \mathrm{max}(a, b)$$
        
        这可以通过类似上面的方式证明。
        
    3. **证明对于$\lambda \to \infty$，有$\lambda^{-1} \mathrm{RealSoftMax}(\lambda a, \lambda b) \to \mathrm{max}(a, b)$**
        
        当$\lambda \to \infty$时，$\exp(\lambda a)$和$\exp(\lambda b)$中的较大者将主导它们的和。因此，我们有：
        
        $$\lambda^{-1} \log (\exp(\lambda a) + \exp(\lambda b)) \to \lambda^{-1} \cdot \lambda \cdot \max(a, b) = \max(a, b)$$
        
    4. **soft-min会是什么样子？**
        
        "soft-min" 定义为： $$\mathrm{RealSoftMin}(a, b) = -\log (\exp(-a) + \exp(-b))$$
        
        这是在对数空间中实现的"min"操作，对应于在原始空间中的概率归一化。
        
    5. **将其扩展到两个以上的数字**
        
        对于$n$个数字的情况，我们可以定义：
        
        $$\mathrm{RealSoftMax}(\mathbf{a}) = \log \left(\sum_{i=1}^{n} \exp(a_i)\right)$$
        
        同样，"soft-min"可以扩展为：
        
        $$\mathrm{RealSoftMin}(\mathbf{a}) = -\log \left(\sum_{i=1}^{n} \exp(-a_i)\right)$$
        
        这些函数在深度学习中非常有用，尤其是在处理概率分布和损失函数时。

