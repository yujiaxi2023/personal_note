# 让训练更加稳定

- 目标：让梯度值在合理的范围内
	- 例如1e-6 - 1e3之间
- 让乘法变加法
	- ResNet，LSTM
- 归一化
	- 梯度归一化，梯度裁剪
- 合理的权重初始和激活函数

# 让每层的方差是一个常数

- 让每层的输出和梯度都看作随机变量
- 让他们的均值和方差保持一致
![[Pasted image 20230928222522.png]]
ab都是常数

# 权重初始化

- 在合理值区间里随机初始参数
- 训练开始的时候更容易又数值不稳定
	- 远离最优解的地方损失函数表面可能很复杂
	- 最优解附近表面比较平滑
- 使用均值0方差0.01来初始化无法对很深的神经网络有用

![[Pasted image 20230928223011.png]]

# 例子：MLP

- 假设
![[Pasted image 20230928223128.png]]
权重是独立同分布的，均值就是0，方差就是γt
这一层的输入是独立于权重的

- 假设没有激活函数
![[Pasted image 20230928223223.png]]
![[Pasted image 20230928223229.png]]
这里均值的和可以拆分开，因为是独立的

# 正向方差

![[Pasted image 20230928223459.png]]
这是基本的概率论的内容
最后就是方差的乘积和
如果需要输入方差和输出方差一样
这里的![[Pasted image 20230928223641.png]]
就要满足上述式子的条件

# 反向均值和方差

![[Pasted image 20230928223702.png]]

要保证均值为0，方差一致的情况下要同时满足下面条件

# Xavier初始

- 难以同时满足![[Pasted image 20230928223956.png]]
- Xavier使得![[Pasted image 20230928224029.png]]
![[Pasted image 20230928224100.png]]

# 假设线性激活函数

![[Pasted image 20230928224327.png]]

![[Pasted image 20230928224516.png]]

如果希望均值0，方差固定，就必须这样选

![[Pasted image 20230928224549.png]]

一般来说tanh和relu在0的附近满足刚才的条件
但是sigmoid不太满足，需要进行调整之后才行

# 总结

- 合理的权重初始值和激活函数的选取可以提升数值稳定性
每一层的输出和每一层的梯度都是一个均值为0，方差为固定数的随机变量
权重初始化使用Xavier，激活函数使用tanh relu或者调整的sigmoid都行

**问题1：nan和inf产生的原因：inf产生的原因，是因为权重初始和lr选择过大的原因，nan产生的原因基本上因为是/0的原因，通过合理的初始化权重，激活函数不要选错，学习率不要太大就是一些有用的方式，均值为0，方差的波动范围减小到不出现这些数值
问题2：一般来说准确率稳定在较低的范围，可能是数值稳定性的问题
问题3：一般训练过程中出现了nan，一般是梯度发生问题，消失或者爆炸
问题4：训练的时候使用浮点数又很大关系，bfloat16跟32位更加接近
问题5：LSTM会使用指数或者log将乘法转换为加法
问题6：加法比乘法稳定性更好的原因是因为100次乘法数值很大，加法数值就很正常
问题7：随机初始化有Xavier初始化比较好
问题8：对于高维损失函数很难进行可视化
问题9：使用4*sigmoidx-2提高稳定性是因为跟y=是一个梯度*
问题10：权重是每次iterate和batch更新，epoch就是更新很多次权重了
问题11：使用孪生网络因为输入数值区间不一样，所以可能引起数值不稳定，可以添加权重让两类数值在差不多的范围内
问题12：对于每一层的均值方差限制，会降低极大极小值出现的机率**