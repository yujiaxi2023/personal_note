# 神经网络的梯度

- 考虑如下有d层的神经网络
![[Pasted image 20230928215720.png]]
这里t表示层数ft是代表输出经过的函数

- 计算损失l关于参数Wt的梯度
![[Pasted image 20230928215738.png]]
这里经过了太多的矩阵相乘

# 数值稳定性的常见两个问题

梯度爆炸和梯度消失
![[Pasted image 20230928220112.png]]
如果梯度都是大于1，如果做100层，但是这样会带来一些浮点上线的问题
如果梯度都是小于1，如果做100层，就会带来一些很小的数的问题

# 例子：MLP

- 加入如下MLP
省略了bias
![[Pasted image 20230928220316.png]]
其中一层就是这样计算

![[Pasted image 20230928220332.png]]
求导数
就是一个对角矩阵

![[Pasted image 20230928220500.png]]

## 梯度爆炸

- 使用ReLU作为激活函数

![[Pasted image 20230928220539.png]]

![[Pasted image 20230928220554.png]]

## 梯度爆炸的问题

- 值超出值域（infinity）
	- 对于16位浮点数严重（数值区间在6e-5 - 6e4）因为cpu在16位浮点数上面计算比较快速
- 对学习率敏感
	- 如果学习率过大 -> 大参数值 -> 更大的梯度
	- 如果学习率太小 -> 训练没有进展
	- 需要在训练过程不断调整学习率

## 梯度消失

- 使用sigmoid作为激活函数
![[Pasted image 20230928221213.png]]
sigmoid函数的梯度是一个 正态分布一样的东西

![[Pasted image 20230928221709.png]]

# 梯度消失的问题

- 梯度值变为0
	- 对16位浮点数严重
- 训练没有进展
	- 无论如何选择学习率
- 对底部层训练非常严重 靠近数据更近的层
	- 仅仅顶部层训练比较好 梯度反传从顶部开始
	- 无法让网络更深

# 总结

- 当数值过大或者过小的时候会导致数值问题
- 常发生在深度模型中，因为会对n个数累乘
