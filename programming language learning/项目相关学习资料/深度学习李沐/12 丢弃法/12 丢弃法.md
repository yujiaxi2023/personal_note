# 动机

- 一个好的模型需要对输入数据的扰动鲁棒
	- 使用有噪音的数据等价于Tikhonov正则
	- 丢弃法：在层之间加入噪音

# 无偏差的加入噪音

- 对x加入噪音得到x‘，我们希望
![[Pasted image 20230928202046.png]]
- 丢弃法对每个元素进行如下扰动
![[Pasted image 20230928202107.png]]
p是0-1之间的数

我们希望添加噪音之后，期望不发生变化
![[Pasted image 20230928202159.png]]
这就是dropout的定义

# 使用丢弃法

- 通常将丢弃法作用在隐藏全连接层的输出上
![[Pasted image 20230928202244.png]]
h是第一个隐藏层的输出
经过一个dropout
后一层是把前一层的某些元素按照概率p变为0，然后一些元素scale的结果进行输入
然后softmax得出结果
![[Pasted image 20230928202822.png]]


# 推理中的dropout

- 正则项只在训练中使用：他们影响模型参数的更新，也就是权重
- 在推理过程中，丢弃法直接返回输入
![[Pasted image 20230928204629.png]]
这样也能保证确定性的输出

使用dropout就是随机采样一些子神经网络来训练，等于很多小的神经网络做平均ensemble
实验上是看起来跟正则是一样的，将这个作为一个正则项

# 总结

- 丢弃法将一些输出项随机变为0来控制模型的复杂度
- 常常作用在MLP的隐藏层输出上
- 丢弃概率是控制模型复杂度的超参数（0.1 0.5 0.9）


trick就是把中间层设置的更大一点，dropout掉一部分
这样比你正常设计的隐藏层中等要更好一点

**问题1：dropout之后0的项的梯度变为0，这一轮就不会被更新，而非0的层是会scale
问题2：dropout的probability设置的大小会导致欠拟合过拟合
问题3：深度学习没有正确性，但是有精确度，实际上如果代码有问题，最后的精度还是不会很大变化
问题4：神经网络的可重复性，固定随机种子，run10次就会重复，结果应该是一样的，但是整个网络的初始化也是随机的，cudnn也会带来一些随机性，因为计算机n个数相加，加起来的结果还会不一样，所以几乎不能重复，如果需要保证可重复性，就不能使用cudnn，但是不需要保证可重复性，只需要保证训练很多epoch之后精度差不多稳定在某一个值，随机性越高其实robust会增加
问题5：每一层在forward的时候会进行一次dropout，每一个layer也就是batch会进行一次
问题6：BN是作用在卷积层的，dropout是给MLP用的
问题7：dropout在推理中是不需要的，因为inference是需要稳定的输出
问题8：dropout的返回值需要除以一个1-p，这个只会改变隐藏层的输出，而且dropout其实是一个正则项，所以对很多都可以进行这种操作，改最开始的输入标签也是一种操作
问题9：使用dropout虽然会消失一部分神经元变为0，但是其余的神经元会在变大1/1-p，所以期望不变结果不变
问题10：weight decay可以用在所有的网络，但是dropout只用在MLP中，而且dropout好调参，weight decay不好调，深度学习需要过拟合，但是需要一个正则让他不要模型学偏了
问题11：CNN可以看作一个特殊的MLP
问题12：dropout可能使得收敛变慢
问题13：transformer可以看作一个kernel machine**
