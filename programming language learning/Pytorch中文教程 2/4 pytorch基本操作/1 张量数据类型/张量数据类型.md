![[Pasted image 20230508135531.png]]
python中的数据类型和pytorch中的数据类型
还有它们的对应关系
整型Int对应IntTensor of size()
float 对应 FloatTensor of size()
Int array 对应 IntTensor of size 【d1，d2，···】
Float array同上
string 无法在pytorch中表示

在nlp中需要处理string数据
有两种类型的表示方法
1. 独热 One-hot
![[Pasted image 20230508135959.png]]
跟计算机表示字母是同样的方式，比如ASCII编码
![[Pasted image 20230508140035.png]]
难以表示语义之间的相关性，而且如果使用独热向量的方式表示会很麻烦，因为在一个位置用1表示一个字符，这样稀疏性太大了

![[Pasted image 20230508140210.png]]
2. embedding方法
![[Pasted image 20230508140320.png]]
pytorch在GPU和CPU中不是一个东西
重要的是浮点型 双浮点型 比特型和整型这几种数据类型

```python
a = torch.randn(2,3)

a.type()
# 'torch.FloatTensor' type方法表示数据类型，python中需要print出来

type(a)
# 'torch.Tensor' python自带的type方法

isinstance(a, torch.FloatTensor)
# True 参数的合法化检验
```
**GPU 的数据类型**
```python
isinstance(data, torch.cuda.DoubleTensor)
# False instance检测，此时data还是在CPU上

data = data.cuda()
# 用cuda搬运到GPU中

isinstance(data, torch.cuda.DoubleTensor)
# True 搬运后data就是cuda类型了
```

**标量**
```python
torch.tensor(1.)
# tensor(1.) 用torch.tensor方法 这是生成的dim=0的tensor 最长用于计算loss，误差平均后求和 创建的标量是浮点数

torch.tensor(1.3)
# tensor(1.300)print(torch.tensor(1.3))的结果是tensor(1.3000)，这是因为PyTorch的张量有一个默认的打印格式，它会显示张量的类型、形状和数据。对于零维的张量，也就是标量，它会显示tensor()，然后在括号内显示数值，数值会保留四位小数。例如：

x = torch.tensor(1.3) 
print(x)

# 输出：tensor(1.3000)

# 如果您想改变张量的打印格式，您可以使用torch.set_printoptions()函数来设置不同的参数，例如精度、阈值、边缘元素等。例如：

torch.set_printoptions(precision=2) 
y = torch.tensor(1.3) 
print(y)

# 输出：tensor(1.30)
```

```python
a = torch.tensor(2.2)

a.shape
# torch.Size([])  空列表的size  可以用来修改张量的形状

a.size()
# torch.Size([]) 是一个成员函数 numpy一般用shape pytorch一般用size

len(a.shape)
# 0 shape的长度就是0
a.dim
# 0 也是得到维度为0 两个东西没有实际区别
```

**向量dim1**
```python
torch.tensor([1.1])
# tensor([1.1000])

torch.tensor([1.1, 2.2])
# tensor([1.1000, 2.2000]) .tensor的方法就是指定tensor的数据

torch.FloatTensor(1)
# tensor([3.22393-25]) .FloatTensor方法生成随机的dim和size为1的张量

torch.FloatTensor(2)
# tensor([3.2239e-25, 4.4915e-41]) 随机初始化两个数值

data = np.ones(2)
# numpy 方法生成长度为2的vector
print(data)
# array([1., 1.])这是形成的array

torch.from_numpy(data)
# tensor([1., 1.],  dtype=torch.float64) 形成了64浮点的tensor
```
dim1的tensor可以用在bias中,也就是偏置项
![[Pasted image 20230508145122.png]]
神经网络线性层的输入
![[Pasted image 20230508145332.png]]
pytorch0.3之后的loss就是标量，之前一直是长度为1的一维向量

如何得到dim1的shape或者size
```python
a = torch.ones(2)

a.shape
a.size()
```
dim指的是size的长度
size或者shape指的时具体的形状
![[Pasted image 20230508145835.png]]
**dim2的张量**
```python
a = torch.randn(2,3)

print(a)

print(a.shape)

print(a.size(1))

print(a.shape[1])

"""
结果如下
tensor([[ 1.0810, -1.3261, -1.5619],
        [ 0.2888, -0.7833,  1.5991]])
torch.Size([2, 3])
2
3
2
3
"""
```
经常使用在线性输入的batch中
![[Pasted image 20230508150345.png]]
图中数字代表的是，第一个数字代表的是第几张照片，后边代表的是照片包含的数据内容

**dim3的张量**
```python
a = torch.rand(2,3,4) # 随机均匀分布

print(a)
print(a.shape)
print(a[0])

print(list(a.shape))

"""
结果
tensor([[[0.7570, 0.0536, 0.3007, 0.2811],
         [0.3846, 0.6879, 0.1021, 0.3182],
         [0.4417, 0.7303, 0.8925, 0.9867]],

        [[0.3103, 0.1378, 0.3425, 0.4979],
         [0.7955, 0.9186, 0.6307, 0.4262],
         [0.6447, 0.3034, 0.1799, 0.7371]]])
torch.Size([2, 3, 4])
tensor([[0.7570, 0.0536, 0.3007, 0.2811],
        [0.3846, 0.6879, 0.1021, 0.3182],
        [0.4417, 0.7303, 0.8925, 0.9867]])
[2, 3, 4]
"""
```
在RNN中使用比较广泛
一句话有10个单词，一个单词用100维度的向量来表示feature
batch插在单词和feature中间
每次送20个句子，每个句子10个单词，一个单词用100维度向量来表示
![[Pasted image 20230508151030.png]]

**dim4的张量**
适合图片进行神经网络构建
```python
a = torch.rand(2,3,28,28) # 2张照片 RGB 3channels 图像的像素size

print(a)
print(a.shape)

```
![[Pasted image 20230508151442.png]]
上面的字母分别代表的是batch channel height width

Mixed补充知识
```python
a.shape

a.numel()
# number of element 2x3x28x28

a.dim()
# 4 len(a.shape)
```
numel是tensor占用内存的大小
dim可以更加直观的得到数据的维度

