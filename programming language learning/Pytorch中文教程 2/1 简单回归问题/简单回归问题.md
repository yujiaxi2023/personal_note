梯度下降
有
loss function = x²sin(x)
![[Pasted image 20230506194938.png]]
使用不同求解器可以让梯度下降的更快更好

一个简单的线性方程
![[Pasted image 20230506195013.png]]
![[Pasted image 20230506195153.png]]
可以精确求解这个方程

![[Pasted image 20230506195307.png]]
在原来的线性方程中添加一个高斯噪声，符合标准差0.01，均值1的高斯分布
因为有噪声，所以对于w和b的求值用两个方程是不准确的，所以需要有数十个数百个数千个方程式求近似解
![[Pasted image 20230506195500.png]]
构造一个最小的损失函数
![[Pasted image 20230506195628.png]]
![[Pasted image 20230506195657.png]]
现有一个具体的模型生成一系列数据
![[Pasted image 20230506195836.png]]![[Pasted image 20230506195845.png]]
这个图是由那个具体的方程生成的 100个存在高斯噪声的数据
实际情况下我们是知道这100个数据，不知道上面的方程

**我们如果需要深入了解规律需要研究凸函数优化这门学科，以及如何将函数变为凸函数**这是深度学习的关键

![[Pasted image 20230506200550.png]]
预测值y‘和y实际值求差值后平方作为均方误差函数，作为linear regression的损失函数

```python
def compute_error_for_line_given_points(b, w, points):
# w 和 b的取值还有points代表一系列xy的取值组合
	totalError = 0
	for i in range(0, len(points)):
	# 对所有的points点进行取值迭代
		x = points[i, 0]
		y = points[i, 1]
		totalError += (y - (w * x + b)) **2
		# 这就是损失函数的加和公式
	return totalError / float(len(points))
	# 计算总的误差除以点的数量求一个平均
```

计算loss function之后
可以计算梯度信息
![[Pasted image 20230506201224.png]]
```python
def step_gradient(b_current, w_current, points, learningRate):
	b_gradient = 0
	w_gradient = 0
	N = float(len(points))
	for i in range(0, len(points)):
		x = points[i, 0] # 取得每一点上x的值
		y = points[i, 1] # 取得每一点上y的值
		b_gradient += -(2/N) * (y - ((w_current * x) + b_current))
		w_gradient += -(2/N) * x * (y - ((w_current) * x) + b_current)
	new_b = b_current - (learningRate * b_gradient)
	new_w = w_current - (learningRate * w_gradient)
	return [new_b, new_w]
```
上面的代码是循环迭代梯度信息

现在添加优化器
```python
def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):
	b = starting_b
	w = starting_w
	for i in range(num_iterations):
		b, w = step_gradient(b, w, np.array(points), learning_rate)
	return [b, w]
```
比如我们迭代100次num iterations，每次根据当前的b w 以及通过np形成的array形式的数组和学习率lr
