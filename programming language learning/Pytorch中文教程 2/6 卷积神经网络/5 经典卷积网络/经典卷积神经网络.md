![[Pasted image 20230510143849.png]]
错误率逐渐下降 层数逐渐的增加
![[Pasted image 20230510143926.png]]
![[Pasted image 20230510144037.png]]
![[Pasted image 20230510144334.png]]
drop out是随机杀死一些神经元
小核心的效果更好
1x1的计算量更少
![[Pasted image 20230510144727.png]]
![[Pasted image 20230510144817.png]]
同层可以使用不同的卷积核
就是用不同的kernel利用padding和stride形成一个超级长的一个feature map
![[Pasted image 20230510145033.png]]
因为每个kernel的感受野不同，综合的理解之后可以更多的特征获得
![[Pasted image 20230510145130.png]]
简单的对叠层数不一定会让结果表现更好
![[Pasted image 20230510145303.png]]
