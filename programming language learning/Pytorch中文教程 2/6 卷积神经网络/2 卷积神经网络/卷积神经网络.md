![[Pasted image 20230510121008.png]]
新的卷积生成的函数就是由上图所示
得到的feature map的函数F（x,y）
其中 I（x，y）是输入数据 kernel 就是感受野也对应一个函数
形成一个新的大小的feature map
如果没有padding 就是会缩小一点
![[Pasted image 20230510121245.png]]
如果有多个kernel计算会形成一个卷积层 由很多个feature map组合成的
从算法的角度理解就是
现在又一个能够对应到原来的图片1，28，28的图片 单通道没彩色的
用一个kernel 1，3，3一个3x3的卷积网络匹配
现在有7个这样的kernel 就会在这个kernel前面添加一个类 7 代表 7个
![[Pasted image 20230510121644.png]]
一次kernel的运算生成的feature map

对于这个案例
input_channels: 1/3 原始图像如果时黑白灰度的就是1 如果彩色就是3
kernel_channels: 一般指有多少个kernel
kernel_size: 是指kernel多大，常见有3x3或者5x5或者7x7
stride: 是指每次移动的步长
padding: 就是边缘增加的空白行列的数量
![[Pasted image 20230510122315.png]]
生成feature map大小的计算方式

![[Pasted image 20230510122407.png]]
对于这个彩色图片卷积运算
这里我们对应的几个数据分别为
input的数据为 b 代表batch    b，3，28，28
一个kernel的数据因为需要对应3通道 所以     3，3，3   这是3x3卷积 这里的kernel 一定要跟输入的x的channel对应起来
对于这里的一共有16个kernel
每一个kernel需要对应一个偏置项
生成的大小是
![[Pasted image 20230510122912.png]]
这里需要注意到是kernel中也是存在channel的并且和input的channel对应
kernel 也可以叫 filter 或者是 weight
对于这里的kernel运算
就是3个kernel内部的channel中对感受野进行点积运算得到结果
每个channel的值加在一起才形成feature map中的1个数值

LeNet-5的形式
![[Pasted image 20230510123912.png]]
为什么需要叠加的方式
stack
可以看出每一层的feature map 的visualization
从网络深度增加 是 逐渐观察更深的特征 就是逐渐高维度的概念
![[Pasted image 20230510124156.png]]
representation learning 的过程


使用pytorch实现卷积神经网络的过程
nn.Conv2d
```python
layer = nn.Conv2d(1,3,kernel_size=3,stride=1,padding=0)
x = torch.rand(1,1,28,28)

out = layer.forward(x)

layer = nn.Conv2d(1,3,kernel_size=3,stride=1,padding=1)
out = layer.forward(x)

layer = nn.Conv2d(1,3,kernel_size=3,stride=2,padding=1)
out = layer.forward(x)

out = layer(x)
```
首先进行一次conv2d的运算
第一个参数是input的channel
第二个参数是kernel的数量
![[Pasted image 20230510124744.png]]
这样输入的图片和kernel的基本信息就已经输入进来
然后调用.forward 函数进行前向运算
这里输出的结果是
![[Pasted image 20230510124902.png]]

我们重新卷积运算一个层，这时候增加了padding = 1
所以结果就发生了变化
![[Pasted image 20230510125048.png]]

如果我们步长使用的不一样，比如我们步长调整为2
![[Pasted image 20230510125121.png]]

![[Pasted image 20230510125235.png]]
上图是pytorch封装的一些hooks 利用调用python中的call函数
所以除非我们明确的要修改某些层进行运算
我们最好不要使用forward函数，而是使用默认调用的hooks

**如何知道层里面的各个属性**
innner weight 和 bias
```python
layer.weight
layer.weight.shape
layer.bias.shape
```
这里是保留有梯度信息的
![[Pasted image 20230510125654.png]]
这里可以给出layer这个tensor包含的具体数值信息，是否可以做梯度下降

**F.conv2d**
```python
w = torch.rand(16,3,5,5)
b = torch.rand(16)

out = F.conv2d(x,w,b,stride=1,padding=1)
```
这里的w就是代表kernels
这里的b 就是bias
因为这里的x是上面的x是单色图片只有一个颜色channel
所以会报错
![[Pasted image 20230510130429.png]]
这里我们需要把x调整一下符合我们输入的kernel 的channels
```python
x = torch.randn(1,3,28,28)

out = F.conv2d(x,w,b,stride=1,padding=1)

out = F.conv2d(x,w,b,stride=2,padding=2)
```
![[Pasted image 20230510130700.png]]
以上就是最普通的卷积神经网络一层是如何计算的

**upsample**
![[Pasted image 20230510132845.png]]
```python
x = out
out = F.interpolate(x, scale_factor = 2, mode = 'nearest')
out.shape
```
![[Pasted image 20230510133020.png]]
可以从结果推测出原来的x 是 1，16，7，7
这里采用的是最近邻放大，可以在python中查interpolate 还有什么别的放大方式

如果我们是放大3倍
```python
out = F.interpolate(x, scale_factor = 3, mode = 'nearest')
out.shape
```
![[Pasted image 20230510133212.png]]

**ReLU**
一个unit单元是
conv2d    batchnormalization  pooling   ReLU
后边三个可以颠倒或者参考别人的结构
ReLU函数是把原来图片响应很少的部分去除
![[Pasted image 20230510133500.png]]

```python
x.shape
layer = nn.ReLU(inplace = True)
out = layer(x)
out.shape

out = F.relu(x)
out.shape
```
经过了conv2d和pooling的操作之后
形成了x的size 是
![[Pasted image 20230510133630.png]]
然后经过relu函数
inplace是把新的能够替代原来的
我们查看x的最小值就会发现是0
因为relu函数已经去除了小于0的数据
![[Pasted image 20230510133745.png]]
![[Pasted image 20230510133801.png]]
F.relu()也是可以进行同样的操作
