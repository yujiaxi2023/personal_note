单层感知机模型就是
![[Pasted image 20230522152235.png]]
对于单层感知机进行一个激活函数的处理
输入层x 的上标表示第几层 下标是 0-n-1个元素
第一层的参数上标是1代表层数 下标为 i j 其中 i 代表连接的上一层的 i 号节点，j 代表连接下一层的 j 号节点
因为现在是单层感知机，所以这里只有一个激活函数一个神经元
然后进入到loss function中计算，可以是均方误差也可以是交叉熵
![[Pasted image 20230522152807.png]]

输出的误差为
![[Pasted image 20230522152841.png]]
这里的0.5 是为了和后边平方消掉，因为这里不改变单调性所以可以乘以0.5
这里求导结果是
![[Pasted image 20230522153105.png]]
打勾的部分是在前向传播中都进行计算了
后边红色的wj0和x0求偏导数的结果是
![[Pasted image 20230522153240.png]]

```python
x = torch.randn(1,10)
w = torch.randn(1,10, requires_grad = True)

o = torch.sigmoid(x@w.t())
o.shape

loss = F.mse_loss(torch.ones(1,1), o)
loss.shape

loss. backward()

w.grad
```
![[Pasted image 20230522153609.png]]
形成了一个一维的label
![[Pasted image 20230522153747.png]]

输出节点设立为多个感知机
![[Pasted image 20230523121606.png]]
这里的O就代表着经过感知机计算的神经元
这里的t代表的是实际状况的label
![[Pasted image 20230523122211.png]]
当k = i 的时候,根据图中可以看出结果只收到wjk这个参数的影响,所以求导结果就是左侧的公式,**其他的差值中间没有wjk这个参数,所以求导的结果都是0**
![[Pasted image 20230523122545.png]]
同样的因为xk1代表的是wjk乘以xj0, xj0代表的是上一层的输入, 所以也是只有wjk会影响存在wjk的那一项求导,其余求导结果为0
最后结果就是
![[Pasted image 20230523122713.png]]
上次的单层感知机就是将上图中的Ok-tk这个变为了O0-t0

具体使用pytorch实现
```python
x = torch.randn(1,10)
w = torch.randn(2,10,requires_grad=True)
o = torch.sigmoid(x@w.t())
o.shape

loss = F.mse_loss(torch.ones(1,1),o)
loss

loss.backward()

w.grad
```
首先生成的x和w两个矩阵最后专职相乘的结果是 1行2列的矩阵
这里生成了一个全是1的1行1列的矩阵本来是没法跟前面的o做均方误差计算
但是算法会直接boardingcast成一个1行2列的矩阵进行计算
loss使用backward计算梯度
要知道w.grad 的形状应该完全等于w的形状,否则做梯度下降的时候没办法直接相减
![[Pasted image 20230523123419.png]]
