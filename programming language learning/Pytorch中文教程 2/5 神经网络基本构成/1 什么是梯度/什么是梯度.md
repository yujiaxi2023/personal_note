导数 derivate 
偏微分 partial derivate
梯度 gradient
![[Pasted image 20230516143222.png]]
梯度是有方向和大小的
![[Pasted image 20230516143444.png]]
箭头的方向代表梯度的方向
箭头的长度代表梯度的模
![[Pasted image 20230516143524.png]]
这个是有两个局部最低点的网络
利用梯度求导极小值点
![[Pasted image 20230516143604.png]]
![[Pasted image 20230516143734.png]]
更新的规则是：
通过现在点的坐标 - 学习率 x 该点的偏导数
![[Pasted image 20230516143808.png]]
代表在向量空间中朝着局部最小值的方向移动了一点
![[Pasted image 20230516143918.png]]
这是对于这个案例的具体的偏导数的值

![[Pasted image 20230516162834.png]]
现在设置一个函数如图所示
随机初始化一个点 x=2.5
上图的过程就是一个学习过程，每一个点就代表减去一个偏导数值
可以发现前面的步子比较小，中间的导数比较大的时候步子比较大，然后中间又变小
![[Pasted image 20230516163120.png]]
使用不同优化器的下降速度
如果不添加一个额外的东西就可能存在卡到一个最低点的情况
凸优化就是搜索到凸函数的最低点的方法
![[Pasted image 20230516163452.png]]
一个正常的训练过程，以ResNet-56为例，图中就是loss function计算出的值，可以看到有一个全局最低点，但是也有很多的局部最低点，这全部是再计算中获得的数值，可以发现，如果我们不进行算法上面的调整就很容易陷入到局部点中

除了得到局部最小值之外还可能获得一个鞍点的位置
也就是对于某一个维度的极度最小值，像是马鞍一样，骑着的位置是一个最小值，但是腿放着的方向是会有更小的数值
![[Pasted image 20230516163859.png]]这是因为我们构成的函数往往是存在几百上千个变量值，因为处理图像时每个数值都会对应一个方程求解，所以会有非常多的维度

但是也有一些可以进行积极影响的操作
- initialization status
- learning rate
- momentum
- etc

**initialization status 初始状态**
![[Pasted image 20230516164159.png]]
这里可以看到初始点一个在左一个在右，使用相同的其他条件得出的最小值是不一样的，左边的就容易陷入到一个局部最小值中
三维的是类似的情形
![[Pasted image 20230516164340.png]]

所以在深度学习中要记得一定要将函数进行初始化，现在主流的初始化方法是何凯明的初始化方法，如果没有特殊的把握的话

**learning rate学习率**
![[Pasted image 20230516164615.png]]
学习率选择过大就容易在最小值周边震荡，或者是不收敛了

如何逃出局部最小值？
**momentum 增加动量**
![[Pasted image 20230516164849.png]]
中间就是没有惯性的情况就会陷入局部
如果增加一个动量的属性就会冲出局部最小值继续进行计算
