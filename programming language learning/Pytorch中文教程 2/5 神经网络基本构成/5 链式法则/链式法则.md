![[Pasted image 20230523145134.png]]
跟微积分中求偏微分方法是一样的
在神经网络中都会添加一个复杂的激活函数
所以使用链式法则会让计算变得更加清晰
![[Pasted image 20230523145402.png]]
tk这边的作用是跟计算结果计算均方误差来计算损失函数

```python
x = torch.tensor(1.)
w1 = torch.tensor(2., requires_grad=True)
b1 = torch.tensor(1.)
w2 = torch.tensor(2., requires_grad=True)
b2 = torch.tensor(1.)

y1 = x*w1 + b1
y2 = y1*w2 +b2

dy2_dy1 = autograd.grad(y2,[y1],retain_graph=True)[0]
dy1_dw1 = autograd.grad(y1,[w1],retain_graph=True)[0]
dy2_dw1 = autograd.grad(y2,[w1],retain_graph=True)[0]

dy2_dy1*dy1_dw1

dy2_dw1
```
上部分就是前向传播过程,输入数据就是定义的几个tensor
下部分就是定义y2对y1的求导一直到w1的链式法则求导
最后一部分结果是相等的
![[Pasted image 20230523150158.png]]
