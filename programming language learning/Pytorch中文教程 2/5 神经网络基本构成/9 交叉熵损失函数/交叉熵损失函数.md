cross entropy loss
![[Pasted image 20230526210051.png]]
entropy 熵代表的是不确定程度，也就是高的熵代表少量的信息代表更大的惊喜

```python
a = torch.full([4], 1/4.)
a*torch.log2(a)
-(a*torch.log2(a)).sum()
a = torch.tensor([0.1, 0.1, 0.1, 0.7])
-(a*torch.log2(a)).sum()
a = torch.tensor([0.001, 0.001, 0.001, 0.997])
-(a*torch.log2(a)).sum()
```
![[Pasted image 20230607170309.png]]
看到最后计算出来的熵,越小代表得到这个结果的惊喜度就很高

交叉熵函数是如下
![[Pasted image 20230607170859.png]]
可以看作是一个熵加上KL散度
KL散度代表两个函数的重叠状况,如果散度为0则重合,散度大就是重合范围小
![[Pasted image 20230607170950.png]]
交叉熵函数存在两种极致的情况
![[Pasted image 20230607171007.png]]
就是代表的散度和熵为0的两种情况
  
交叉熵（Cross-Entropy）是信息论中用于衡量两个概率分布之间差异的度量方式。在机器学习和深度学习领域中，交叉熵常被用作损失函数，特别是在分类问题中。

假设有两个概率分布p和q，交叉熵可以通过以下公式计算：

$H(p, q) = -\sum_{i} p(i) \log(q(i))$

其中，i表示可能的事件或类别，p(i)和q(i)分别是实际概率和预测概率。交叉熵衡量的是在实际分布p下，用概率分布q来编码事件所需的平均比特数。当两个分布完全相同时，交叉熵为最小值0，否则交叉熵的值大于0。

在机器学习中，交叉熵通常用作损失函数，特别是在分类问题中，用于衡量预测值和真实标签之间的差异。通过最小化交叉熵损失，模型可以通过调整参数来优化预测概率分布，使其尽可能接近真实概率分布，从而提高模型的准确性和分类性能。

总结起来，交叉熵是一种用于度量两个概率分布之间差异的方式，常被用作分类问题中的损失函数，用于衡量模型的预测与真实标签之间的差异。

在信息论中，熵（Entropy）是衡量随机变量不确定性的度量方式。对于一个离散随机变量X，其熵可以通过以下公式计算：

$H(X) = -\sum_{x} P(x) \log(P(x))$

其中，x表示随机变量X的可能取值，P(x)表示随机变量X取值为x的概率。熵越大表示随机变量的不确定性越高。

KL散度（Kullback-Leibler Divergence），也称为相对熵，用于衡量两个概率分布之间的差异。对于两个离散概率分布P和Q，KL散度可以通过以下公式计算：

$D_{KL}(P|Q) = \sum_{x} P(x) \log\left(\frac{P(x)}{Q(x)}\right)$

KL散度测量了在真实分布P下，用分布Q来编码数据所需的额外比特数。KL散度的值大于等于0，当且仅当P和Q是相同的分布时，KL散度为0。

将熵和KL散度结合起来，可以得到交叉熵（Cross-Entropy）。对于两个离散概率分布P和Q，交叉熵可以通过以下公式计算：

$H(P, Q) = H(P) + D_{KL}(P|Q)$

可以看出，交叉熵可以看作是真实分布P的熵和KL散度的和。在机器学习中，当将交叉熵作为损失函数时，最小化交叉熵等价于最小化真实分布的熵和真实分布与预测分布之间的KL散度，从而使得模型的预测分布尽可能接近真实分布。

**对于二分类问题binary classification**
![[Pasted image 20230607171923.png]]
现在有分类问题,P代表真实分布 Q代表的是预测结果分布
![[Pasted image 20230607172121.png]]
这是预测狗的结果
如果使用了cross entropy的操作之后
![[Pasted image 20230607172151.png]]
如果使用直接输出的预测结果计算,得到的交叉熵结果是0.9左右很大
但是如果我现在的预测结果是狗的预测正确概率很高,跟原来的图相差很小,然后重新计算这个交叉熵损失函数就会把函数减小到0.02一个很小的值
这时候损失函数就可以很代表预测正确的结论

如果是使用MSE计算损失函数,这个结果会减小0.3左右,所以对于反向传播的方向计算是使用交叉熵损失函数会下降的更快

但是一般在实验中MSE计算起来非常简单,所以做一些新的探索的时候使用MSE是一个选择
一般来说softmax和crossentropy联合使用,不然容易出现数据不稳定的情况
```python
x = torch.randn(1, 784)
w = torch.randn(10, 784)

logits = x@w.t()

pred = F.softmax(logits, dim=1)
pred_log = torch.log(prded)

F.cross_entropy(logits, torch.tensor([3]))

F.nll_loss(pred_log, torch.tensor([3]))
```
上边就是自己处理的softmax和crossentropy的过程
这里cross_entropy不能使用前面的pred 因为已经在cross entropy功能中包含了softmax操作了
如果需要使用pred数据,需要保证先log之后使用nll_loss功能这样结果才是一样的
![[Pasted image 20230607173415.png]]
