```python
import numpy as np  
import pandas as pd  
  
pd.set_option("display.max_rows", None)  
pd.set_option("display.max_columns", None)  
pd.set_option("display.width", 1000)  
from sklearn import preprocessing  
from sklearn.linear_model import LogisticRegression  
from sklearn.feature_selection import RFE, RFECV  
from sklearn.model_selection import train_test_split, cross_val_score, cross_validate  
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss  
import matplotlib.pyplot as plt  
  
plt.rc("font", size=14)  
  
import seaborn as sns  
  
sns.set(style="whitegrid", color_codes=True)  # 设置背景风格，字体等  
import warnings  
  
warnings.simplefilter(action="ignore")  # 忽略警告信息  
  
# Read CSV train data file into DataFrame  
train_df = pd.read_csv("./titanic/train.csv")  
# Read CSV test data file into DataFrame  
test_df = pd.read_csv("./titanic/test.csv")  
# preview train data  
# print(train_df.head())  
  
# print(train_df.isnull().sum()) # 统计缺失的数据个数  
  
# # hist直方图  
# ax = train_df["Age"].hist(bins=15, density=True, stacked=True, color="teal", alpha=0.6)  
# # plt是线型图  
# train_df["Age"].plot(kind="density", color="teal")  
# ax.set(xlabel="Age")  
# plt.xlim(-10, 85)  
# plt.show()  
  
# 使用平均值或者中位值补齐  
# print("The mean of Age is %.2f" %(train_df["Age"].mean(skipna=True)))  
# print("The median of Age is %.2f" %(train_df["Age"].median(skipna=True)))  
  
# 补齐缺失值 train和test要一起一样处理  
train_data = train_df.copy()  
train_data["Age"].fillna(train_df["Age"].median(skipna=True), inplace=True)  
train_data["Embarked"].fillna(train_df["Embarked"].value_counts().idxmax(), inplace=True)  
train_data.drop("Cabin", axis=1, inplace=True)  
  
# 检查是否有缺失值了  
# print(train_data.isnull().sum())  
  
# 特征融合--创造新的特征  
train_data["TravelAlone"] = np.where((train_data["SibSp"] + train_data["Parch"]) > 0, 0, 1)  
train_data.drop("SibSp", axis=1, inplace=True)  
train_data.drop("Parch", axis=1, inplace=True)  
  
# 要把字符串转化为数字进行训练 get_dummies是创建one hot的方式  
training = pd.get_dummies(train_data, columns=["Pclass", "Embarked", "Sex"])  
training.drop("Sex_female", axis=1, inplace=True)  
training.drop("PassengerId", axis=1, inplace=True)  
training.drop("Name", axis=1, inplace=True)  
training.drop("Ticket", axis=1, inplace=True)  
  
# 处理完了之后的数据  
final_train = training  
# print(final_train.head())  
  
# 训练集的手段测试集要一样使用  
test_data = test_df.copy()  
test_data["Age"].fillna(train_data["Age"].median(skipna=True), inplace=True)  
test_data["Fare"].fillna(train_df["Fare"].median(skipna=True), inplace=True)  
test_data.drop("Cabin", axis=1, inplace=True)  
  
test_data["TravelAlone"] = np.where((test_data["SibSp"] + test_data["Parch"]) > 0, 0, 1)  
test_data.drop("SibSp", axis=1, inplace=True)  
test_data.drop("Parch", axis=1, inplace=True)  
  
testing = pd.get_dummies(test_data, columns=["Pclass", "Embarked", "Sex"])  
testing.drop("Sex_female", axis=1, inplace=True)  
testing.drop("PassengerId", axis=1, inplace=True)  
testing.drop("Name", axis=1, inplace=True)  
testing.drop("Ticket", axis=1, inplace=True)  
  
final_test = testing  
# print(final_test.head())  
  
# 数据分析过程  
# 会一个个进行特征分析，会首先画图，看变量和结果的关系  
plt.figure(figsize=(15, 8))  
ax = sns.kdeplot(final_train["Age"][final_train.Survived == 1], color="darkturquoise",  
                 shade=True)  # kdeplot核密度估计图，绘制变量y的分布，看看数据符合哪种分布，只用了解就行  
sns.kdeplot(final_train["Age"][final_train.Survived == 0], color="lightcoral", shade=True)  
plt.legend(["Survived", "Died"])  
plt.title("Density Plot of Age for Surviving Population and Deceased Population")  
ax.set(xlabel="Age")  
plt.xlim(-10, 85)  
# plt.show()  
  
plt.figure(figsize=(20, 8))  
avg_survival_byage = final_train[["Age", "Survived"]].groupby(["Age"], as_index=False).mean()  
g = sns.barplot(x="Age", y="Survived", data=avg_survival_byage, color="LightSeaGreen")  
# plt.show()  
  
# 观察可以看出是二次曲线的分布，这样可以分为两部分数据，分为大人和小孩，这样就把年龄也变化了  
final_train["IsMinor"] = np.where(final_train["Age"] <= 16, 1, 0)  
final_test["IsMinor"] = np.where(final_test["Age"] <= 16, 1, 0)  
  
# 接下来分析经济实力  
plt.figure(figsize=(15, 8))  
ax = sns.kdeplot(final_train["Fare"][final_train.Survived == 1], color="darkturquoise", shade=True)  
sns.kdeplot(final_train["Fare"][final_train.Survived == 0], color="lightcoral", shade=True)  
plt.legend(["Survived", "Died"])  
plt.title("Density Plot of Fare for Surviving Population and Deceased Population")  
ax.set(xlabel="Fare")  
plt.xlim(-20, 200)  
# plt.show()  
  
# 分析乘客的vip等级，经济实例  
# sns.barplot(data=train_df, x="Pclass", y="Survived", color='darkturquoise')  
# plt.show()  
  
# 分析从哪个港口上岸  
# sns.barplot(data=train_df, x="Embarked", y="Survived", color='darkturquoise')  
# plt.show()  
  
# 分析是否独自旅行  
# sns.barplot(data=final_train, x="TravelAlone", y="Survived", color='darkturquoise')  
# plt.show()  
  
# 分析男女比例  
# sns.barplot(data=train_df, x="Sex", y="Survived", color='darkturquoise')  
# plt.show()  
  
# 用逻辑回归模型对预处理数据进行训练  
cols = ["Age", "Fare", "TravelAlone", "Pclass_1", "Pclass_2", "Embarked_C", "Embarked_S", "Sex_male", "IsMinor"]  
X = final_train[cols]  
y = final_train["Survived"]  
# Build a logreg and compute the feature importances  
model = LogisticRegression()  
# create RFE model and select 8 attributes  
# 这个函数是把所有变量中选出8个特征出来  
rfe = RFE(model, n_features_to_select=8)  
rfe = rfe.fit(X, y)  
# summarize the selection of the attributes  
print("Selected features: %s" % list(X.columns[rfe.support_]))  
  
# 我们也可以使用交叉验证cross-validation的方法来消除无关变量  
# Create the RFE object and compute a cross-validated score  
# The "accuracy" scoring is proportional to the number of correct classifications  
rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring="accuracy")  
rfecv.fit(X, y)  
  
print("Optimal number of features: %d" % rfecv.n_features_)  
print("Selected features: %s" % list(X.columns[rfecv.support_]))  
  
# Plot number of features VS. cross-validation scores  
plt.figure(figsize=(10, 6))  
plt.xlabel("Number of features selected")  
plt.ylabel("Cross validation score (nb of correct classifications)")  
plt.plot(range(1, len(rfecv.cv_results_["mean_test_score"])+1), rfecv.cv_results_["mean_test_score"])  
# plt.show()  
# RFECV 和 RFE的区别 Recursive feature elimination（and cross-validation）  
# RFECV就是保证交叉验证找到的最优特征数量，如果减少特征数量会产生性能损失，就不会去除特征，这个方法适合于选择单模型特征  
# 如果采用不一样的学习器（评估器），比如交叉熵换成softmax，最佳组合可能改变，所以有时候会产生不同影响  
  
Selected_features = ["Age", "TravelAlone", "Pclass_1", "Pclass_2", "Embarked_C", "Embarked_S", "Sex_male", "IsMinor"]  
X = final_train[Selected_features]  
  
plt.subplots(figsize=(8,5))  
# heatmap 表示热力图 表明特征是强关联还是弱关联  
sns.heatmap(X.corr(), annot=True, cmap="RdYlGn")  
# plt.show()  
  
# 上面就是特征选择的内容  
# sklearn中有很多机器学习的库  
# 如何分配训练集和测试集占比  
X = final_train[Selected_features]  
y = final_train["Survived"]  
  
# use train/test split with different random_state values  
# can change the random_state values that changes the accuracy scores  
# the scores change a lot, this is why testing scores is a high-variance estimate  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)  
  
# check classification scores of logistic regression  
logreg = LogisticRegression()  
logreg.fit(X_train, y_train)  
y_pred = logreg.predict(X_test)  
y_pred_proba = logreg.predict_proba(X_test)[:, 1]  
# 通过sklearn的roc_curve函数计算false_positive_rate和true positive rate和对应的threshold  
[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)  
  
print("Train/Test split results:")  
print(logreg.__class__.__name__+"accuracy is %2.3f" % accuracy_score(y_test, y_pred))  
print(logreg.__class__.__name__+"log_loss is %2.3f" % log_loss(y_test, y_pred_proba))  
print(logreg.__class__.__name__+"auc is %2.3f" % auc(fpr, tpr))  
  
idx = np.min(np.where(tpr>0.95)) # index of the first threshold for which the sensibility > 0.95  
  
plt.figure()  
plt.plot(fpr, tpr, color="coral", label="ROC curve (area = %0.3f)" % auc(fpr, tpr))  
plt.plot([0, 1], [0, 1], "k--")  
plt.plot([0, fpr[idx]], [tpr[idx], tpr[idx]], "k--", color="blue")  
plt.plot([fpr[idx], fpr[idx]], [0, tpr[idx]], "k--", color="blue")  
plt.xlim([0.0, 1.0])  
plt.ylim([0.0, 1.05])  
plt.xlabel("False Positive Rate (1 - specificity)", fontsize=14)  
plt.ylabel("True Positive Rate (recall)", fontsize=14)  
plt.title("Receiver operating characteristic (ROC) curve")  
plt.legend(loc="lower right")  
plt.show()  
  
print("Using a threshold of %.3f" % thr[idx] + " guarantees a sensitivity of %.3f" % tpr[idx] +  
      " and a specificity of %.3f" % (1-fpr[idx]) +  
      ", i.e. a false positive rate of %.2f%%" % (np.array(fpr[idx])*100))  
  
# predict : model.predict()得到的预测结果是预测类别，如果是二分类就是0和1  
# predict_proba：得到的结果是多维数组，如果是二分类就是二维数组，第一列是样本预测0的概率，第二列是样本预测1的概率  
# roc_curve：roc区间就是关于fpr和tpr的图，因此计算auc就要知道fpr和tpr  
  
# tpr=tp/tp+fn称为灵敏度 fpr=fp/fp+tn 1-fpr称为特异度 roc曲线在左上角最好，我们可以通过这个确定大于0.几多少是阈值  
  
## k折交叉验证方法 使用cross_val_score来计算  
# 10折交叉验证逻辑回归 cross_val_score(): 交叉验证方法  
logreg = LogisticRegression()  
# 使用逻辑回归方程，将X和y整个的传入而不是X_train或者y_train  
# cv = 10就是10折  
# scoring = {"accuracy", "neg_log_loss", "roc_auc"} for evaluation metric -althought they are many  
scores_accuracy = cross_val_score(logreg, X, y, cv=10, scoring="accuracy")  
scores_log_loss = cross_val_score(logreg, X, y, cv=10, scoring="neg_log_loss")  
scores_auc = cross_val_score(logreg, X, y, cv=10, scoring="roc_auc")  
print("K-fold cross-validation results:")  
print(logreg.__class__.__name__+"average accuracy is %2.3f" % scores_accuracy.mean())  
print(logreg.__class__.__name__+"average log_loss is %2.3f" % -scores_log_loss.mean())  
print(logreg.__class__.__name__+"average auc is %2.3f" % scores_auc.mean())  
  
# accuracy 是返回正确分类的比例  
# log_loss是正样本和负样本的损失函数  
# auc是roc的面积大小  
  
  
## k折交叉验证使用cross_validate函数  
scoring = {"accuracy":"accuracy",  
           "log_loss":"neg_log_loss",  
           "auc": "roc_auc"}  
  
modelCV = LogisticRegression()  
results = cross_validate(modelCV, X, y, cv=10, scoring=list(scoring.values()),  
                         return_train_score=False)  
  
print("K-fold cross-validation results:")  
for sc in range(len(scoring)):  
    print(modelCV.__class__.__name__+"average %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], -results["test_%s" % list(scoring.values())[sc]].mean()  
          if list(scoring.values())[sc]=="neg_log_loss"  
          else results["test_%s" % list(scoring.values())[sc]].mean(),  
          results['test_%s' % list(scoring.values())[sc]].std()))
```