![[Pasted image 20230517172116.png]]
原始的方程可以转换为右边的全矩阵相乘的格式进行计算是深度学习中的计算格式
![[Pasted image 20230521164828.png]]
![[Pasted image 20230521165106.png]]
两个向量的乘积可以如图表示
如果需要转换为矩阵乘法就必须进行转置
![[Pasted image 20230521165813.png]]
这个是矩阵的乘法，两个结果是一样的但是计算不是一样的规则

向量和矩阵的乘法
![[Pasted image 20230521165924.png]]
一个1x2的矩阵乘以一个2x2的矩阵
结果是1x2的矩阵
结果就是
![[Pasted image 20230521170027.png]]

扩展到矩阵乘法
转置向量和矩阵是不同的操作，转置向量更多的像是在扩展向量的维度，将一维转换为多维向量
转置矩阵就是交换行和列
条件如下
![[Pasted image 20230521170338.png]]
深度学习计算中本来可以看作两个向量相乘，因为参数和输入的数值是一一对应的关系
但是由于有很多的向量需要在一步计算中获得，为了能够让其符合矩阵计算
所以需要进行转置
![[Pasted image 20230521170521.png]]

**矩阵乘法规则**
2x2的矩阵看起来不明显，现在我们转换一下
A是一个3个列向量组成的矩阵
![[Pasted image 20230521170646.png]]
再看一下W是什么组成的
因为A这边的输入是2个元素成一个列向量
所以W为了可以跟这个对应上计算所以也需要两个参数狗成一个参数的列向量
这样才能满足参数跟数据一一对应的关系
![[Pasted image 20230521170839.png]]
可以看作a1向量和w1向量点乘的结果就构成了这个函数的一个点
![[Pasted image 20230521171151.png]]
框出来的就是两个向量结果在结果矩阵中的位置
现在随便指定3行2列的数据计算，或者是2行3列的数据计算
![[Pasted image 20230521171300.png]]
所以这里就是抓住前边的第二行和后边的第三列进行计算
![[Pasted image 20230521171429.png]]
![[Pasted image 20230521171439.png]]
最后的结果如上图所示
前边的矩阵的列向量必须等于后边矩阵的行向量，因为两者的向量计算必须要能元素对应上才能进行向量计算
![[Pasted image 20230521171548.png]]

**神经网络中的矩阵计算代码实现**
![[Pasted image 20230522114455.png]]
对A和A转置的代码在numpy中构成为
```python
A = np.array([[1,-1,0.1],[2,-2,0.2]])
AT = np.array([[1,2],[-1,-2],[0.1,0.2]])
AT = A.T
```
![[Pasted image 20230522114747.png]]
对w的代码
```python
W = np.array([[3,5,7,9],[4,6,8,0]])
```
![[Pasted image 20230522114848.png]]
计算过程变成
```python
Z = np.matmul(AT,W)
```
![[Pasted image 20230522114937.png]]
```python
Z = AT @ W
```
也可以表示一样的矩阵计算方式

在前向传播中计算
![[Pasted image 20230522115356.png]]
```python
AT = np.array([[200,17]])
W = np.array([[1,-3,5],[-2,4,-6]])
b = np.array([[-1,1,2]])

def dense(AT,W,b,g):
	z = np.matmul(AT,W) + b
	a_out = g(z)
	return a_out
```