![[Pasted image 20240206213037.png]]
k-means聚类算法:
- 首先选择两个不同的中心点, 完全随机的
- 会重复做两件事
- 第一是将点分配给集群的质心
- 第二是移动集群质心
- 第一步是遍历所有点,看是否接近红色或者蓝色十字
![[Pasted image 20240206213253.png]]
- 做的第一件事就是猜测cluster的中心,叫做cluster centroid
- 然后将每一个点分配给更近的cluster centroids
![[Pasted image 20240206213412.png]]
- 第二部是求红点的平均值,然后将红十字移动到红点的品均位置
![[Pasted image 20240206213503.png]]
- 然后重复刚才的聚类过程不断调整质心
![[Pasted image 20240206213632.png]]
- 慢慢的会稳定出来两个稳定的收敛中心
![[Pasted image 20240206213730.png]]

如何实现K-means的算法
- 随机初始化两个cluster centroids
![[Pasted image 20240206213909.png]]
每次都要计算$\mu_k$ 这个和dataset的数量相同的dimension, k是设定的维度数量也就是特征数量
这里我们设定的是training examples是由2个特征维度
所以我们初始化随机生成的$\mu$也是2个特征维度,因为是根据所有点求质心得到的

![[Pasted image 20240206214513.png]]

![[Pasted image 20240206214624.png]]
上面计算质心的过程可以简化成求$\mu$和$x$的距离
这其实是计算L2范数
然后需要最小化这个距离
![[Pasted image 20240206214722.png]]

所以我们只要设定更近的那个值是属于1还是2就行
![[Pasted image 20240206214901.png]]

![[Pasted image 20240206215023.png]]
第二步是移动质心,这时候需要的是我们要把1和2两个聚类分开来算
两个part分别求xy坐标的平均值

![[Pasted image 20240206215223.png]]
例如1,5,6,10是作为归类于1的点,就应该如上式计算

如果有一个极端情况就是一个聚类并没有任何分配的内容
![[Pasted image 20240206215343.png]]
一个方法就是重新随机初始化
另一个办法是实在没有点分配到簇上面,那就减少1个簇

K-means的应用场景
对于某些没有明显特征的内容也能进行分类
![[Pasted image 20240206215547.png]]

![[Pasted image 20240206215646.png]]
这个就可以很好给T-shirt设计师设计SML三款T-shirt大小

