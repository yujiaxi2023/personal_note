![[Pasted image 20230502160101.png]]
模型做的事情就是有输入的x的时候，通过计算出的w和b得到y的结果是1还是0

![[Pasted image 20230502160152.png]]
从梯度下降中，我们有cost function，为了最小化这个cost function
图中是最常见的梯度下降算法

这里的wj代表有j个参数
![[Pasted image 20230502160449.png]]
![[Pasted image 20230502160509.png]]
将cost function对于wj也就是参数求偏导，可以得到右边的式子
这里的i才是训练集中的例子对应的序号，前面的j是代表的第几个参数w
![[Pasted image 20230502160640.png]]
上图中的式子是b的偏导
更新是w和b同时更新的

可以发现这个求偏导之后的式子和线性回归的式子是一样的，至少在不带入f的情况下是一样的
![[Pasted image 20230502160825.png]]
但是带入f之后就不一样
![[Pasted image 20230502160843.png]]
可以通过监测学习曲线观察梯度下降什么时候达到合适的值
也可以通过向量化让运算变得更快速
![[Pasted image 20230502161025.png]]
同时我们可以使用特征缩放，正则化的方式让 其更快收敛
![[Pasted image 20230502161128.png]]
[[Feature scaling]]
![[Pasted image 20230502161725.png]]
