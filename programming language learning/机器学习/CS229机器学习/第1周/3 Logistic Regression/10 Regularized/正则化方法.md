**线性回归中的正则化方法**

![[Pasted image 20230504190452.png]]
首先我们有一个均方误差的方程加上正则化项
我们需要最小化这个式子采用梯度下降的方式
![[Pasted image 20230504190538.png]]
![[Pasted image 20230504190543.png]]
通过不断地更新w和b这两种参数，通过求cost function的偏导数，减去偏导数可以帮助更新w和b这些参数
![[Pasted image 20230504190636.png]]
这是原来对linear regression进行梯度下降的方式
对于我们新的cost function J 我们的梯度下降的式子就会变成
![[Pasted image 20230504190759.png]]
我们要注意，我们一般对b不会添加正则化项，因为实际应用中没有用
所以新的b的梯度下降和原来的一样
![[Pasted image 20230504190844.png]]
展开我们的式子就会是上图中的式子重复，并且同时更新


![[Pasted image 20230504191023.png]]
我们可以从数学上面推导一下
把wj式子展开之后调换第二和第三项的顺序就会变为上图中的式子
合并同类项
![[Pasted image 20230504191154.png]]
![[Pasted image 20230504191201.png]]
变为这两个式子的和
我们可以发现后边的是一个不用正则化的线性回归正常梯度下降usual update

我们可以分析第一个多项式
我们知道α学习率是一个很小的数字，λ也不能是一个很大的数，数据集一般也比较大，比如有50个训练数据
![[Pasted image 20230504191455.png]]
每次进行迭代中，就是乘以一个略小于1的数
![[Pasted image 20230504191535.png]]
这就是正则化工作的原理，每次乘以一个略微小于1一点点的数去收缩wj

![[Pasted image 20230504191656.png]]
我们可以试着对wj求偏导，式子如图所示
![[Pasted image 20230504191759.png]]
这样我们可以推到出最后的这个式子，这就是我们为什么使用这个正则化之后的方程进行梯度下降


**逻辑回归中的正则化方法**
![[Pasted image 20230504192102.png]]
我们使用过多特征的函数去描述决策边界就会产生过拟合boundary
![[Pasted image 20230504192136.png]]

![[Pasted image 20230504192152.png]]
这是我们原本用于分类问题的逻辑回归方程，我们需要的是在后边增加一个正则化项
![[Pasted image 20230504192248.png]]
![[Pasted image 20230504192314.png]]
经过正则化之后，决策边界将更加的圆滑，泛用性更好

同样的过程，我们有式子如下图所示
![[Pasted image 20230504192401.png]]
按照正常的更新步骤是如下图进行梯度下降
![[Pasted image 20230504192433.png]]
![[Pasted image 20230504192452.png]]
上图就是增加了正则化项的梯度下降的式子
