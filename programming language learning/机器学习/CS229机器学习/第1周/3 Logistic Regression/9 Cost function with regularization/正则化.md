![[Pasted image 20230504162847.png]]
我们有两个拟合曲线，一个是比较正常的，一个是过拟合的
![[Pasted image 20230504162936.png]]
目标是尽量减少w3和w4
![[Pasted image 20230504163037.png]]
这里我们假设在linear regression的cost function后面增加两个特征值分别如图所示，这是有可能的，当我增添了描述这个事件的另两个影响力比较小的特征后，为了代偿影响力的等级不同，所以前面的常数项就会很大

![[Pasted image 20230504163322.png]]
为了减少这个巨大的常数项的干扰，我们需要将w3和w4尽量等于0
![[Pasted image 20230504163611.png]]
在我们获得了很多个特征量，比如说一百个，我们此时不知道什么是应该惩罚的什么是不应该惩罚的特征量
regularization就是penalize所有的w参数
正则化可以让模型更简单，更加平滑，且不容易过度拟合
![[Pasted image 20230504163917.png]]
这里的λ＞0
![[Pasted image 20230504164040.png]]
把b正则化通常是没有什么用的
![[Pasted image 20230504164246.png]]
为了让cost function尽可能的减小，需要保证第一项均方误差的尽可能小去拟合我们的采样点，然后第二项中的正则化需要尽可能的让wj更小
![[Pasted image 20230504164434.png]]
如果我们设置λ是0，那就会过拟合
如果λ设置为很大，比如100000000000
拟合曲线就会变为f = b
![[Pasted image 20230504164532.png]]
这样模型是欠拟合的

所以我们在J cost function中不仅包含有loss function的和，还需要包含一个正则化项让我们的参数变多的情况下，模型过拟合导致泛化性能变差

所以我们需要的λ是一个中间值
![[Pasted image 20230504164731.png]]
