![[Pasted image 20230503204700.png]]
现在我们有一个hign variance的模型

一种直接的解决方法是使用更多的training data
![[Pasted image 20230503204801.png]]
![[Pasted image 20230503204915.png]]
这样可以训练出一个不是那么摇摆的模型

解决过拟合可以是用更少的特征feature
![[Pasted image 20230503205029.png]]
![[Pasted image 20230503205057.png]]
如果我们的特征很多，但是训练数据又不足够，也可能造成过拟合
这个时候就需要选择出影响最大的条件
![[Pasted image 20230503205202.png]]
这种过程称为 feature selection
代表着如果不选择出影响最大的条件，同时数据量不足够的话，那就会产生过拟合，**这是否可以代表着，找出现有的数据中和原方程中最相似的可以替代的特征是一种潜在的研究方向**

这种缺陷会导致影响的信息确实，比如一百个因素都是影响房价的，但是为了减少overfitting我们会丢失一部分内容

![[Pasted image 20230503205508.png]]
可以发现这些过拟合的模型方程，前面的数字都很大
如果我们设置一个feature是0 ，也就代表着 不考虑这个feature
![[Pasted image 20230503205602.png]]

这个时候regularization是一种不是完全丢失掉一种信息的选择
regularization是一种算法让模型的参数尽量的小，而不是让参数降低为0
![[Pasted image 20230503205736.png]]
regularization是让所有的feature保留能够更好的泛化模型

可以解决过拟合的方法
1. 收集更多的数据
2. 选择特定的特征-feature selection
3. 降低参数的大小-regularization
