![[Pasted image 20230419141359.png]]
梯度下降方法不仅是适用于线性回归的cost function
可以适用于更复杂的function，例如在深度学习中的有很多的参数w的方程
在线性回归的梯度下降中我们需要设置一个初始值w 和 b
w和b是可以随意设置的，例如从w = 0 b = 0开始设置
![[Pasted image 20230419141634.png]]
我们还需要注意在图示的这种有不止一个极小值的时候

我们线性回归的方程都是一个碗装或者弓形的函数

在不止一个极小值的函数中，我们的初始值和下降方向不同，导致的极小值也是不同的
![[Pasted image 20230419142124.png]]

**梯度下降算法**
![[Pasted image 20230419142247.png]]

前一个**参数w** 减去 **学习率α** 乘以cost function的微分，也可以称为偏导数，反正求值方法是一样的

![[Pasted image 20230419142546.png]]
左边是代表赋值的等号，右边是数学的等号
如果在编程的时候 数学的等于会写成==

![[Pasted image 20230419142914.png]]
b也是类似的求法，但是是求对b的偏导

然后就是重复这两个计算过程，直到函数收敛到一个极小值，或者称局部最小值
我希望在算法中能够同时更新我的所有参数

写代码的时候为了能够实现更新的同时进行
![[Pasted image 20230419143309.png]]
我们需要注意这里的参数更新必须是之前一步的梯度下降过程结束之后才能更新

![[Pasted image 20230419195823.png]]
我们可以看到求偏导之后参数会逐渐收敛到极小值

**学习率α**
学习率太小会很久才能到最小值
如果学习率太大，可能会出现跳过最低点的情况，也会出现不到最低点的方法
![[Pasted image 20230421132030.png]]
从专业术语上来说，就是收敛到最低点的速度太慢，或者是无法收敛converge，甚至是发散diverge

有的时候会达到一个局部最低点local minimum
当我们的函数不是均方误差square error function这种只有一个最低点的函数，而是复数个最低点
![[Pasted image 20230421132355.png]]
我们可以采用固定的学习率达到收敛
因为学习率还要乘以导数，当导数变得平滑的时候，变缓的时候导数就变小，这样会逐渐的缩小达到收敛数值的速度
![[Pasted image 20230421132541.png]]

![[Pasted image 20230421132743.png]]
当我们的cost function是均方误差函数的时候，我们进行偏导数的求导
![[Pasted image 20230421133243.png]]
简单的求偏导数的过程

我们要达到的是全局最低点global minimum不是局部最低点local minimum
均方误差函数是convex function，凸函数，也就是由最小值的函数
concave function是凹函数，也就是有最大值的函数

![[Pasted image 20230421134219.png]]
“batch” gradient descent
批量梯度下降
我们是选择一个batch作为训练集

