![[Pasted image 20230417151130.png]]
wb作为参数,可以称为权重或者是系数
选择不同的权重可以形成不同的线性图像
![[Pasted image 20230417151416.png]]
我们有一个traing set在这里
![[Pasted image 20230417151445.png]]
我们需要有一个能够描述这个训练集的线性模型
![[Pasted image 20230417151631.png]]
我们需要用一个cost function去进行误差比较,也就是预测值和实际值之间的差别

为了不让cost function因为训练集数量增大而无限增大,我们需要进行平均平方误差,平均平方误差是一种cost function,cost function我们用J来表示这个函数
![[Pasted image 20230417151929.png]]
平均平方误差可以最好的应用于线性回归问题
![[Pasted image 20230417152101.png]]
就是要找出一个w和b来让这个cost function更小

![[Pasted image 20230417155139.png]]
计算cost function的时候可以简化问题
![[Pasted image 20230417155310.png]]
利用这个简化模型可以找到w的最小值
![[Pasted image 20230417155900.png]]
我们可以绘制两张图,第一是线性回归的简化方程,就是过原点的直线,当w取值为1的时候,cost function的取值是0,所以在右侧图像中,可以找到一个点1,0作为J(W)函数过的一个点
这里的w可以取任意值,所以取别的值足够多可以描述出J(w)的图形
![[Pasted image 20230417160212.png]]
![[Pasted image 20230417160252.png]]
![[Pasted image 20230417160322.png]]
最后可以拟合出J(w)的函数图像
实际上,我们可以通过原函数直接求得图像,本次的cost function就是一个双曲线
然后我们需要求得这个最低点,作为参数取得最小的情况,我们的cost function就最小,这个线性回归的例子是比较简单的cost function,当更加复杂的现实情况的时候,就可能不只有一个低谷的地方,不止一个极小值,这个时候我们要设置学习率榜之我们的模型能够更好的求出最小的cost function

![[Pasted image 20230418190001.png]]
现在的模型是由偏置项b的
添加了b之后就比较像一个三维空间的一个有一个最低点的函数
![[Pasted image 20230418190349.png]]
![[Pasted image 20230418190829.png]]
如果下面是我们的cost function，这里蓝橙绿的三个点虽然w和b这两个参数不一样，但是拥有一样的cost function j

![[Pasted image 20230418191122.png]]
当我们选择 w是-0.15 b是800的时候，它对应的右边的图的坐标，可以从三维降到二维进行可视化，就可以得到一条等高线，但是这里距离最低点还是有一定的距离

![[Pasted image 20230418191449.png]]
此时我们选择的是0和360两个参数

![[Pasted image 20230418191525.png]]
这个已经到非常接近的数值了w取值0.13 b取值71

通过梯度下降可以得出w和b自动化的