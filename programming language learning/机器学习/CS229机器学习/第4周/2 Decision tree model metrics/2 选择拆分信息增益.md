熵的减少称为information gain 也就是信息增益
减少熵意味着增加数据的纯度
按照最初的猫狗分类的示例
按照不同的特征分类得到的熵的结果并不相同
如果比较两个分支的H，是一个困难的选择
需要综合比较两个分支的H，所以需要进行加权平均得到最终的一个index进行比较
这个加权平均需要比较的对象root node 的纯度和H
因为经过分类器之后 H 一定会下降
所以取两者的差值，差值最多代表的是降低这个混乱度最多的分类方式
这个称为information gain
![[Pasted image 20230711142116.png]]
决定是否需要继续拆分，就是看这个entropy减少的是否足够多
计算information gain的公式
定义左分支的阳性比率是p1left，wleft代表的是去左边分支的数量占总输入的比例
例如左边分类 得到了5个案例，原数据集有10个示例，这就是5/10
如果左边得到了7个，原数据集有10个示例，就是7/10
同样方式定义右边的参数
![[Pasted image 20230711142533.png]]
定义root的纯度为p1root
information gain 的公式就如图所示
![[Pasted image 20230711142636.png]]
information gain越高代表的是选择的node越能提高数据集的纯度
