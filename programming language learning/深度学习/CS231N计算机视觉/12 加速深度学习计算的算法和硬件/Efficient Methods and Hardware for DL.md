![[Pasted image 20230330173739.png]]
主要是在保证精度的前提上，降低能耗，训练时间
![[Pasted image 20230330182237.png]]
可以看到浮点算法等消耗的能量并不高，但是读取DRAM的内存是非常耗能的
![[Pasted image 20230330182500.png]]
![[Pasted image 20230330182517.png]]
利用四个象限进行训练数据加速
![[Pasted image 20230330183857.png]]
可以 用不同的bite数表示浮点数，可以是64 32 16个浮点
CPU可以支持不同精度的浮点运算GPU中只能计算单独的计算单元对应的浮点运算

**加速推断的算法**
![[Pasted image 20230330194027.png]]

**purning**
![[Pasted image 20230330194115.png]]
在训练过程中使用的是dropout的思想，利用这个思想进行推断过程的优化
其实是起到正则化的效果
首先 训练模型，训练完毕后修建神经元或树突，然后训练保留下的模型
![[Pasted image 20230330194506.png]]
可以看到剪掉连接之后准确率下降，然后进行重新训练后，准确率会有回升
![[Pasted image 20230330194558.png]]
在该案例中减去了80多的连接准确率依然没有降低
然后我们可以不断地迭代的减去连接，重新训练，但是减去过多又会存在有欠拟合的情况
![[Pasted image 20230330194735.png]]
![[Pasted image 20230330194809.png]]
类似仿生学，当人出生到1岁，大脑突触最多，成年后反而减少
![[Pasted image 20230330194858.png]]
可以看到，我们原来的模型是一个连续的值，减去一部分的神经元和连接之后成为了一个中间空开的不连续的值，重新训练之后又成为了一个连续的值
其实本质上是引入了稀疏性，保留了特化的主要神经元，让不参与识别的神经元可以减少作用

**weight sharing**
![[Pasted image 20230330200142.png]]
我们可以让2附近的权重更换为2
![[Pasted image 20230330200209.png]]
首先我们可以进行聚类，然后判断出一个数来表示所有的权重，因为这些2左右的数字都是来自于不同的地方，所以我们需要把这些数值记录在 一个codebook上，记录下他们的位置 ，最后合并之后训练

![[Pasted image 20230330201256.png]]
如图所示，每个颜色代表的聚类不相同，我们将这些聚类变为2，1.5，0，-1.0等，然后给这四个数字编上号，编号之后再原图中标注出这些序号，就能表明他们的位置
这样，这个code book 4x4大小只占用了2bit
![[Pasted image 20230330201539.png]]
![[Pasted image 20230330201552.png]]
将这些 区域的梯度计算出来后累加，然后根据反向传播修正原来的数据
![[Pasted image 20230330201615.png]]
原来的权重变为离散的状态
![[Pasted image 20230330201850.png]]
![[Pasted image 20230330201916.png]]
训练后的数据是对于这个训练前的进行微调
![[Pasted image 20230330202014.png]]
文章中是用4bits就可以表示卷积层，2bits表示全连接层
![[Pasted image 20230330202102.png]]
当purning操作和quantization操作一起使用的时候，在alexnet模型中表现是压缩的更多
![[Pasted image 20230330202346.png]]
![[Pasted image 20230330202406.png]]
出现频率高用的比特少
![[Pasted image 20230330202549.png]]
这是一种储存方式，我们从小到大进行数据的存储，首先进行的是最底层的1，然后逐渐上升，当中间的连接越多，比特数就越多，代表的就是出现越多的数采用的比特数就越少
[霍夫曼编码树（Huffman Tree）是一种特殊的二叉树，它是基于霍夫曼编码的一种编码方式。霍夫曼编码是一种压缩编码的编码算法，它是带权路径长度最短的二叉树](https://zhuanlan.zhihu.com/p/415467000)[1](https://zhuanlan.zhihu.com/p/415467000)[。霍夫曼编码与ASCII编码不同，ASCII编码是对照ASCII表进行的编码，每一个字符符号都有对应的编码，其编码长度是固定的。而霍夫曼编码对于不同字符的出现频率其使用的编码是不一样的。其会对频率较高的字符使用较短的编码，频率低的字符使用较高的编码。这样保证总体使用的编码长度会更少，从而实现到了数据压缩的目的](https://zhuanlan.zhihu.com/p/415467000)[1](https://zhuanlan.zhihu.com/p/415467000)。
![[Pasted image 20230330203428.png]]
![[Pasted image 20230330203458.png]]
![[Pasted image 20230330203602.png]]
压缩模型到35倍以上
![[Pasted image 20230330203644.png]]
先把1x1卷积降维，线降到S1维，然后用两种方式升维，可以有效地减少运算量
![[Pasted image 20230330204634.png]]
将每个权重和激活值变为浮点数进行训练，将每个权重和激活值分为256段表示，这就是8bit的数据，利用这种方式可以减少消耗的内存量
![[Pasted image 20230330204517.png]]
用低秩矩阵替换满秩矩阵
![[Pasted image 20230330204949.png]]
可以把全连接层这一长条矩阵变为许多小矩阵的乘法可以降低计算量
![[Pasted image 20230330205029.png]]
3值量化就是用-1 0 1三个数值表示权重
![[Pasted image 20230330205127.png]]
用原来的数据训练的出结果，然后分成三段后用三个数值表示，然后这三个数值可以带上权重
也就是缩放因子
![[Pasted image 20230330205307.png]]
用这种抽象方式得出的卷积核，可以看出提取转角特征，边缘特征等等，跟原始的细粒度的模型是类似的内容
![[Pasted image 20230330205539.png]]
输入和卷积核映射到同样大小的空间后进行相乘之后转回原来的结构
![[Pasted image 20230330205705.png]]
![[Pasted image 20230330210230.png]]
![[Pasted image 20230330210346.png]]
对于训练时候算法的优化
进行并行运算
![[Pasted image 20230330210426.png]]
数据并行是采取的权重是每个节点都对应有一套完整的
模型并行是参数和节点相互对应
![[Pasted image 20230330210543.png]]
![[Pasted image 20230330210635.png]]
可以训练的时候进行16位的训练然后返回32位的权重
![[Pasted image 20230330210711.png]]

![[Pasted image 20230330210810.png]]
知识蒸馏的方法，用很多预训练的模型去训练新模型
![[Pasted image 20230330210910.png]]
原始的one-hot标签，用多个预训练的模型进行预测，但是优秀的模型预测错的分数很小，我们要让他们能够在同一级别进行讨论就需要小的放大，大的缩小，但是保留相对应的关系
![[Pasted image 20230330211111.png]]
就是在分母地方增加一个数，作为蒸馏的温度，让其形成对于softmax分类的一个占比问题，最后把T变回1就是一个softmax分类函数
这样用3%的数据就可以训练一个很好的学生模型
![[Pasted image 20230330211339.png]]
两个损失函数叠加后形成最终的损失函数
![[Pasted image 20230330211511.png]]
DSD训练就是密集稀疏密集的形式训练可以实现更高的准确率