专门用于处理序列数据
可以保留先后发生的记忆
![[Pasted image 20230331163407.png]]
![[Pasted image 20230331163430.png]]
描述一段文字将其分类
![[Pasted image 20230331163517.png]]
多语言翻译就是机器翻译
![[Pasted image 20230331163613.png]]
过去的帧和未来是相关的,所以保留了过去的内容,对现在内容产生影响
![[Pasted image 20230331163716.png]]
按照某个序列读这个数据,产生图像分类的效果
![[Pasted image 20230331163806.png]]
也可以按照序列生成数字
图像生成还可以使用GAN,变分自动编码器来生成
![[Pasted image 20230331163911.png]]
![[Pasted image 20230331163918.png]]
过去的输出,是内部隐含状态
![[Pasted image 20230331163943.png]]
上一个时刻的记忆和当前的输入形成新的记忆
![[Pasted image 20230331164008.png]]
它是沿时间维度的权值共享
![[Pasted image 20230331164048.png]]
上一时刻的权重矩阵,当前输入的权重矩阵,和当前时刻的输出的矩阵,这个权重是共享的
![[Pasted image 20230331164246.png]]
随机初始化隐含状态,乘以权重加上x乘以权重之和,再乘以一个权重就是输出,循环下去
这当中三个权重是不会发生变化的 
反向传播是沿着时间维度传回去
![[Pasted image 20230331165428.png]]
BPTT算法
![[Pasted image 20230331165556.png]]
![[Pasted image 20230331165627.png]]
语言模型是用上文预测下文
![[Pasted image 20230331165958.png]]
首先把字母编码为独热向量,得到隐含状态的三个神经元各自输出一个隐含状态
![[Pasted image 20230331170116.png]]
然后由隐含状态形成一个多分类的结果,希望对应的绿的值是最大的,输入h让e的输出概率尽可能的最大,就可以源源不断的生成后边的内容
![[Pasted image 20230331170508.png]]
时间序列是一个很长的序列,每个节点损失函数相加就是总的loss function
因为是一个总的损失函数,是来自于不同的分支,所以损失函数的梯度就需要从上到下的求,就是先求出每一个分支对应的梯度,然后加起来
![[Pasted image 20230331170732.png]]
真实情况下,是对于这些序列数据截成很多个片段
![[Pasted image 20230331170912.png]]
自动生成莎士比亚文章的模型,上图是学习过程中得出的结果
![[Pasted image 20230331172002.png]]
image captioning模型,卷积抽取图片特征,得到向量,乘以权重作为初始隐含状态,start输入进去循环神经网络输出一个文字描述
编码器解码器的结构是把不同数据类别进行转换使用的
![[Pasted image 20230331173020.png]]
使用VGG16模型,将模型的最后全连接层的4096个作为initialization
![[Pasted image 20230331173351.png]]
CNN feature maps数据进入RNN中当前时刻输出L维的向量,这个权重告诉你feature map中哪一块的值得关注
![[Pasted image 20230331173603.png]]
![[Pasted image 20230331173703.png]]
如果是包含各个位置的权重,就是会关注整体,如果只包含最大的权重,就是一个点
他会根据生成的词,格外关注这个位置的信息,这是生成的结果得出的结论,并非是开始的目的
可以保留图像中的位置信息
![[Pasted image 20230331173948.png]]

长短时记忆神经网络
![[Pasted image 20230331174040.png]]
RNN堆叠多层隐含层后容易出现梯度消失现象
这个可以用LSTM来解决
![[Pasted image 20230331174134.png]]
wx和ws来说,需要求导的式子越来越长,每一项×的数越多,可以写成右上角的形式,当这个连乘的数值大于或者小于1,就会出现梯度消失和梯度爆炸的情况
![[Pasted image 20230331174604.png]]
我们可以进行惩罚的方式
![[Pasted image 20230331174633.png]]
用长短时记忆神经网络或者门控循环单元改进
LSTM上面是长期记忆,下面是短期记忆
存在 遗忘门和输出门
![[Pasted image 20230331174755.png]]
![[Pasted image 20230331174859.png]]
这一部分是乘以一个0-1之间的数,让网络忘掉某一个记忆,越大记忆越深
![[Pasted image 20230331175002.png]]
这里是增加一段记忆
先忘记在增加新的记忆
![[Pasted image 20230331175032.png]]
长期记忆只有忘记和增加两个操作
![[Pasted image 20230331175058.png]]
Ct就是长期记忆
![[Pasted image 20230331175120.png]]
遗忘门输入当前时刻的x和上一时刻的短期记忆,这里有一个权重W,进行函数处理加一个偏置项
![[Pasted image 20230331175215.png]]
用另一套权重,经过函数输出这里有一个W,形成i值,还有形成新的Ct的权重
然后这个i和新的C相乘就是新输入的记忆
![[Pasted image 20230331175443.png]]
上面一路长期记忆就完成了
![[Pasted image 20230331175512.png]]
这里使用第四套权重将新输入值,把原来的记忆乘以tanh,最后相乘就成为了短期记忆
![[Pasted image 20230331175852.png]]
![[Pasted image 20230331175911.png]]
梯度从长期记忆方向是方便传回去的
![[Pasted image 20230331180030.png]]
RNN和CNN中运用到是同样的技巧避免梯度消失,就是添加之前的内容让其至少能保存原来上一层神经网络的记忆
GRU是一个重置门和一个激发门
![[Pasted image 20230331180234.png]]
