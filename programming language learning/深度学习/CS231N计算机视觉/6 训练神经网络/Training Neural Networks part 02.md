
![[Pasted image 20230328144330.png]]
![[Pasted image 20230328144555.png]]
传统随机梯度下降的缺点是会在梯度交大的方向发生震荡，如果从学习率减小的角度解决问题就有可能在应该减小的方向更小了，所以也不合适
![[Pasted image 20230328144806.png]]
会陷入局部最优点
![[Pasted image 20230328145033.png]]
随机梯度下降方法SGD容易找到局部最优点就不移动了
![[Pasted image 20230328145144.png]]
找到一批数据的损失函数然后求得这一批损失函数对于每一个权重的梯度
![[Pasted image 20230328145343.png]]
![[Pasted image 20230328145552.png]]
绿色的小球就是惯性动量添加之后的方式
其中的dx是为了抵消掉朝着较大梯度方向运动的作用
![[Pasted image 20230328150004.png]]
计算出下降的速率，然后和这个点的梯度，然后进行矢量和就可以得到实际的下降方向
![[Pasted image 20230328150132.png]]
先按照速度的方向前进，然后在终点处计算梯度，这可以帮助模型提前感知到坡度的平缓，避免增加的梯度较大导致偏离最低点
![[Pasted image 20230328150320.png]]
紫色的线就是比绿色的线更加提前一步感知到前进的方向
![[Pasted image 20230328150438.png]]
这里的gradient红色的箭头是梯度的反方向
其中rho是摩擦力
![[Pasted image 20230328150829.png]]
但是按照并不是 按照实际的公式写的代码，是按照上图的公式写出的密码
用摩擦力减去两点之间的速度差然后加上前一个点的坐标加上新的速度
![[Pasted image 20230328150949.png]]
这个模型的惩罚量为梯度的平方，但是随着训练的层数 增加，容易导致最后变化量越来越小
![[Pasted image 20230328151308.png]]
下面的模型是添加了decay rate这个量
![[Pasted image 20230328151423.png]]
这个图展现了rmsprop和adagrade的差别，adagrade在下降过程最开始的阶段和rmsprop没有多大差别，甚至于rmsprop的下降速率更慢
但是到了末尾阶段rmsprop逐渐的超过了adagrad的下降速率
![[Pasted image 20230328151603.png]]
adam算法
![[Pasted image 20230328151950.png]]
这个算法考虑了momentum动量算法和后面的rmsprop的动量惩罚两个方面
其中第一动量代表了添加了rho的动量叠加
adam里面有beta1和2两个超参数 
![[Pasted image 20230328152519.png]]
这里的first-unbias会让动量开始的时候有一个比较高的值
![[Pasted image 20230328152700.png]]
这里的学习率和两个超参数的一般取值

**学习率**
![[Pasted image 20230328152936.png]]
![[Pasted image 20230328153027.png]]
如何选择学习率的原理
![[Pasted image 20230328153059.png]]
在resnet残差网络中采用的方式是每30轮将学习率乘以0.1
![[Pasted image 20230328153158.png]]
![[Pasted image 20230328153216.png]]
设定函数让学习率一开始下降快后边下降的慢
上面介绍的都是一阶导数的优化算法，就是计算一阶偏导数和一阶梯度，其实用二阶导其实可以看出速度的变化情况，其实可以更快的考虑收敛情况

![[Pasted image 20230328153855.png]]
二阶导数可以快速找到一阶导数为0 的情况，此时就可以找到损失函数的极值点
![[Pasted image 20230328153942.png]]
二阶优化方法不需要设置学习率，而且可以考虑加速度
但是深度学习的计算量会变得非常大，因为海森矩阵的逆矩阵计算量很大，所以一般用不到二阶优化方法
![[Pasted image 20230328154233.png]]
此时有拟牛顿法用一个矩阵去替代海森矩阵的逆矩阵，就是找一个可以替代的项去减少计算量
![[Pasted image 20230328154506.png]]
需要降低过拟合，更好的优化算法是可以减少训练损失的
![[Pasted image 20230328154622.png]]
当出现过拟合现象的时候可以early stopping
![[Pasted image 20230328154700.png]]
进行模型集成，需要保证各个模型都是有较高的学习率，而且是需要进行不同的数据分布进行学习
SVM训练就是决策树随机森林模型，进行有放回的抽样进行的决策树的训练
![[Pasted image 20230328155026.png]]
上述是将学习率周期性增大来计算不同的局部最优点的方法，这样每次跳到更大的学习率产生的局部最优点，都可以看作为一个新的模型，还有可以利用训练过程中不同时刻的模型进行集成，例如使用第98轮和第99轮的数据进行集成，成为checkpoint的集成方法，但是这种方法实际上不能满足采用了不同模型的这个原则
![[Pasted image 20230328155346.png]]
正则化
**dropout**
![[Pasted image 20230328155450.png]]
随机的在前向过程中掐死一部分神经元
![[Pasted image 20230328155532.png]]
在输入过程中在数据的上方添加一个mask， 在mask上有符合伯努利分布也就是二项分布的孔，下面的数据可以透过这个空，就可以进行到下一步的计算，否则将变为0
![[Pasted image 20230328155607.png]]
为什么dropout可以实现正则化减小过拟合
对于猫来说，每个神经元侧重的重点检测是不一样的，当我们进行神经元随机的掐死之后，神经元之间的固定的联系就会减少，例如可能毛茸茸特征和尾巴的特征会在训练中形成较强的关联性，但是随机掐死之后，两个特征就不会产生之前那么强烈的关联性，就能让特化的神经元不会根据别的神经元来判断自己需要识别的特征属性
![[Pasted image 20230328160600.png]]
以一个简单的两个神经元的过程为例，训练阶段的期望是右下角的公式，但是在测试阶段我们希望所有的神经元都可以参与到预测中，我们就需要将测试过程中的数学期望补充回来
![[Pasted image 20230328161850.png]]
代码中就需要在scale的时候同时乘以p的概率
另一种方法inverted dropout
![[Pasted image 20230328162029.png]]
就是将掩模版除以一个p，p代表着数学期望的值
![[Pasted image 20230328162122.png]]
随机的让神经元参与或者不参与工作
![[Pasted image 20230328162219.png]]
当p=0.5实际上代表的各个模型是独立的出现概率相当，每次训练的时候合作的上游和下游的神经元都不一样
![[Pasted image 20230328162413.png]]
有各种说法，首先他可以看作非常巨量的不同的表现的模型的集成，其余的剩下几个说法如图所示
![[Pasted image 20230328162827.png]]
![[Pasted image 20230328162930.png]]
![[Pasted image 20230328163535.png]]
增加了各个神经元之间的独立性，不再需要协同合作就能够使得结果判断准确
![[Pasted image 20230328163618.png]]
还增加了稀疏性，找到了主要矛盾
![[Pasted image 20230328163748.png]]
数据集够大才能使用dropout，使用了dropout会使得训练事件变长
![[Pasted image 20230328163859.png]]
**图像增强**
进行反转，或者将原始图像按照位置截图，进行翻转可以得出避免偶然误差的结果
或者将像素进行相同数量的偏移
![[Pasted image 20230328164056.png]]
![[Pasted image 20230328164118.png]]
![[Pasted image 20230328164201.png]]
池化窗口也可以 是除了正方形的别的形状
![[Pasted image 20230328164238.png]]
可以随机掐死某一些层，这也是dropout的一种变化
![[Pasted image 20230328164322.png]]
这种方式将某些区域抹去，这样可以增加神经元鲁棒
![[Pasted image 20230328164409.png]]
混合图像也可以增加正则化
![[Pasted image 20230328164451.png]]
用以上的这些技巧可以防止过拟合

**超参数的选择**
![[Pasted image 20230328164657.png]]
![[Pasted image 20230328164924.png]]
![[Pasted image 20230328164941.png]]
![[Pasted image 20230328164955.png]]
![[Pasted image 20230328165003.png]]
![[Pasted image 20230328165018.png]]
这种情况就可能是初始化不好
![[Pasted image 20230328165042.png]]
这种是学习率比较大
![[Pasted image 20230328165057.png]]
过早降低学习率也不好
![[Pasted image 20230328165117.png]]
过拟合情况
![[Pasted image 20230328165143.png]]
一起增高就有可能是欠拟合，需要更大的模型训练更长的时间
![[Pasted image 20230328165222.png]]
上图为整体调参过程
![[Pasted image 20230328165559.png]]
![[Pasted image 20230328165704.png]]
找到合适的更新的量级，0.001是比较正常的
![[Pasted image 20230328165732.png]]
