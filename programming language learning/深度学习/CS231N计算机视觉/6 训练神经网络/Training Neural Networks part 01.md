![[Pasted image 20230327172137.png]]
![[Pasted image 20230327172157.png]]
mini batch是一种折中的训练方式，如果是每次一个数据，那就是震荡的很猛烈，如果每次使用全部数据，那就是直接下降，但是非常需要性能
![[Pasted image 20230327172314.png]]
训练前可以配置的
激活函数的选择，数据预处理，权重的选择，初始化，正则化和梯度校验
训练过程中关注的
是不是过拟合，验证集中的误差，选择优化器和超参数
最后进行模型评估和集成
![[Pasted image 20230327173640.png]]
![[Pasted image 20230327173659.png]]
非线性的激活函数可以让模型拟合非线性的分类边界

**sigmoid函数**
![[Pasted image 20230327185717.png]]
好处是可以类比神经细胞是否激活
sigmoid的饱和性会使它存在梯度消失现象，而且会永远输出为正值
![[Pasted image 20230327195331.png]]
输出都为正的情况下，所有的权重的偏导数都是正数，都是同增减性的，只是幅度不同
![[Pasted image 20230327195628.png]]
导致更新方向始终是第一和第三象限，如果我需要再蓝色方向更新优化，需要先让w2w1同时增大然后同时减小，但是幅度上为了保证在第四象限，需要w1增大的数量比w1减小的多，w2增大的量小于w2减小的数量，（w1b-w1a，w2b-w2a），这是不太好的路径
![[Pasted image 20230327200854.png]]
指数计算比较消耗资源

**双曲正切函数**
![[Pasted image 20230327200927.png]]
**ReLU Function**
![[Pasted image 20230327200949.png]]
它不会存在饱和，非常容易计算
![[Pasted image 20230327201336.png]]
使用ReLU函数和双曲正切函数在降低损失函数到0.25的情况下的表现
这个图中表示的是ReLU函数只用了比较少的轮数就可以收敛到0.25
![[Pasted image 20230327201519.png]]
![[Pasted image 20230327201931.png]]
关于0没有对称，小于 0的永远不会更新
![[Pasted image 20230327202015.png]]
人们为了避免出现dead ReLU现象，会增加一个偏置项
![[Pasted image 20230327202152.png]]
这里的α也可以使用反向传播数据驱动的方式去选定
![[Pasted image 20230327202303.png]]
改善了ReLU没有关于原点对称的特点
![[Pasted image 20230327202405.png]]
Maxout函数，利用多个神经元去解释两个神经元输出一个神经元
![[Pasted image 20230327202517.png]]

**Data Preprocessing**
![[Pasted image 20230327202635.png]]
第一种方式是标准差化
![[Pasted image 20230327203443.png]]
第二种方向是主成分分析
按照第一和第二主成分分析，变成中间图的样子，然后白化PCA标准化是直接从图二标准化
ZCA标准化是PCA旋转回到原坐标系中

为什么进行数据处理
![[Pasted image 20230327203803.png]]
损失函数就不会那么敏感，使用更大的学习率进行学习也会更加容易
![[Pasted image 20230327203952.png]]
三种不同的网络使用的不同的预处理方式
![[Pasted image 20230327204134.png]]

**初始化**
初始化initialization不可以变为同一个数，这样和同一个神经元就没有区别了
逻辑回归这种机器学习的方式是可以初始化为同一个常数
![[Pasted image 20230327204422.png]]
初始化让每个神经元中的矩阵都服从标准正太分布，均值为0方差为1的数
![[Pasted image 20230327204638.png]]
在这个w采用0.01的幅度，标准化的情况，会出现集中在0的情况，这样梯度就会消失
当w采用0.05的幅度时候，会集中到两端的饱和区域
![[Pasted image 20230327204928.png]]
这样也会导致梯度消失
![[Pasted image 20230327205115.png]]
此处的Xavier初始化是根据输入维度决定的，根据输入维度的平方根进行初始化
随着层数的增加是均匀的分布在区间内的，**这样可以更好的保留梯度**到后面的层数的时候输入和输出的方差是非常相近的，没有 出现急剧的聚集或者分散的现象
这个原理定性上说，输入的维度也就是特征值越多，它产生的鱼龙混杂的不同的数据也更多，我就需要更大的惩罚值
![[Pasted image 20230327210029.png]]
![[Pasted image 20230327210154.png]]
![[Pasted image 20230327210300.png]]
在ReLU函数中没法使用这种方式
![[Pasted image 20230327210449.png]]
![[Pasted image 20230327210531.png]]
权重初始化的领域研究

![[Pasted image 20230327210758.png]]
这样可以强行让每一列的数据都变成了标准正态分布
![[Pasted image 20230327210906.png]]
有的时候强行标准化并不能带来良好的表现，就有人想到了外边再套一层线性的函数
这样可以尽可能的暴露出原图的梯度
![[Pasted image 20230327211040.png]]
这个公式中的γ和β是学习得来的
![[Pasted image 20230327211414.png]]
BN层也就是batch normalization一般是放在非线性层之前的
这种操作的好处如下
![[Pasted image 20230327211527.png]]
训练和测试的时候使用的BN层不一样，训练的时候采用的是每一批的均值和方差，测试阶段采用的是全局的均值和方差
![[Pasted image 20230327211730.png]]
还有别的normalization的方式
![[Pasted image 20230327211801.png]]
![[Pasted image 20230327212236.png]]
