![[Pasted image 20230322154749.png]]
![[Pasted image 20230322154922.png]]
梯度下降的方法求得损失函数对于每一个权重W的梯度，也就是求偏导，从这梯度的反方向，乘以学习率去更新权重
![[Pasted image 20230322155106.png]]
通过数值解去进行检验，通过解析解求得结果
![[Pasted image 20230322155150.png]]
线性分类器的局限性是无法进行图示的同心圆的数据分类，或者是异或型的数据分类
解决方案
1.不使用单纯的像素特征，而是使用曲率直方图等别的特征，转换后可能就变成线性可分的了
2.使用非线性引入线性分类器
![[Pasted image 20230322155411.png]]
传统机器学习需要知道需要提取什么特征，深度学习不需要知道特征是什么
线性分类器引入非线性就会成为神经网络中的神经元
![[Pasted image 20230322160047.png]]
神经突触进行数学抽象后可以变为，从各个”树突“也就是 输入端输入x作为 信号，信号 叠加一个权重w进行线性乘积后汇合到神经元中加入一个偏置项b，这个先作为一个初步的节点，然后我们需要一个激活函数，在这里是sigmid激活函数，因为它可以把很大的数缩减到0-1之间的如图所示的有规律的变化中去
![[Pasted image 20230322160317.png]]
![[Pasted image 20230322160625.png]]
通过训练从input层一直到最后的output层中间的所有的层所有的权重w
这个sigmoid函数 让线性分类器进行了一个非线性的输出
![[Pasted image 20230322160931.png]]
除了sigmoid激活函数还有这种relu激活函数，修正线性单元激活函数，神经元之间都是密集相连的，所有的都有权重，这叫做全连接神经网络FC fully-connected network 或者叫做多层感知机MLP multi-layer perceptrons
![[Pasted image 20230322161338.png]]
这里是32x32的3通道的图片，拉平后变成一个3072 的长向量，添加入权重后变成100个神经元，这100个神经元都包含有3072个权重W1，乘以输入端就是100个输出值，然后这100个值就有100个权重w2，再得出s的10个output
![[Pasted image 20230322162150.png]]
使用了非线性激活函数才能让每个神经元都有差异的变化，而不是每个神经网络都是线性变化
![[Pasted image 20230322162247.png]]
常见的激活函数类型如上图所示，最常用的是relu激活函数
![[Pasted image 20230322163912.png]]
![[Pasted image 20230322163953.png]]
上图是两层神经网络
![[Pasted image 20230322164254.png]]
有可能过拟合，当层数过多的时候
生物的神经元由很多不同点
![[Pasted image 20230322164510.png]]
![[Pasted image 20230322164626.png]]
![[Pasted image 20230322165032.png]]
用链式法则求导，也就是求偏导的过程，求全局梯度就是求这个损失函数对于这个参数w或者输入自变量xn的偏导数
![[Pasted image 20230322180454.png]]
加法每个因素对于神经元的影响是一样的，所以最后的梯度结果是梯度均分，乘法是梯度交换，以两个变量为例子，下边的求偏导数的结果是上边的数，上边求偏导数的结果是下面的数，max门是只走最大的那一个门路径，copy门类似于add gate，只不过是反向的
![[Pasted image 20230322180841.png]]
前向传播的过程是图示的从左至右，反向传播就是从右至左，前向传播就是求函数值，求损失函数的结果，按照f关系求后面的结果，反向传播 就是求每一步的偏导数
![[Pasted image 20230322181059.png]]
如图所示，每个x对应有一个w，他们的偏导都是互相对应的，求完偏导之后反向更新原函数，这个问题一直没有解释！**更新，怎么更新**
![[Pasted image 20230322181547.png]]
上图是定义前向传播和反向传播的python代码，首先定义前向传播，算出的损失函数结果，然后定义反向传播
![[Pasted image 20230322181733.png]]
上图是由实际具有意义的函数组成的pytorch代码，是pytorch中的模块
![[Pasted image 20230322182356.png]]
![[Pasted image 20230322182822.png]]
上图下面的红色部分是反向传播得出结果
![[Pasted image 20230322183048.png]]
数学上证明了，就算只有一层神经网络，只要神经元数量足够多，就可以拟合任意的函数，模型越深代表进行了更多的非线性运算，我们希望更深的网络