![[Pasted image 20230329152138.png]]
keras深度学习框架中提供的预训练模型
top1准确率是代表着只猜测1次就能够得出正确结果的概率，top5就是同理5次
![[Pasted image 20230329152352.png]]
这次的内容就是搞定这些模型的原理
NASNet是神经架构搜索，利用人工智能设计神经网络
![[Pasted image 20230329153417.png]]
**手写数字识别**
该模型奠定了卷积池化全连接输出的结构1998年发表的论文 LeNet-5模型
![[Pasted image 20230329155119.png]]
平移不变性来自于池化操作，但是这样就弱化了信息的空间属性
**Alex net**
![[Pasted image 20230329155333.png]]
NORM1是局部响应归一化层，现在已经不怎么使用了
![[Pasted image 20230329155639.png]]
原始的输入为图示的图像，3通道的彩图
第一层使用了96个11x11的卷积核，stride 为4，第一次生成的activation maps的大小为55x55
存在有96个卷积核
![[Pasted image 20230329155935.png]]
就有了上图所示的参数个数
![[Pasted image 20230329160010.png]]
进行3x3的stride为2的最大池化，池化后的结果就是27x27的feature maps
![[Pasted image 20230329160055.png]]
这里采用的是max的池化，选出最大的值
![[Pasted image 20230329160310.png]]
这里 创新型采用了ReLU函数，局部响应归一化是让神经元不会表现的很活跃，然后就是集成的思想，让模型的表现更加优良
![[Pasted image 20230329160702.png]]
当时的GPU性能不行，所以需要两块GPU并行计算一半的channel
![[Pasted image 20230329161252.png]]
![[Pasted image 20230329161315.png]]
所以原文中的图可以转换为一块GPU运算，图示就可以表示为下面这样
![[Pasted image 20230329171556.png]]
![[Pasted image 20230329171631.png]]
这个就是AlexNet网络中的图示过程
![[Pasted image 20230329184316.png]]
采用数据增强的方式 和ReLU激活函数可以提高精度，加速过程
![[Pasted image 20230329184407.png]]

**ZFNet**
![[Pasted image 20230329184937.png]]
优化了AlexNet，更改了一些 超参数帮助训练好了模型
![[Pasted image 20230329185111.png]]
改论文是可视化，使用 反卷积的方法重构回原来的像素空间
![[Pasted image 20230329190008.png]]
在反卷积中，其实就是反池化反卷积，只有在反池化和正向不一样，反向需要考虑位置信息，就是池化过程中选择的数据是从哪里来的，我们需要记录池化过程中的数据是从哪里来的
也就是记录在switches的部分
**VGGNet**
![[Pasted image 20230329191738.png]]
在VGG中都是3x3的卷积，步长为1， padding为1
![[Pasted image 20230329192013.png]]
使用3x3卷积核是为了代替7x7卷积核的作用，并且使用小的代替大的可以让层数更多
![[Pasted image 20230329194304.png]]
前两个block有两个卷积层，后面三个都有3个卷积层
VGG19相比于VGG16在后3个block每个都是多一个卷积层，变成4个卷积层
![[Pasted image 20230329200215.png]]
卷积层中的参数是比较少的，全连接层 是比较多的
![[Pasted image 20230329200314.png]]
![[Pasted image 20230329200415.png]]
第一层全连接层太占用内存量，反向内存更大
所以VGG在计算上效率并不是很好
![[Pasted image 20230329200607.png]]
fc feature代表把原图变成一个向量，这个可以做visualization的工作
VGG19的性能更好一些，但是更加消耗资源
更多的内存读写会消耗更多的能耗，更多的散热成本会影响速度
**GoogLeNet**
![[Pasted image 20230329201044.png]]
创新型的提出了inception模块
例如狗的图片，在该图中的狗占图片的比例是不同的，所以我们需要采用的卷积核大小是待定的
![[Pasted image 20230329201718.png]]
![[Pasted image 20230329201809.png]]
inception模块就是采用了不同尺度的卷积核，最后堆叠到一起
举例说明是，为了让得到的作业本是同样大小，卷积核就需要采取设计，让其厚度不一样，但是最后得出的大小是一样
![[Pasted image 20230329202019.png]]
![[Pasted image 20230329202041.png]]
![[Pasted image 20230329202113.png]]
原始的模块
![[Pasted image 20230329202146.png]]
![[Pasted image 20230329202158.png]]
![[Pasted image 20230329202233.png]]
对应的乘法计算量为800多m，非常大，而且，这是由输入端的长宽决定的模型，所以只可能增加不可能减少
![[Pasted image 20230329202521.png]]
这里使用1x1的卷积核是用来升维或者降维的，例如原始的卷积层是64个通道，我们可以选取n个1x1的卷积核，当n>64的时候是升维，<64的时候是降维
所以为了减少计算量，我们需要用1x1卷积核来减少计算量
下图就是表明利用1x1卷积核来进行降维的过程
![[Pasted image 20230329202844.png]]
![[Pasted image 20230329202928.png]]
如果我们把维度降下来，刚才的inception module会变成下图
![[Pasted image 20230329203105.png]]
其中绿色是把所有的融合一起的层，其中三个黄色的是可以进行1000分类的地方，称之为辅助分类
![[Pasted image 20230329203530.png]]
![[Pasted image 20230329203629.png]]
![[Pasted image 20230329203700.png]]
在最后我们要进行global average pooling，在每一个通道计算平均数之后输出为一个长向量
![[Pasted image 20230329203833.png]]
![[Pasted image 20230329203932.png]]
GAP的作用可以可视化神经网络的作用，图示中狗对应的每一个通道都对应有一个参数W，用W乘以每一个通道的activation maps，最后叠加到一起，就是表示图中最关注的内容是什么
![[Pasted image 20230329204146.png]]
这里起到的是正则化的作用，将三个损失函数按照加权平均的方法求和，在训练时候可以保证在LA处就可以进行反向传播，这样就可以避免梯度消失的问题发生

inception模块的计算效率很高，避免了全连接层的过量数据计算，利用了1x1的卷积核减少了计算等等好处
![[Pasted image 20230329204555.png]]
后续出现的inception模块的更新版本是V4和renet

![[Pasted image 20230329204713.png]]
inceptionv2采用2个3x3的卷积代替1个5x5的卷积
![[Pasted image 20230329204758.png]]
然后3x3的卷积又变化为了1xn和nx1的卷积
这种算法可以从深度和宽度两个方向进行展开 

**ResNet**
![[Pasted image 20230329205029.png]]
核心在于Residual block
![[Pasted image 20230329205158.png]]
网络越深就造成了梯度消失的现象
![[Pasted image 20230329205301.png]]
残差网络让加深网络至少不会变得更差，方法就是把浅层网络添加到深层网络上
![[Pasted image 20230329205344.png]]
左侧是普通的网络，经过了F(x)后得到的结果是F(x)与原来的x相加，当F(x)中的参数都为0的时候这个加法得到的就是原来的x，所以下一步的表现至少和原来的表现相同，如果能够做出很微小的贡献，这种贡献就可以成为残差
![[Pasted image 20230329205622.png]]
![[Pasted image 20230329205902.png]]
残差就是学习线性回归模型
![[Pasted image 20230329210022.png]]
经过卷积之后的形状应该是一样的才能在对应位置进行相加，也就是HWC大小一样
![[Pasted image 20230329210113.png]]
让步长为2替代了下采样的效果
![[Pasted image 20230329210245.png]]
引入1x1卷积核可以先降维为64个通道，然后进行3x3的卷积，对应的通道数也就是64个，大大减少了计算量，然后再用1x1升维回原来的通道数量，这样能够保证原图的数据丢失比较少
![[Pasted image 20230329210427.png]]
![[Pasted image 20230329210613.png]]
stochastic depth可以随机让残差块消失作用，可以减少梯度消失现象以及减少相关性
![[Pasted image 20230329210713.png]]
Dense Net中就是让前一层和后一层相连，一个Dense模块就是让残差网络的每一层都互相产生关联
![[Pasted image 20230329210807.png]]
最后模块组合到一起就能够形成一个网络
![[Pasted image 20230329210917.png]]
靠左上角的模型表现得更好，VGG就比较臃肿，消耗内存比较大，ALexNet表现效果不好，消耗内存也比较多
![[Pasted image 20230329211052.png]]
![[Pasted image 20230329211253.png]]
集成模型也可以增加预测的效果
![[Pasted image 20230329211422.png]]
这篇文章利用自适应学习学习到每一个通道的权重
对于一个卷积后的一个feature map进行GAP压缩成一个1x1xC的值，用2层FC层进行excitation，最后得到了一个彩色的1x1xC的新的向量，这个是带权重的，我们认为这个就反映了U这个feature map中的每个channel的权重，再全连接回去，就可以了，可以街道inception后面，也可以接到resnet后面
再2017年后imagenet竞赛就迁移到了kaggle上面，线上的竞赛内容
![[Pasted image 20230329211958.png]]
这个方法可以压缩模型，并且不损失太多的精度
![[Pasted image 20230329212411.png]]
上图为发展的整体流程
![[Pasted image 20230329212505.png]]
这里是可以调用模型，从keras网站上

**Network in Network**
![[Pasted image 20230329212647.png]]
两个卷积层中添加一个全连接层形成一个网中网，1x1的卷积相当于是一个全连接层MLP，因为它就只有一行或者说一列，这其实就是bottleneck的前身
![[Pasted image 20230329212829.png]]
利用Identity Mapping来改进resnet
![[Pasted image 20230329212924.png]]
在中间加入Batch Normalization层可以优化resnet，让原来的数据更加标准化，也就是更加的圆润，不会受到边缘数据的影响，不会那么敏感
![[Pasted image 20230329213159.png]]
该文章表明了残差块的质量比数量更加重要，也就是让残差网络模块更宽，形成成倍数的F的宽度的残差模块，卷积核数量增多是并行运算，所以这样可以增加计算的效率
![[Pasted image 20230329213650.png]]
这个是把resnet和inception结合在一起，运用升维和降维减少计算量
![[Pasted image 20230329213904.png]]
利用分形的思想，因为 分形在每一个小的part中都是相似的内容，但是这样计算需要利用droppath的思想，就是在这个模块中随机选择一路进行失活作用
![[Pasted image 20230329214148.png]]
densenet减缓梯度消失，每一层与前面的层相连
![[Pasted image 20230329214223.png]]
mobilenet核心思想是深度可分离卷积，每个通道都单独的利用一个卷积核进行卷积，这样可以提取空间特征，因为每一个channel计算出的数值不一样，就可以与空间位置进行对应
再用长的1x1卷积提取跨层的特征
这种轻量化的模型是为了减少计算量设计的
![[Pasted image 20230329214517.png]]
元学习，利用强化学习让机器学习去学习设计网络结构，让RNN设计网络结构，然后抽取其中的结构，然后给与奖励之后，回到RNN中强化学习
![[Pasted image 20230329214650.png]]
从这个controller中采样一些结构，达到了R奖励，并且保持一定的精度，计算这个概率的梯度，传回去，增加好模型的似然概率，减少坏模型的似然概率，达成一个让机器学习设计神经网络的作用
![[Pasted image 20230329214822.png]]
先找到基础单元，先在cifar10上训练，然后泛化到imagenet中，自动找到最优的神经网络