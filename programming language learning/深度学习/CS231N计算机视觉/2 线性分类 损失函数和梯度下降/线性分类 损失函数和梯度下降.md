![[Pasted image 20230313155114.png]]
通过梯度下降让训练数据可以使用线性分类器拟合
![[Pasted image 20230313160439.png]]
损失函数就是比较真实与分类模型的差距的定量分析
![[Pasted image 20230313160740.png]]
其中f函数是求得的分数，与真实的标签进行比较，最后求平均值就是损失函数

![[Pasted image 20230313160855.png]]
hinge loss function
![[Pasted image 20230313161152.png]]
![[Pasted image 20230313161223.png]]
![[Pasted image 20230313161257.png]]
分类错误的值和正确值求差值+1后与0比大小，取更大的那个值，这里面猫的损失函数就是2.9.汽车的损失函数就是0，青蛙的就是12.9![[Pasted image 20230313161508.png]]
其中1是冗余的安全度，为什么选择1作为安全度可以看cs231n笔记中note的部分
铰链损失函数分类错误的损失求和，如果随机初始化parameter W，导致所有的值都相近，≈0，代表损失函数就是n-1，其中n是分类的数量
而且无论是求平均还是把所有类别都求差值跟0比较后相加，都是增加一个常数变化
![[Pasted image 20230313162631.png]]
注意同一损失函数对应的可以是不同的权重
下面如何选择这些不同的权重需要引入正则化
![[Pasted image 20230316142018.png]]
在之前构造的损失函数中加一个正则化的项，其强度由λ控制
![[Pasted image 20230316142155.png]]
![[Pasted image 20230316142217.png]]
λ是为了参数和权重更小，上图是L2正则化和L1正则化，其意思是每个权重的平方和或者是绝对值的和，还有弹性网络正则化，同时考虑L1和L2正则化，β衡量对于L1和L2的关注程度，这种操作是为了防止overfit
![[Pasted image 20230316142944.png]]
后续还有更多的防止过拟合的措施![[Pasted image 20230316143041.png]]
为了让权重更加简单，为了让模型能更好的表现在测试集上泛化
![[Pasted image 20230316143715.png]]
假设现在又w1和w2模型用来描述同一个数据集x，他们的运算结果都是1，但是正则化项利用L2的话是w2的模型比较小
![[Pasted image 20230316144346.png]]
其中更好的是绿色的模型，因为他没有刻意记住一些噪声点，可以更好的泛化这个模型
![[Pasted image 20230316144633.png]]
在进行图片分类的时候需要把这些打分转换为百分数，使用的就是指数函数，把分数变为e的n次幂，这样可以保留其单调性，并且永远是正数
![[Pasted image 20230316144817.png]]
然后进行归一化，就可以得到一个概率的数
![[Pasted image 20230316144958.png]]
整体的公式是如上图，其中的p就是该概率，s是根据算法进行图像的打分
![[Pasted image 20230316145048.png]]
最后是变成交叉熵损失函数，正确类别概率越靠近1，损失函数约等于0，当正确类别的概率为0损失函数就越接近于正无穷
![[Pasted image 20230316145245.png]]
当事件相互独立的时候，对于所有事件的概率求乘积就是整体模型的预测概率，这个结果是非常小的，当我们把这些乘积外边套一个对数log就能够把乘积变成求和，log函数在0-1之间是负值，所以这个时候把负值变为正值就要加－我们需要的是这个乘积的结果最大，转换为和的结果最大，但是他是负数所以需要变成正数求和最大，这就叫极大似然估计
这种交叉熵损失函数是其中的一种，其余还有KL散度，相对熵，是交叉熵减去信息熵，这个熵是物理中衡量混乱程度的量，熵越小越能正确的分类
![[Pasted image 20230316150304.png]]
![[Pasted image 20230316150545.png]]
![[Pasted image 20230316150623.png]]
交叉熵损失函数的最小值是0最大值是正无穷
![[Pasted image 20230316150922.png]]
当最开始的时候每个类别的分数都是相近的，最后的结果会等于多少
答案：log（c），c是类别的数量，最开始时候Li=-log（1/c），因为分了c类，每一类占比一份就是1/c
![[Pasted image 20230316151420.png]]
上图是铰链损失函数和交叉熵损失函数的对比图
解释如下：
左侧是一个权重的矩阵W，它代表的是把图片分成三种类别，xi是测试图像的各个像素的代表值，最后加上截距b
其中W需要进行转置后才能和xi相乘，最后得出的三个数是每个类别的分数，以猫图片为例子，就是把猫作为青蛙，车和猫三种类别的分数
铰链损失函数是把错误的类别和正确的类别分数相减加上1之后和0比较大小，就可以得到那些分数比正确分类分数高的那些图片就会构成损失函数的项
交叉熵损失函数就是求百分比，但是中间运用数学过程，先用e作为底进行幂次方，然后归一化，最后为了求得整体表现需要进行-log开对数
经过softmax变换，会把原来相隔不是很大的数据拉开很大的距离，也就是下图问题中的回答
![[Pasted image 20230316162918.png]]
![[Pasted image 20230316163052.png]]
首先拥有一个数据集，然后运用函数和参数权重打分，然后求得softmax损失函数，还有hinge loss function，选用什么就是自己的选择，然后再加上正则化的λR(W)就是总体损失函数

接下来是优化过程optimization
这个权重可以随机生成选择，也能获得一定的成功率
第二种方法就是求导数，就是求导之后再求相反数，才是下降最快的路径
![[Pasted image 20230316164039.png]]
这种是数值梯度解法，是通过把参数W增加一个很小的数，然后求tan，也就是斜率，看他的变化，也就是求导数的一个粗略的近似数，然后通过求出的这个tan，求负数之后反映到原来的参数W中，这种方式比较慢，但是比较容易写出来，而且是一个近似的值
也可以求解析解，也就是求导数，这样快速准确，但是对于数据的要求比较严格
实践运用中我们会通过数值梯度解法去求相应的梯度的检查
这种反向过程我们不希望太大，所以会乘以一个学习率step size
![[Pasted image 20230316164541.png]]
这是最出名的梯度下降的算法，是使得损失函数下降
![[Pasted image 20230316164830.png]]
实际问题不是一个梯度下降的终点，是由很多的坑底组成的高维空间，取决于矩阵构成的向量空间的维度
![[Pasted image 20230316165229.png]]
一般是采用minibatch的方式输入进去，是可以快速的反应的，可以提高算法的速度，一般会选择2的指数作为batchsize，要通过交叉验证的方式去求证出超参数正则化的数值 

![[Pasted image 20230316170300.png]]
可以手动的对图片进行特征标注
![[Pasted image 20230316170351.png]]
可以用图像中一块区域的方向梯度作为特征，方向梯度直方图作为特征，比单纯的使用像素点所谓特征要好非常多，sift，索贝尔算子等特征都可以使用
![[Pasted image 20230316170552.png]]
首先是对于原始图像进行特征的提取，然后把他这些特征构成形成的单词本记录下来，然后进入一张新的相同特征的图片，然后提取其中的特征，然后把特征的单词记录下来
![[Pasted image 20230316170836.png]]
这张图解释了首先是颜色梯度特征，然后是sift特征，然后是方向直方图特征，最后组合到一起就成为了这张图片的特征表示，这个流程就是特征工程
深度学习就是使用大量的数据集作为样本，然后使得神经网络可以自动提取出对他有用的识别图片特征的cluster，让损失函数最小化
![[Pasted image 20230316171403.png]]
梯度下降的方法实际上就是求各个参数的损失函数的偏导数