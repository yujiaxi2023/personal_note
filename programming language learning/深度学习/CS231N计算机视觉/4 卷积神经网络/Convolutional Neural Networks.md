卷积神经网络的基本结构就是
Convolutions >>> Subsampling >>> Fully Connected
![[Pasted image 20230323144301.png]]
pooling 池化 == 下采样 subsampling
![[Pasted image 20230323144453.png]]
![[Pasted image 20230323144517.png]]
![[Pasted image 20230323144616.png]]
![[Pasted image 20230323144640.png]]
![[Pasted image 20230323144757.png]]
![[Pasted image 20230323144817.png]]
视觉是分层的，而且刺激的区域是不同的，这些是脑科学的结论
![[Pasted image 20230323144945.png]]
以上的发现都启发了卷积神经网络
![[Pasted image 20230323145046.png]]

卷积神经网络可以做到的内容：
![[Pasted image 20230323145156.png]]
![[Pasted image 20230323145205.png]]
![[Pasted image 20230323145246.png]]
边缘计算是本地计算，要求的算力比较高，GPU并行计算是可以加快运算的
![[Pasted image 20230323145407.png]]
![[Pasted image 20230323145416.png]]
遥感图像分析
![[Pasted image 20230323145608.png]]
神经风格迁移
![[Pasted image 20230323145645.png]]
![[Pasted image 20230323155224.png]]
cifar10 的图片变成一个长向量，用10个分类器分类，每一个分类器有3072个权重，得出了最后的结论数字，但是图片变成长向量之后丢失了位置信息
![[Pasted image 20230323155426.png]]
卷积层池化层交替前行后到全连接层
![[Pasted image 20230323155510.png]]
![[Pasted image 20230323155542.png]]
左图中红色的数字部分是卷积核，图像的像素和乘以对应的卷积层的权重，黄色是感受野，右侧图最后生成的是feature map
![[Pasted image 20230323155751.png]]
最左侧是感受野，中间块是卷积核，最后蓝色部分是feature map
这个过程可以作为矩阵乘以权重矩阵的转置，求矩阵的值
![[Pasted image 20230323160343.png]]
图示是利用三种权重进行卷积核的操作获得的三个结果feature map作为下一个卷积层的输入
我们发现了中间的信息进行了多次的卷积运算，边角的部分进行很少次数的卷积运算
我们这个时候会再边缘的区域增加0
![[Pasted image 20230323160603.png]]
每次往右滑动1格就是横向的步长是1，往下1格就是纵向的步长是1，步长可以变化
![[Pasted image 20230323160741.png]]
照片一般都是RGB的3通道矩阵，卷积核也要有3通道
![[Pasted image 20230323163338.png]]
当卷积核出现在原图中的比例很高的时候feature map的结果就会很高
![[Pasted image 20230323163507.png]]
当左边的矩阵作为卷积核，就是对于图像平均的进行模糊，当右边的矩阵作为卷积核，就可以检测出图形的边界
![[Pasted image 20230323163632.png]]
第一个卷积核会把左边暗右边亮的部分边缘提取出来，因为卷积核左边值更低右边值更高，第二个卷积核是把左边亮右边暗的部分提取出来，其余都是类似的
不同的卷积核会提取出原图中的不同的特征
![[Pasted image 20230323164150.png]]
第一个图是没有补0，第二个图是周围补了若干的0，第三个图补0后形成跟原图一样的维度，下边的图是步长padding为2
还有从小图变成大图的feature map，叫做反卷积
![[Pasted image 20230323164437.png]]
![[Pasted image 20230323164451.png]]
还有棋盘型的卷积，卷积核kernal size不一定采集连续的像素，卷积的目的就是 把原图中符合 定义的部分提取出来
feature map也可以成为activation maps
![[Pasted image 20230323171033.png]]
当一个activation map的数值比较大的时候，如果是每个像素值进行计算，那获得的结论就会比较大，这个时候就需要池化这个操作
像是图示中，一个颜色为一个池子，从这个pool中选择出一个值，比如说平均或者是最大值用来代表这个框的区域的特征
![[Pasted image 20230323171414.png]]
这样可以使得卷积神经网络具有平移不变性
这个过程叫做下采样操作subsampling，池化过程不需要权重
池化的作用是 1 减少参数量 2 防止过拟合 3 平移不变性
![[Pasted image 20230323172147.png]]
训练对象是权重和偏置项，在全过程中的值
![[Pasted image 20230323172452.png]]
这里面每个神经元都只接入一块区域，也就是receptive field，卷积神经网络的三大特点
局部感受野，权值共享和下采样
MNIST数据集是手写数字的数据集
![[Pasted image 20230323181703.png]]
该篇论文是讲的上采样的隐含层的内容
有多少个卷积核就会有多少个activation maps
![[Pasted image 20230323183857.png]]
我们需要尽量避免无法把所有的区域利用在内的步长
![[Pasted image 20230323183956.png]]
使用原始数据的边长减去卷积核的大小长度除以步长，需要除尽，否则就会出现部分未采样的情况出现
![[Pasted image 20230323184139.png]]
当padding之后需要在两侧加入padding的值的两倍
![[Pasted image 20230323184900.png]]
正常情况下需要把长方形的图片reshape成正方形的图片
![[Pasted image 20230323185946.png]]
当步长为1，卷积核为5，padding为2的时候维度不变，但是有10个卷积核就有10个activation maps
因为有3通道，所以需要有5x5x3然后还有一个偏置项d所以是75+1
![[Pasted image 20230323190336.png]]
![[Pasted image 20230323190516.png]]
卷积核的个数最好是2的指数次方
和batch size一样
1x1卷积是如下图所示的卷积
![[Pasted image 20230327141546.png]]
可以通过不同卷积核的大小来进行降为或者升维
如图中的比192小就是降维，比192大就是升维
![[Pasted image 20230327141739.png]]
1x1的卷积核在network in network文章中第一次展现
![[Pasted image 20230327142035.png]]
在该文章中介绍了1x1卷积核的作用，如果我有原始数据28x28x256项数据，如果用5x5x256的卷积核进行计算进行一次计算的数量为200k左右，整体进行计算最终是160m左右
如果我先用1x1卷积核进行降维，然后用5x5卷积核升维，计算量会减小很多
好处有四点，第一进行降维升维，第二然后可以进行跨通道的信息交融，因为1x1的卷积核可以直接扎穿整个元数据，第三是减少参数量，第四是可以增加模型的深度
每次卷积都是神经元的计算
![[Pasted image 20230327143315.png]]
池化是容易丢失信息的，所以想要保留空间信息最好不要使用池化，FCN网络是倾向使用的