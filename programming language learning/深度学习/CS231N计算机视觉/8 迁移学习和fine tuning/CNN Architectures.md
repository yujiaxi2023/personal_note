![[Pasted image 20230328210431.png]]
首先拥有别人训练好的模型，然后把自己的输出分类的类别层更改，然后冻结模型前面层的结构和权重，根据数据集的大小决定需要修改的层数，底层的权重我们需要冻结
![[Pasted image 20230328210736.png]]
当我们的数据量很大的时候，而且分类的类别也不一样，那我们需要训练的层数就要更多了
![[Pasted image 20230328211317.png]]
现成的优秀模型实际上是主干网络
![[Pasted image 20230328211401.png]]
很多训练好的模型都是imagenet上的冠军模型，所以需要多关注这些模型的发布
![[Pasted image 20230328211614.png]]
有的时候预训练模型的表现并不是那么万能的作用，在这篇文章中预训练模型只在早期起到了加速的作用
![[Pasted image 20230328211739.png]]
如果自己的数据集不合适，那第一个方法可以是找到一个大的类似的数据集，然后进行微调和迁移学习到小的数据集
![[Pasted image 20230328211839.png]]
我们认为 底层的卷积层都是类似的，所以我们可以采用迁移学习的方式去进行研究
![[Pasted image 20230328212654.png]]如果训练时候采用的数据集跟我们研究问题是很类似的内容，而且很小的数据集，那我们需要做的只是把最后一层的线性分类器进行更改就可以了
如果我们的数据集很小，但是分类的类别不一样，我们就自定义线性分类层，而且采用是前面的预训练模型比较基础的几层，去掉最后特化的层数
![[Pasted image 20230328212853.png]]
如果我们的训练数据集很大，但是类别和原来的数据集的类别很相似，那我们就直接用已有的深度学习模型框架去学习这个数据集，也就是初始化等初始设定都保留的相似的内容，所以不是随机初始化的内容，最后的线性分类层需要自己确定
![[Pasted image 20230328213124.png]]
如果数据集很大，而且类别也不一样，跟上述的内容是类似的
![[Pasted image 20230328213317.png]]

**皮肤癌的案例研究**
![[Pasted image 20230329145757.png]]
使用了inceptionv3模型
![[Pasted image 20230329142115.png]]
初始化了最后的全连接层
![[Pasted image 20230329143007.png]]
true positive 占比所有得病的人数表示的是sensitivity敏感度，true negative占比健康的人数为specificity特异性
![[Pasted image 20230329143242.png]]
我们也可以称真实检测出生病的人数占比所有的人数称之为recall，真实检测出生病的人数占比本模型检测出生病的人数，也就是包含了假阳性的数据，可以称之为precision
![[Pasted image 20230329143556.png]]
可以设置阈值作为检测的依据，我们尽量把阈值减小尽量避免漏检
![[Pasted image 20230329143716.png]]
ROC曲线靠近右上角就表现得更好，代表着sensitivity和specificity都表现得良好
![[Pasted image 20230329144000.png]]
降维可视化解释了模型是如何认识不同的病变特征
![[Pasted image 20230329144107.png]]
saliency map这张图是根据梯度变化的情况反向绘制的，是把损失函数反向传播计算相对于输入数据的梯度，表明图像只要按照图示的方式进行像素方向的变化，最后会得到该病症的诊断结果
![[Pasted image 20230329144242.png]]
![[Pasted image 20230329144251.png]]
混淆矩阵，是为了表示被误分类的概率，对角线的颜色越深代表的误分类概率越小
![[Pasted image 20230329144407.png]]
在对比CNN和专家对于不同类别皮肤病的混淆矩阵中，可以看出，专家对于皮肤病混淆的判断是类似的，对于某一些病症的判断能力是非常优秀的，CNN对类型判断的极限值没有专家那么准确

**迁移学习案例研究**
![[Pasted image 20230329145829.png]]
该篇论文告诉我们如何选择中间层进行迁移学习
![[Pasted image 20230329150016.png]]
根据是否锁住判断是B3B还是B3B+，而AB代表是原始的模型类别，A就是A数据训练出来的
![[Pasted image 20230329150902.png]]
**红色的transfer AnB，取A模型的前N层在B模型上解决问题，就可以看出如果保留了前7层在B模型上进行测试的结果很低
蓝色的selffer BnB，取B模型的前N层，后边的层数随机初始化，重新训练后在B模型上进行测试，出现了先下降后上升，代表了下降的第4层和第5层是具有联合依赖性，而且这个依赖性是很脆弱的依赖性，表示这几层的联合依赖性一旦粗暴的停止这个层的功能，最后会降低模型的表现
带+号的都是表现得不错，因为可以重新构建依赖性，如果采用的是原来的依赖性，在新模型表现就不行，如果是重新初始化训练，依赖性被重构了，那重新变成了一个表现良好的黑箱子**
![[Pasted image 20230329151756.png]]
这张图的结论就是我们需要有fine tuning就是重新训练进行微弱的调整，不是死套原来的模型
